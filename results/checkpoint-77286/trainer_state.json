{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 77286,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.019408431022436147,
      "grad_norm": 3.0156495571136475,
      "learning_rate": 1.9871904355251923e-05,
      "loss": 6.0953,
      "step": 500
    },
    {
      "epoch": 0.038816862044872294,
      "grad_norm": 2.5376834869384766,
      "learning_rate": 1.974251481510235e-05,
      "loss": 5.175,
      "step": 1000
    },
    {
      "epoch": 0.05822529306730844,
      "grad_norm": 3.942051887512207,
      "learning_rate": 1.9613125274952772e-05,
      "loss": 5.1,
      "step": 1500
    },
    {
      "epoch": 0.07763372408974459,
      "grad_norm": 4.331577777862549,
      "learning_rate": 1.94837357348032e-05,
      "loss": 5.0535,
      "step": 2000
    },
    {
      "epoch": 0.09704215511218073,
      "grad_norm": 3.4310264587402344,
      "learning_rate": 1.9354346194653627e-05,
      "loss": 5.051,
      "step": 2500
    },
    {
      "epoch": 0.11645058613461688,
      "grad_norm": 3.5291943550109863,
      "learning_rate": 1.9224956654504052e-05,
      "loss": 5.0154,
      "step": 3000
    },
    {
      "epoch": 0.13585901715705304,
      "grad_norm": 4.405115604400635,
      "learning_rate": 1.9095567114354476e-05,
      "loss": 4.9982,
      "step": 3500
    },
    {
      "epoch": 0.15526744817948918,
      "grad_norm": 5.135713577270508,
      "learning_rate": 1.8966177574204904e-05,
      "loss": 4.9702,
      "step": 4000
    },
    {
      "epoch": 0.17467587920192532,
      "grad_norm": 3.2893805503845215,
      "learning_rate": 1.883678803405533e-05,
      "loss": 4.9703,
      "step": 4500
    },
    {
      "epoch": 0.19408431022436146,
      "grad_norm": 6.493264675140381,
      "learning_rate": 1.8707398493905756e-05,
      "loss": 4.9742,
      "step": 5000
    },
    {
      "epoch": 0.2134927412467976,
      "grad_norm": 4.357235431671143,
      "learning_rate": 1.857800895375618e-05,
      "loss": 4.9568,
      "step": 5500
    },
    {
      "epoch": 0.23290117226923376,
      "grad_norm": 2.8122124671936035,
      "learning_rate": 1.8448619413606605e-05,
      "loss": 4.9517,
      "step": 6000
    },
    {
      "epoch": 0.2523096032916699,
      "grad_norm": 3.5628552436828613,
      "learning_rate": 1.8319229873457033e-05,
      "loss": 4.9229,
      "step": 6500
    },
    {
      "epoch": 0.27171803431410607,
      "grad_norm": 3.464205503463745,
      "learning_rate": 1.8189840333307457e-05,
      "loss": 4.9347,
      "step": 7000
    },
    {
      "epoch": 0.2911264653365422,
      "grad_norm": 3.894460916519165,
      "learning_rate": 1.806045079315788e-05,
      "loss": 4.9191,
      "step": 7500
    },
    {
      "epoch": 0.31053489635897835,
      "grad_norm": 3.4287831783294678,
      "learning_rate": 1.793106125300831e-05,
      "loss": 4.926,
      "step": 8000
    },
    {
      "epoch": 0.32994332738141446,
      "grad_norm": 4.080543518066406,
      "learning_rate": 1.7801671712858734e-05,
      "loss": 4.9238,
      "step": 8500
    },
    {
      "epoch": 0.34935175840385063,
      "grad_norm": 3.5280754566192627,
      "learning_rate": 1.767228217270916e-05,
      "loss": 4.9193,
      "step": 9000
    },
    {
      "epoch": 0.3687601894262868,
      "grad_norm": 4.128464221954346,
      "learning_rate": 1.7542892632559586e-05,
      "loss": 4.9027,
      "step": 9500
    },
    {
      "epoch": 0.3881686204487229,
      "grad_norm": 4.004984378814697,
      "learning_rate": 1.741376187149031e-05,
      "loss": 4.8945,
      "step": 10000
    },
    {
      "epoch": 0.4075770514711591,
      "grad_norm": 3.562501907348633,
      "learning_rate": 1.7284372331340736e-05,
      "loss": 4.9077,
      "step": 10500
    },
    {
      "epoch": 0.4269854824935952,
      "grad_norm": 3.1492257118225098,
      "learning_rate": 1.715498279119116e-05,
      "loss": 4.8967,
      "step": 11000
    },
    {
      "epoch": 0.44639391351603136,
      "grad_norm": 4.780231952667236,
      "learning_rate": 1.7025593251041588e-05,
      "loss": 4.8736,
      "step": 11500
    },
    {
      "epoch": 0.4658023445384675,
      "grad_norm": 3.2589855194091797,
      "learning_rate": 1.6896203710892012e-05,
      "loss": 4.8896,
      "step": 12000
    },
    {
      "epoch": 0.48521077556090364,
      "grad_norm": 3.1079773902893066,
      "learning_rate": 1.6766814170742437e-05,
      "loss": 4.8893,
      "step": 12500
    },
    {
      "epoch": 0.5046192065833398,
      "grad_norm": 3.26347279548645,
      "learning_rate": 1.6637424630592865e-05,
      "loss": 4.8899,
      "step": 13000
    },
    {
      "epoch": 0.524027637605776,
      "grad_norm": 3.2418606281280518,
      "learning_rate": 1.6508035090443292e-05,
      "loss": 4.9097,
      "step": 13500
    },
    {
      "epoch": 0.5434360686282121,
      "grad_norm": 4.905200004577637,
      "learning_rate": 1.6378904329374014e-05,
      "loss": 4.8878,
      "step": 14000
    },
    {
      "epoch": 0.5628444996506482,
      "grad_norm": 3.583787202835083,
      "learning_rate": 1.6249514789224442e-05,
      "loss": 4.8834,
      "step": 14500
    },
    {
      "epoch": 0.5822529306730844,
      "grad_norm": 4.404994487762451,
      "learning_rate": 1.6120125249074866e-05,
      "loss": 4.8527,
      "step": 15000
    },
    {
      "epoch": 0.6016613616955205,
      "grad_norm": 3.8764243125915527,
      "learning_rate": 1.599073570892529e-05,
      "loss": 4.8945,
      "step": 15500
    },
    {
      "epoch": 0.6210697927179567,
      "grad_norm": 6.267667770385742,
      "learning_rate": 1.586134616877572e-05,
      "loss": 4.8602,
      "step": 16000
    },
    {
      "epoch": 0.6404782237403929,
      "grad_norm": 4.009965419769287,
      "learning_rate": 1.5731956628626143e-05,
      "loss": 4.863,
      "step": 16500
    },
    {
      "epoch": 0.6598866547628289,
      "grad_norm": 3.580256938934326,
      "learning_rate": 1.5602567088476567e-05,
      "loss": 4.8768,
      "step": 17000
    },
    {
      "epoch": 0.6792950857852651,
      "grad_norm": 5.5662760734558105,
      "learning_rate": 1.5473436327407293e-05,
      "loss": 4.8725,
      "step": 17500
    },
    {
      "epoch": 0.6987035168077013,
      "grad_norm": 3.416393280029297,
      "learning_rate": 1.534404678725772e-05,
      "loss": 4.8737,
      "step": 18000
    },
    {
      "epoch": 0.7181119478301374,
      "grad_norm": 3.1308705806732178,
      "learning_rate": 1.5214916026188444e-05,
      "loss": 4.8617,
      "step": 18500
    },
    {
      "epoch": 0.7375203788525736,
      "grad_norm": 3.2318172454833984,
      "learning_rate": 1.508552648603887e-05,
      "loss": 4.8644,
      "step": 19000
    },
    {
      "epoch": 0.7569288098750097,
      "grad_norm": 2.86999773979187,
      "learning_rate": 1.4956136945889294e-05,
      "loss": 4.8451,
      "step": 19500
    },
    {
      "epoch": 0.7763372408974458,
      "grad_norm": 4.827009677886963,
      "learning_rate": 1.482674740573972e-05,
      "loss": 4.8598,
      "step": 20000
    },
    {
      "epoch": 0.795745671919882,
      "grad_norm": 3.603459358215332,
      "learning_rate": 1.4697357865590147e-05,
      "loss": 4.8578,
      "step": 20500
    },
    {
      "epoch": 0.8151541029423182,
      "grad_norm": 5.074081897735596,
      "learning_rate": 1.4567968325440574e-05,
      "loss": 4.8673,
      "step": 21000
    },
    {
      "epoch": 0.8345625339647543,
      "grad_norm": 3.7811038494110107,
      "learning_rate": 1.4438578785290997e-05,
      "loss": 4.8615,
      "step": 21500
    },
    {
      "epoch": 0.8539709649871904,
      "grad_norm": 2.9897711277008057,
      "learning_rate": 1.4309189245141423e-05,
      "loss": 4.8524,
      "step": 22000
    },
    {
      "epoch": 0.8733793960096266,
      "grad_norm": 4.158779621124268,
      "learning_rate": 1.417979970499185e-05,
      "loss": 4.8449,
      "step": 22500
    },
    {
      "epoch": 0.8927878270320627,
      "grad_norm": 3.920840263366699,
      "learning_rate": 1.4050410164842277e-05,
      "loss": 4.8488,
      "step": 23000
    },
    {
      "epoch": 0.9121962580544989,
      "grad_norm": 3.6061723232269287,
      "learning_rate": 1.39210206246927e-05,
      "loss": 4.8442,
      "step": 23500
    },
    {
      "epoch": 0.931604689076935,
      "grad_norm": 7.652431488037109,
      "learning_rate": 1.3791631084543126e-05,
      "loss": 4.8478,
      "step": 24000
    },
    {
      "epoch": 0.9510131200993711,
      "grad_norm": 5.96688175201416,
      "learning_rate": 1.3662241544393554e-05,
      "loss": 4.8517,
      "step": 24500
    },
    {
      "epoch": 0.9704215511218073,
      "grad_norm": 3.919732093811035,
      "learning_rate": 1.3533110783324277e-05,
      "loss": 4.8453,
      "step": 25000
    },
    {
      "epoch": 0.9898299821442434,
      "grad_norm": 4.069413661956787,
      "learning_rate": 1.3403721243174703e-05,
      "loss": 4.8473,
      "step": 25500
    },
    {
      "epoch": 1.0,
      "eval_loss": 4.8997955322265625,
      "eval_runtime": 489.9016,
      "eval_samples_per_second": 45.266,
      "eval_steps_per_second": 2.829,
      "step": 25762
    },
    {
      "epoch": 1.0092384131666796,
      "grad_norm": 4.559021949768066,
      "learning_rate": 1.327433170302513e-05,
      "loss": 4.8418,
      "step": 26000
    },
    {
      "epoch": 1.0286468441891157,
      "grad_norm": 3.7166647911071777,
      "learning_rate": 1.3144942162875554e-05,
      "loss": 4.8284,
      "step": 26500
    },
    {
      "epoch": 1.048055275211552,
      "grad_norm": 4.522943019866943,
      "learning_rate": 1.3015811401806279e-05,
      "loss": 4.8287,
      "step": 27000
    },
    {
      "epoch": 1.067463706233988,
      "grad_norm": 4.184176445007324,
      "learning_rate": 1.2886680640737004e-05,
      "loss": 4.8362,
      "step": 27500
    },
    {
      "epoch": 1.0868721372564243,
      "grad_norm": 3.894599676132202,
      "learning_rate": 1.275729110058743e-05,
      "loss": 4.8224,
      "step": 28000
    },
    {
      "epoch": 1.1062805682788603,
      "grad_norm": 4.857422828674316,
      "learning_rate": 1.2627901560437856e-05,
      "loss": 4.8124,
      "step": 28500
    },
    {
      "epoch": 1.1256889993012964,
      "grad_norm": 4.272945404052734,
      "learning_rate": 1.249851202028828e-05,
      "loss": 4.8367,
      "step": 29000
    },
    {
      "epoch": 1.1450974303237327,
      "grad_norm": 4.39076566696167,
      "learning_rate": 1.2369122480138707e-05,
      "loss": 4.819,
      "step": 29500
    },
    {
      "epoch": 1.1645058613461687,
      "grad_norm": 4.512894153594971,
      "learning_rate": 1.223999171906943e-05,
      "loss": 4.8283,
      "step": 30000
    },
    {
      "epoch": 1.183914292368605,
      "grad_norm": 7.498649597167969,
      "learning_rate": 1.2110602178919856e-05,
      "loss": 4.8301,
      "step": 30500
    },
    {
      "epoch": 1.203322723391041,
      "grad_norm": 4.55357551574707,
      "learning_rate": 1.1981212638770282e-05,
      "loss": 4.819,
      "step": 31000
    },
    {
      "epoch": 1.2227311544134771,
      "grad_norm": 3.008187770843506,
      "learning_rate": 1.1851823098620709e-05,
      "loss": 4.8129,
      "step": 31500
    },
    {
      "epoch": 1.2421395854359134,
      "grad_norm": 4.845812797546387,
      "learning_rate": 1.1722692337551434e-05,
      "loss": 4.8287,
      "step": 32000
    },
    {
      "epoch": 1.2615480164583495,
      "grad_norm": 5.356081008911133,
      "learning_rate": 1.159330279740186e-05,
      "loss": 4.8118,
      "step": 32500
    },
    {
      "epoch": 1.2809564474807855,
      "grad_norm": 5.9515557289123535,
      "learning_rate": 1.1463913257252284e-05,
      "loss": 4.8025,
      "step": 33000
    },
    {
      "epoch": 1.3003648785032218,
      "grad_norm": 4.917377948760986,
      "learning_rate": 1.133452371710271e-05,
      "loss": 4.7975,
      "step": 33500
    },
    {
      "epoch": 1.319773309525658,
      "grad_norm": 5.057592391967773,
      "learning_rate": 1.1205134176953137e-05,
      "loss": 4.8162,
      "step": 34000
    },
    {
      "epoch": 1.3391817405480941,
      "grad_norm": 3.34102201461792,
      "learning_rate": 1.1075744636803563e-05,
      "loss": 4.819,
      "step": 34500
    },
    {
      "epoch": 1.3585901715705302,
      "grad_norm": 5.530290126800537,
      "learning_rate": 1.0946355096653987e-05,
      "loss": 4.8097,
      "step": 35000
    },
    {
      "epoch": 1.3779986025929665,
      "grad_norm": 3.6972334384918213,
      "learning_rate": 1.0816965556504413e-05,
      "loss": 4.8227,
      "step": 35500
    },
    {
      "epoch": 1.3974070336154025,
      "grad_norm": 4.014361381530762,
      "learning_rate": 1.068757601635484e-05,
      "loss": 4.8027,
      "step": 36000
    },
    {
      "epoch": 1.4168154646378386,
      "grad_norm": 5.344301223754883,
      "learning_rate": 1.0558445255285563e-05,
      "loss": 4.8051,
      "step": 36500
    },
    {
      "epoch": 1.4362238956602749,
      "grad_norm": 4.027596473693848,
      "learning_rate": 1.0429055715135989e-05,
      "loss": 4.8044,
      "step": 37000
    },
    {
      "epoch": 1.455632326682711,
      "grad_norm": 4.645623207092285,
      "learning_rate": 1.0299666174986415e-05,
      "loss": 4.8065,
      "step": 37500
    },
    {
      "epoch": 1.4750407577051472,
      "grad_norm": 4.759020805358887,
      "learning_rate": 1.017027663483684e-05,
      "loss": 4.8092,
      "step": 38000
    },
    {
      "epoch": 1.4944491887275833,
      "grad_norm": 3.3040475845336914,
      "learning_rate": 1.0041145873767566e-05,
      "loss": 4.8094,
      "step": 38500
    },
    {
      "epoch": 1.5138576197500195,
      "grad_norm": 4.919401168823242,
      "learning_rate": 9.91175633361799e-06,
      "loss": 4.8213,
      "step": 39000
    },
    {
      "epoch": 1.5332660507724556,
      "grad_norm": 4.3448991775512695,
      "learning_rate": 9.782366793468417e-06,
      "loss": 4.8005,
      "step": 39500
    },
    {
      "epoch": 1.5526744817948916,
      "grad_norm": 4.914795875549316,
      "learning_rate": 9.652977253318843e-06,
      "loss": 4.8032,
      "step": 40000
    },
    {
      "epoch": 1.572082912817328,
      "grad_norm": 3.5400009155273438,
      "learning_rate": 9.523846492249568e-06,
      "loss": 4.8147,
      "step": 40500
    },
    {
      "epoch": 1.591491343839764,
      "grad_norm": 4.673180103302002,
      "learning_rate": 9.394456952099992e-06,
      "loss": 4.8125,
      "step": 41000
    },
    {
      "epoch": 1.6108997748622,
      "grad_norm": 3.7648890018463135,
      "learning_rate": 9.265067411950418e-06,
      "loss": 4.7948,
      "step": 41500
    },
    {
      "epoch": 1.6303082058846363,
      "grad_norm": 5.1210408210754395,
      "learning_rate": 9.135677871800845e-06,
      "loss": 4.8025,
      "step": 42000
    },
    {
      "epoch": 1.6497166369070726,
      "grad_norm": 4.430244445800781,
      "learning_rate": 9.00654711073157e-06,
      "loss": 4.7882,
      "step": 42500
    },
    {
      "epoch": 1.6691250679295084,
      "grad_norm": 4.885284900665283,
      "learning_rate": 8.877157570581994e-06,
      "loss": 4.8112,
      "step": 43000
    },
    {
      "epoch": 1.6885334989519447,
      "grad_norm": 4.94572639465332,
      "learning_rate": 8.74776803043242e-06,
      "loss": 4.7982,
      "step": 43500
    },
    {
      "epoch": 1.707941929974381,
      "grad_norm": 3.8920679092407227,
      "learning_rate": 8.618378490282846e-06,
      "loss": 4.8176,
      "step": 44000
    },
    {
      "epoch": 1.727350360996817,
      "grad_norm": 4.163534164428711,
      "learning_rate": 8.488988950133272e-06,
      "loss": 4.7906,
      "step": 44500
    },
    {
      "epoch": 1.746758792019253,
      "grad_norm": 4.738983154296875,
      "learning_rate": 8.359858189063996e-06,
      "loss": 4.7946,
      "step": 45000
    },
    {
      "epoch": 1.7661672230416894,
      "grad_norm": 4.410732746124268,
      "learning_rate": 8.230468648914422e-06,
      "loss": 4.796,
      "step": 45500
    },
    {
      "epoch": 1.7855756540641254,
      "grad_norm": 5.760485649108887,
      "learning_rate": 8.101079108764848e-06,
      "loss": 4.7931,
      "step": 46000
    },
    {
      "epoch": 1.8049840850865615,
      "grad_norm": 3.6736419200897217,
      "learning_rate": 7.971689568615274e-06,
      "loss": 4.7963,
      "step": 46500
    },
    {
      "epoch": 1.8243925161089978,
      "grad_norm": 3.7348740100860596,
      "learning_rate": 7.842558807546e-06,
      "loss": 4.8134,
      "step": 47000
    },
    {
      "epoch": 1.843800947131434,
      "grad_norm": 6.82490348815918,
      "learning_rate": 7.713169267396424e-06,
      "loss": 4.7866,
      "step": 47500
    },
    {
      "epoch": 1.8632093781538699,
      "grad_norm": 4.694186687469482,
      "learning_rate": 7.583779727246851e-06,
      "loss": 4.7901,
      "step": 48000
    },
    {
      "epoch": 1.8826178091763062,
      "grad_norm": 4.843177795410156,
      "learning_rate": 7.454390187097276e-06,
      "loss": 4.7864,
      "step": 48500
    },
    {
      "epoch": 1.9020262401987424,
      "grad_norm": 5.365417003631592,
      "learning_rate": 7.3250006469477005e-06,
      "loss": 4.7911,
      "step": 49000
    },
    {
      "epoch": 1.9214346712211785,
      "grad_norm": 4.942513942718506,
      "learning_rate": 7.195869885878426e-06,
      "loss": 4.7946,
      "step": 49500
    },
    {
      "epoch": 1.9408431022436146,
      "grad_norm": 7.173553943634033,
      "learning_rate": 7.066739124809151e-06,
      "loss": 4.8052,
      "step": 50000
    },
    {
      "epoch": 1.9602515332660508,
      "grad_norm": 5.093269348144531,
      "learning_rate": 6.937349584659576e-06,
      "loss": 4.7882,
      "step": 50500
    },
    {
      "epoch": 1.979659964288487,
      "grad_norm": 6.274416923522949,
      "learning_rate": 6.807960044510002e-06,
      "loss": 4.7739,
      "step": 51000
    },
    {
      "epoch": 1.999068395310923,
      "grad_norm": 5.963187217712402,
      "learning_rate": 6.678570504360427e-06,
      "loss": 4.8038,
      "step": 51500
    },
    {
      "epoch": 2.0,
      "eval_loss": 4.866925239562988,
      "eval_runtime": 488.2835,
      "eval_samples_per_second": 45.416,
      "eval_steps_per_second": 2.839,
      "step": 51524
    },
    {
      "epoch": 2.0184768263333592,
      "grad_norm": 3.9758663177490234,
      "learning_rate": 6.549180964210854e-06,
      "loss": 4.7932,
      "step": 52000
    },
    {
      "epoch": 2.0378852573557955,
      "grad_norm": 3.885803699493408,
      "learning_rate": 6.419791424061279e-06,
      "loss": 4.7876,
      "step": 52500
    },
    {
      "epoch": 2.0572936883782313,
      "grad_norm": 3.955672264099121,
      "learning_rate": 6.290401883911706e-06,
      "loss": 4.7683,
      "step": 53000
    },
    {
      "epoch": 2.0767021194006676,
      "grad_norm": 6.21928596496582,
      "learning_rate": 6.161271122842431e-06,
      "loss": 4.7769,
      "step": 53500
    },
    {
      "epoch": 2.096110550423104,
      "grad_norm": 5.843352317810059,
      "learning_rate": 6.031881582692855e-06,
      "loss": 4.7796,
      "step": 54000
    },
    {
      "epoch": 2.1155189814455397,
      "grad_norm": 6.880984306335449,
      "learning_rate": 5.9024920425432805e-06,
      "loss": 4.7768,
      "step": 54500
    },
    {
      "epoch": 2.134927412467976,
      "grad_norm": 3.4838833808898926,
      "learning_rate": 5.773102502393707e-06,
      "loss": 4.7775,
      "step": 55000
    },
    {
      "epoch": 2.1543358434904123,
      "grad_norm": 4.513483047485352,
      "learning_rate": 5.643712962244132e-06,
      "loss": 4.7846,
      "step": 55500
    },
    {
      "epoch": 2.1737442745128486,
      "grad_norm": 4.099808692932129,
      "learning_rate": 5.514323422094559e-06,
      "loss": 4.7745,
      "step": 56000
    },
    {
      "epoch": 2.1931527055352844,
      "grad_norm": 4.004847526550293,
      "learning_rate": 5.384933881944983e-06,
      "loss": 4.7758,
      "step": 56500
    },
    {
      "epoch": 2.2125611365577207,
      "grad_norm": 5.439427852630615,
      "learning_rate": 5.25554434179541e-06,
      "loss": 4.79,
      "step": 57000
    },
    {
      "epoch": 2.231969567580157,
      "grad_norm": 5.015589237213135,
      "learning_rate": 5.1261548016458355e-06,
      "loss": 4.7712,
      "step": 57500
    },
    {
      "epoch": 2.251377998602593,
      "grad_norm": 3.8506691455841064,
      "learning_rate": 4.996765261496261e-06,
      "loss": 4.7895,
      "step": 58000
    },
    {
      "epoch": 2.270786429625029,
      "grad_norm": 4.847903728485107,
      "learning_rate": 4.867375721346687e-06,
      "loss": 4.7663,
      "step": 58500
    },
    {
      "epoch": 2.2901948606474654,
      "grad_norm": 5.129690647125244,
      "learning_rate": 4.737986181197112e-06,
      "loss": 4.7692,
      "step": 59000
    },
    {
      "epoch": 2.3096032916699016,
      "grad_norm": 4.483580589294434,
      "learning_rate": 4.609114199208136e-06,
      "loss": 4.7608,
      "step": 59500
    },
    {
      "epoch": 2.3290117226923375,
      "grad_norm": 4.348310470581055,
      "learning_rate": 4.479724659058562e-06,
      "loss": 4.783,
      "step": 60000
    },
    {
      "epoch": 2.3484201537147738,
      "grad_norm": 4.473496913909912,
      "learning_rate": 4.350335118908988e-06,
      "loss": 4.7641,
      "step": 60500
    },
    {
      "epoch": 2.36782858473721,
      "grad_norm": 6.0381083488464355,
      "learning_rate": 4.220945578759413e-06,
      "loss": 4.7763,
      "step": 61000
    },
    {
      "epoch": 2.387237015759646,
      "grad_norm": 4.95905065536499,
      "learning_rate": 4.091556038609839e-06,
      "loss": 4.7566,
      "step": 61500
    },
    {
      "epoch": 2.406645446782082,
      "grad_norm": 4.568081378936768,
      "learning_rate": 3.962166498460265e-06,
      "loss": 4.7784,
      "step": 62000
    },
    {
      "epoch": 2.4260538778045184,
      "grad_norm": 5.64059591293335,
      "learning_rate": 3.8327769583106904e-06,
      "loss": 4.7878,
      "step": 62500
    },
    {
      "epoch": 2.4454623088269543,
      "grad_norm": 5.462943077087402,
      "learning_rate": 3.703387418161116e-06,
      "loss": 4.7706,
      "step": 63000
    },
    {
      "epoch": 2.4648707398493905,
      "grad_norm": 8.366272926330566,
      "learning_rate": 3.574256657091841e-06,
      "loss": 4.7704,
      "step": 63500
    },
    {
      "epoch": 2.484279170871827,
      "grad_norm": 5.188848972320557,
      "learning_rate": 3.445125896022566e-06,
      "loss": 4.7779,
      "step": 64000
    },
    {
      "epoch": 2.5036876018942626,
      "grad_norm": 6.389596939086914,
      "learning_rate": 3.3157363558729917e-06,
      "loss": 4.7777,
      "step": 64500
    },
    {
      "epoch": 2.523096032916699,
      "grad_norm": 5.19334602355957,
      "learning_rate": 3.1863468157234174e-06,
      "loss": 4.7731,
      "step": 65000
    },
    {
      "epoch": 2.542504463939135,
      "grad_norm": 5.416764259338379,
      "learning_rate": 3.056957275573843e-06,
      "loss": 4.7603,
      "step": 65500
    },
    {
      "epoch": 2.561912894961571,
      "grad_norm": 6.572749614715576,
      "learning_rate": 2.9275677354242687e-06,
      "loss": 4.7678,
      "step": 66000
    },
    {
      "epoch": 2.5813213259840073,
      "grad_norm": 4.910022258758545,
      "learning_rate": 2.7981781952746944e-06,
      "loss": 4.7677,
      "step": 66500
    },
    {
      "epoch": 2.6007297570064436,
      "grad_norm": 5.388822078704834,
      "learning_rate": 2.6687886551251197e-06,
      "loss": 4.7586,
      "step": 67000
    },
    {
      "epoch": 2.62013818802888,
      "grad_norm": 5.110483646392822,
      "learning_rate": 2.5393991149755454e-06,
      "loss": 4.7847,
      "step": 67500
    },
    {
      "epoch": 2.639546619051316,
      "grad_norm": 5.22611665725708,
      "learning_rate": 2.4102683539062705e-06,
      "loss": 4.7814,
      "step": 68000
    },
    {
      "epoch": 2.658955050073752,
      "grad_norm": 4.518706321716309,
      "learning_rate": 2.2811375928369952e-06,
      "loss": 4.7815,
      "step": 68500
    },
    {
      "epoch": 2.6783634810961883,
      "grad_norm": 5.832429885864258,
      "learning_rate": 2.15200683176772e-06,
      "loss": 4.7597,
      "step": 69000
    },
    {
      "epoch": 2.6977719121186245,
      "grad_norm": 5.588045120239258,
      "learning_rate": 2.0226172916181456e-06,
      "loss": 4.7722,
      "step": 69500
    },
    {
      "epoch": 2.7171803431410604,
      "grad_norm": 3.409862518310547,
      "learning_rate": 1.8932277514685715e-06,
      "loss": 4.7603,
      "step": 70000
    },
    {
      "epoch": 2.7365887741634967,
      "grad_norm": 5.297553539276123,
      "learning_rate": 1.7638382113189972e-06,
      "loss": 4.7609,
      "step": 70500
    },
    {
      "epoch": 2.755997205185933,
      "grad_norm": 6.902538776397705,
      "learning_rate": 1.6344486711694227e-06,
      "loss": 4.7633,
      "step": 71000
    },
    {
      "epoch": 2.7754056362083688,
      "grad_norm": 3.745264768600464,
      "learning_rate": 1.5050591310198484e-06,
      "loss": 4.757,
      "step": 71500
    },
    {
      "epoch": 2.794814067230805,
      "grad_norm": 7.204573631286621,
      "learning_rate": 1.375669590870274e-06,
      "loss": 4.7754,
      "step": 72000
    },
    {
      "epoch": 2.8142224982532413,
      "grad_norm": 5.0786027908325195,
      "learning_rate": 1.2462800507206998e-06,
      "loss": 4.7565,
      "step": 72500
    },
    {
      "epoch": 2.833630929275677,
      "grad_norm": 6.400927543640137,
      "learning_rate": 1.1168905105711257e-06,
      "loss": 4.751,
      "step": 73000
    },
    {
      "epoch": 2.8530393602981134,
      "grad_norm": 4.606122016906738,
      "learning_rate": 9.875009704215511e-07,
      "loss": 4.7722,
      "step": 73500
    },
    {
      "epoch": 2.8724477913205497,
      "grad_norm": 5.706255912780762,
      "learning_rate": 8.581114302719769e-07,
      "loss": 4.7643,
      "step": 74000
    },
    {
      "epoch": 2.8918562223429856,
      "grad_norm": 4.149639129638672,
      "learning_rate": 7.289806692027017e-07,
      "loss": 4.7552,
      "step": 74500
    },
    {
      "epoch": 2.911264653365422,
      "grad_norm": 4.252857685089111,
      "learning_rate": 5.995911290531274e-07,
      "loss": 4.7756,
      "step": 75000
    },
    {
      "epoch": 2.930673084387858,
      "grad_norm": 5.212728977203369,
      "learning_rate": 4.702015889035531e-07,
      "loss": 4.7477,
      "step": 75500
    },
    {
      "epoch": 2.9500815154102944,
      "grad_norm": 3.750203847885132,
      "learning_rate": 3.4081204875397876e-07,
      "loss": 4.7601,
      "step": 76000
    },
    {
      "epoch": 2.9694899464327302,
      "grad_norm": 5.8782172203063965,
      "learning_rate": 2.1142250860440445e-07,
      "loss": 4.7661,
      "step": 76500
    },
    {
      "epoch": 2.9888983774551665,
      "grad_norm": 3.9091098308563232,
      "learning_rate": 8.203296845483011e-08,
      "loss": 4.7687,
      "step": 77000
    },
    {
      "epoch": 3.0,
      "eval_loss": 4.8571858406066895,
      "eval_runtime": 4559.552,
      "eval_samples_per_second": 4.864,
      "eval_steps_per_second": 0.304,
      "step": 77286
    }
  ],
  "logging_steps": 500,
  "max_steps": 77286,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.282683773341401e+18,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
