{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 51524,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.019408431022436147,
      "grad_norm": 3.0156495571136475,
      "learning_rate": 1.9871904355251923e-05,
      "loss": 6.0953,
      "step": 500
    },
    {
      "epoch": 0.038816862044872294,
      "grad_norm": 2.5376834869384766,
      "learning_rate": 1.974251481510235e-05,
      "loss": 5.175,
      "step": 1000
    },
    {
      "epoch": 0.05822529306730844,
      "grad_norm": 3.942051887512207,
      "learning_rate": 1.9613125274952772e-05,
      "loss": 5.1,
      "step": 1500
    },
    {
      "epoch": 0.07763372408974459,
      "grad_norm": 4.331577777862549,
      "learning_rate": 1.94837357348032e-05,
      "loss": 5.0535,
      "step": 2000
    },
    {
      "epoch": 0.09704215511218073,
      "grad_norm": 3.4310264587402344,
      "learning_rate": 1.9354346194653627e-05,
      "loss": 5.051,
      "step": 2500
    },
    {
      "epoch": 0.11645058613461688,
      "grad_norm": 3.5291943550109863,
      "learning_rate": 1.9224956654504052e-05,
      "loss": 5.0154,
      "step": 3000
    },
    {
      "epoch": 0.13585901715705304,
      "grad_norm": 4.405115604400635,
      "learning_rate": 1.9095567114354476e-05,
      "loss": 4.9982,
      "step": 3500
    },
    {
      "epoch": 0.15526744817948918,
      "grad_norm": 5.135713577270508,
      "learning_rate": 1.8966177574204904e-05,
      "loss": 4.9702,
      "step": 4000
    },
    {
      "epoch": 0.17467587920192532,
      "grad_norm": 3.2893805503845215,
      "learning_rate": 1.883678803405533e-05,
      "loss": 4.9703,
      "step": 4500
    },
    {
      "epoch": 0.19408431022436146,
      "grad_norm": 6.493264675140381,
      "learning_rate": 1.8707398493905756e-05,
      "loss": 4.9742,
      "step": 5000
    },
    {
      "epoch": 0.2134927412467976,
      "grad_norm": 4.357235431671143,
      "learning_rate": 1.857800895375618e-05,
      "loss": 4.9568,
      "step": 5500
    },
    {
      "epoch": 0.23290117226923376,
      "grad_norm": 2.8122124671936035,
      "learning_rate": 1.8448619413606605e-05,
      "loss": 4.9517,
      "step": 6000
    },
    {
      "epoch": 0.2523096032916699,
      "grad_norm": 3.5628552436828613,
      "learning_rate": 1.8319229873457033e-05,
      "loss": 4.9229,
      "step": 6500
    },
    {
      "epoch": 0.27171803431410607,
      "grad_norm": 3.464205503463745,
      "learning_rate": 1.8189840333307457e-05,
      "loss": 4.9347,
      "step": 7000
    },
    {
      "epoch": 0.2911264653365422,
      "grad_norm": 3.894460916519165,
      "learning_rate": 1.806045079315788e-05,
      "loss": 4.9191,
      "step": 7500
    },
    {
      "epoch": 0.31053489635897835,
      "grad_norm": 3.4287831783294678,
      "learning_rate": 1.793106125300831e-05,
      "loss": 4.926,
      "step": 8000
    },
    {
      "epoch": 0.32994332738141446,
      "grad_norm": 4.080543518066406,
      "learning_rate": 1.7801671712858734e-05,
      "loss": 4.9238,
      "step": 8500
    },
    {
      "epoch": 0.34935175840385063,
      "grad_norm": 3.5280754566192627,
      "learning_rate": 1.767228217270916e-05,
      "loss": 4.9193,
      "step": 9000
    },
    {
      "epoch": 0.3687601894262868,
      "grad_norm": 4.128464221954346,
      "learning_rate": 1.7542892632559586e-05,
      "loss": 4.9027,
      "step": 9500
    },
    {
      "epoch": 0.3881686204487229,
      "grad_norm": 4.004984378814697,
      "learning_rate": 1.741376187149031e-05,
      "loss": 4.8945,
      "step": 10000
    },
    {
      "epoch": 0.4075770514711591,
      "grad_norm": 3.562501907348633,
      "learning_rate": 1.7284372331340736e-05,
      "loss": 4.9077,
      "step": 10500
    },
    {
      "epoch": 0.4269854824935952,
      "grad_norm": 3.1492257118225098,
      "learning_rate": 1.715498279119116e-05,
      "loss": 4.8967,
      "step": 11000
    },
    {
      "epoch": 0.44639391351603136,
      "grad_norm": 4.780231952667236,
      "learning_rate": 1.7025593251041588e-05,
      "loss": 4.8736,
      "step": 11500
    },
    {
      "epoch": 0.4658023445384675,
      "grad_norm": 3.2589855194091797,
      "learning_rate": 1.6896203710892012e-05,
      "loss": 4.8896,
      "step": 12000
    },
    {
      "epoch": 0.48521077556090364,
      "grad_norm": 3.1079773902893066,
      "learning_rate": 1.6766814170742437e-05,
      "loss": 4.8893,
      "step": 12500
    },
    {
      "epoch": 0.5046192065833398,
      "grad_norm": 3.26347279548645,
      "learning_rate": 1.6637424630592865e-05,
      "loss": 4.8899,
      "step": 13000
    },
    {
      "epoch": 0.524027637605776,
      "grad_norm": 3.2418606281280518,
      "learning_rate": 1.6508035090443292e-05,
      "loss": 4.9097,
      "step": 13500
    },
    {
      "epoch": 0.5434360686282121,
      "grad_norm": 4.905200004577637,
      "learning_rate": 1.6378904329374014e-05,
      "loss": 4.8878,
      "step": 14000
    },
    {
      "epoch": 0.5628444996506482,
      "grad_norm": 3.583787202835083,
      "learning_rate": 1.6249514789224442e-05,
      "loss": 4.8834,
      "step": 14500
    },
    {
      "epoch": 0.5822529306730844,
      "grad_norm": 4.404994487762451,
      "learning_rate": 1.6120125249074866e-05,
      "loss": 4.8527,
      "step": 15000
    },
    {
      "epoch": 0.6016613616955205,
      "grad_norm": 3.8764243125915527,
      "learning_rate": 1.599073570892529e-05,
      "loss": 4.8945,
      "step": 15500
    },
    {
      "epoch": 0.6210697927179567,
      "grad_norm": 6.267667770385742,
      "learning_rate": 1.586134616877572e-05,
      "loss": 4.8602,
      "step": 16000
    },
    {
      "epoch": 0.6404782237403929,
      "grad_norm": 4.009965419769287,
      "learning_rate": 1.5731956628626143e-05,
      "loss": 4.863,
      "step": 16500
    },
    {
      "epoch": 0.6598866547628289,
      "grad_norm": 3.580256938934326,
      "learning_rate": 1.5602567088476567e-05,
      "loss": 4.8768,
      "step": 17000
    },
    {
      "epoch": 0.6792950857852651,
      "grad_norm": 5.5662760734558105,
      "learning_rate": 1.5473436327407293e-05,
      "loss": 4.8725,
      "step": 17500
    },
    {
      "epoch": 0.6987035168077013,
      "grad_norm": 3.416393280029297,
      "learning_rate": 1.534404678725772e-05,
      "loss": 4.8737,
      "step": 18000
    },
    {
      "epoch": 0.7181119478301374,
      "grad_norm": 3.1308705806732178,
      "learning_rate": 1.5214916026188444e-05,
      "loss": 4.8617,
      "step": 18500
    },
    {
      "epoch": 0.7375203788525736,
      "grad_norm": 3.2318172454833984,
      "learning_rate": 1.508552648603887e-05,
      "loss": 4.8644,
      "step": 19000
    },
    {
      "epoch": 0.7569288098750097,
      "grad_norm": 2.86999773979187,
      "learning_rate": 1.4956136945889294e-05,
      "loss": 4.8451,
      "step": 19500
    },
    {
      "epoch": 0.7763372408974458,
      "grad_norm": 4.827009677886963,
      "learning_rate": 1.482674740573972e-05,
      "loss": 4.8598,
      "step": 20000
    },
    {
      "epoch": 0.795745671919882,
      "grad_norm": 3.603459358215332,
      "learning_rate": 1.4697357865590147e-05,
      "loss": 4.8578,
      "step": 20500
    },
    {
      "epoch": 0.8151541029423182,
      "grad_norm": 5.074081897735596,
      "learning_rate": 1.4567968325440574e-05,
      "loss": 4.8673,
      "step": 21000
    },
    {
      "epoch": 0.8345625339647543,
      "grad_norm": 3.7811038494110107,
      "learning_rate": 1.4438578785290997e-05,
      "loss": 4.8615,
      "step": 21500
    },
    {
      "epoch": 0.8539709649871904,
      "grad_norm": 2.9897711277008057,
      "learning_rate": 1.4309189245141423e-05,
      "loss": 4.8524,
      "step": 22000
    },
    {
      "epoch": 0.8733793960096266,
      "grad_norm": 4.158779621124268,
      "learning_rate": 1.417979970499185e-05,
      "loss": 4.8449,
      "step": 22500
    },
    {
      "epoch": 0.8927878270320627,
      "grad_norm": 3.920840263366699,
      "learning_rate": 1.4050410164842277e-05,
      "loss": 4.8488,
      "step": 23000
    },
    {
      "epoch": 0.9121962580544989,
      "grad_norm": 3.6061723232269287,
      "learning_rate": 1.39210206246927e-05,
      "loss": 4.8442,
      "step": 23500
    },
    {
      "epoch": 0.931604689076935,
      "grad_norm": 7.652431488037109,
      "learning_rate": 1.3791631084543126e-05,
      "loss": 4.8478,
      "step": 24000
    },
    {
      "epoch": 0.9510131200993711,
      "grad_norm": 5.96688175201416,
      "learning_rate": 1.3662241544393554e-05,
      "loss": 4.8517,
      "step": 24500
    },
    {
      "epoch": 0.9704215511218073,
      "grad_norm": 3.919732093811035,
      "learning_rate": 1.3533110783324277e-05,
      "loss": 4.8453,
      "step": 25000
    },
    {
      "epoch": 0.9898299821442434,
      "grad_norm": 4.069413661956787,
      "learning_rate": 1.3403721243174703e-05,
      "loss": 4.8473,
      "step": 25500
    },
    {
      "epoch": 1.0,
      "eval_loss": 4.8997955322265625,
      "eval_runtime": 489.9016,
      "eval_samples_per_second": 45.266,
      "eval_steps_per_second": 2.829,
      "step": 25762
    },
    {
      "epoch": 1.0092384131666796,
      "grad_norm": 4.559021949768066,
      "learning_rate": 1.327433170302513e-05,
      "loss": 4.8418,
      "step": 26000
    },
    {
      "epoch": 1.0286468441891157,
      "grad_norm": 3.7166647911071777,
      "learning_rate": 1.3144942162875554e-05,
      "loss": 4.8284,
      "step": 26500
    },
    {
      "epoch": 1.048055275211552,
      "grad_norm": 4.522943019866943,
      "learning_rate": 1.3015811401806279e-05,
      "loss": 4.8287,
      "step": 27000
    },
    {
      "epoch": 1.067463706233988,
      "grad_norm": 4.184176445007324,
      "learning_rate": 1.2886680640737004e-05,
      "loss": 4.8362,
      "step": 27500
    },
    {
      "epoch": 1.0868721372564243,
      "grad_norm": 3.894599676132202,
      "learning_rate": 1.275729110058743e-05,
      "loss": 4.8224,
      "step": 28000
    },
    {
      "epoch": 1.1062805682788603,
      "grad_norm": 4.857422828674316,
      "learning_rate": 1.2627901560437856e-05,
      "loss": 4.8124,
      "step": 28500
    },
    {
      "epoch": 1.1256889993012964,
      "grad_norm": 4.272945404052734,
      "learning_rate": 1.249851202028828e-05,
      "loss": 4.8367,
      "step": 29000
    },
    {
      "epoch": 1.1450974303237327,
      "grad_norm": 4.39076566696167,
      "learning_rate": 1.2369122480138707e-05,
      "loss": 4.819,
      "step": 29500
    },
    {
      "epoch": 1.1645058613461687,
      "grad_norm": 4.512894153594971,
      "learning_rate": 1.223999171906943e-05,
      "loss": 4.8283,
      "step": 30000
    },
    {
      "epoch": 1.183914292368605,
      "grad_norm": 7.498649597167969,
      "learning_rate": 1.2110602178919856e-05,
      "loss": 4.8301,
      "step": 30500
    },
    {
      "epoch": 1.203322723391041,
      "grad_norm": 4.55357551574707,
      "learning_rate": 1.1981212638770282e-05,
      "loss": 4.819,
      "step": 31000
    },
    {
      "epoch": 1.2227311544134771,
      "grad_norm": 3.008187770843506,
      "learning_rate": 1.1851823098620709e-05,
      "loss": 4.8129,
      "step": 31500
    },
    {
      "epoch": 1.2421395854359134,
      "grad_norm": 4.845812797546387,
      "learning_rate": 1.1722692337551434e-05,
      "loss": 4.8287,
      "step": 32000
    },
    {
      "epoch": 1.2615480164583495,
      "grad_norm": 5.356081008911133,
      "learning_rate": 1.159330279740186e-05,
      "loss": 4.8118,
      "step": 32500
    },
    {
      "epoch": 1.2809564474807855,
      "grad_norm": 5.9515557289123535,
      "learning_rate": 1.1463913257252284e-05,
      "loss": 4.8025,
      "step": 33000
    },
    {
      "epoch": 1.3003648785032218,
      "grad_norm": 4.917377948760986,
      "learning_rate": 1.133452371710271e-05,
      "loss": 4.7975,
      "step": 33500
    },
    {
      "epoch": 1.319773309525658,
      "grad_norm": 5.057592391967773,
      "learning_rate": 1.1205134176953137e-05,
      "loss": 4.8162,
      "step": 34000
    },
    {
      "epoch": 1.3391817405480941,
      "grad_norm": 3.34102201461792,
      "learning_rate": 1.1075744636803563e-05,
      "loss": 4.819,
      "step": 34500
    },
    {
      "epoch": 1.3585901715705302,
      "grad_norm": 5.530290126800537,
      "learning_rate": 1.0946355096653987e-05,
      "loss": 4.8097,
      "step": 35000
    },
    {
      "epoch": 1.3779986025929665,
      "grad_norm": 3.6972334384918213,
      "learning_rate": 1.0816965556504413e-05,
      "loss": 4.8227,
      "step": 35500
    },
    {
      "epoch": 1.3974070336154025,
      "grad_norm": 4.014361381530762,
      "learning_rate": 1.068757601635484e-05,
      "loss": 4.8027,
      "step": 36000
    },
    {
      "epoch": 1.4168154646378386,
      "grad_norm": 5.344301223754883,
      "learning_rate": 1.0558445255285563e-05,
      "loss": 4.8051,
      "step": 36500
    },
    {
      "epoch": 1.4362238956602749,
      "grad_norm": 4.027596473693848,
      "learning_rate": 1.0429055715135989e-05,
      "loss": 4.8044,
      "step": 37000
    },
    {
      "epoch": 1.455632326682711,
      "grad_norm": 4.645623207092285,
      "learning_rate": 1.0299666174986415e-05,
      "loss": 4.8065,
      "step": 37500
    },
    {
      "epoch": 1.4750407577051472,
      "grad_norm": 4.759020805358887,
      "learning_rate": 1.017027663483684e-05,
      "loss": 4.8092,
      "step": 38000
    },
    {
      "epoch": 1.4944491887275833,
      "grad_norm": 3.3040475845336914,
      "learning_rate": 1.0041145873767566e-05,
      "loss": 4.8094,
      "step": 38500
    },
    {
      "epoch": 1.5138576197500195,
      "grad_norm": 4.919401168823242,
      "learning_rate": 9.91175633361799e-06,
      "loss": 4.8213,
      "step": 39000
    },
    {
      "epoch": 1.5332660507724556,
      "grad_norm": 4.3448991775512695,
      "learning_rate": 9.782366793468417e-06,
      "loss": 4.8005,
      "step": 39500
    },
    {
      "epoch": 1.5526744817948916,
      "grad_norm": 4.914795875549316,
      "learning_rate": 9.652977253318843e-06,
      "loss": 4.8032,
      "step": 40000
    },
    {
      "epoch": 1.572082912817328,
      "grad_norm": 3.5400009155273438,
      "learning_rate": 9.523846492249568e-06,
      "loss": 4.8147,
      "step": 40500
    },
    {
      "epoch": 1.591491343839764,
      "grad_norm": 4.673180103302002,
      "learning_rate": 9.394456952099992e-06,
      "loss": 4.8125,
      "step": 41000
    },
    {
      "epoch": 1.6108997748622,
      "grad_norm": 3.7648890018463135,
      "learning_rate": 9.265067411950418e-06,
      "loss": 4.7948,
      "step": 41500
    },
    {
      "epoch": 1.6303082058846363,
      "grad_norm": 5.1210408210754395,
      "learning_rate": 9.135677871800845e-06,
      "loss": 4.8025,
      "step": 42000
    },
    {
      "epoch": 1.6497166369070726,
      "grad_norm": 4.430244445800781,
      "learning_rate": 9.00654711073157e-06,
      "loss": 4.7882,
      "step": 42500
    },
    {
      "epoch": 1.6691250679295084,
      "grad_norm": 4.885284900665283,
      "learning_rate": 8.877157570581994e-06,
      "loss": 4.8112,
      "step": 43000
    },
    {
      "epoch": 1.6885334989519447,
      "grad_norm": 4.94572639465332,
      "learning_rate": 8.74776803043242e-06,
      "loss": 4.7982,
      "step": 43500
    },
    {
      "epoch": 1.707941929974381,
      "grad_norm": 3.8920679092407227,
      "learning_rate": 8.618378490282846e-06,
      "loss": 4.8176,
      "step": 44000
    },
    {
      "epoch": 1.727350360996817,
      "grad_norm": 4.163534164428711,
      "learning_rate": 8.488988950133272e-06,
      "loss": 4.7906,
      "step": 44500
    },
    {
      "epoch": 1.746758792019253,
      "grad_norm": 4.738983154296875,
      "learning_rate": 8.359858189063996e-06,
      "loss": 4.7946,
      "step": 45000
    },
    {
      "epoch": 1.7661672230416894,
      "grad_norm": 4.410732746124268,
      "learning_rate": 8.230468648914422e-06,
      "loss": 4.796,
      "step": 45500
    },
    {
      "epoch": 1.7855756540641254,
      "grad_norm": 5.760485649108887,
      "learning_rate": 8.101079108764848e-06,
      "loss": 4.7931,
      "step": 46000
    },
    {
      "epoch": 1.8049840850865615,
      "grad_norm": 3.6736419200897217,
      "learning_rate": 7.971689568615274e-06,
      "loss": 4.7963,
      "step": 46500
    },
    {
      "epoch": 1.8243925161089978,
      "grad_norm": 3.7348740100860596,
      "learning_rate": 7.842558807546e-06,
      "loss": 4.8134,
      "step": 47000
    },
    {
      "epoch": 1.843800947131434,
      "grad_norm": 6.82490348815918,
      "learning_rate": 7.713169267396424e-06,
      "loss": 4.7866,
      "step": 47500
    },
    {
      "epoch": 1.8632093781538699,
      "grad_norm": 4.694186687469482,
      "learning_rate": 7.583779727246851e-06,
      "loss": 4.7901,
      "step": 48000
    },
    {
      "epoch": 1.8826178091763062,
      "grad_norm": 4.843177795410156,
      "learning_rate": 7.454390187097276e-06,
      "loss": 4.7864,
      "step": 48500
    },
    {
      "epoch": 1.9020262401987424,
      "grad_norm": 5.365417003631592,
      "learning_rate": 7.3250006469477005e-06,
      "loss": 4.7911,
      "step": 49000
    },
    {
      "epoch": 1.9214346712211785,
      "grad_norm": 4.942513942718506,
      "learning_rate": 7.195869885878426e-06,
      "loss": 4.7946,
      "step": 49500
    },
    {
      "epoch": 1.9408431022436146,
      "grad_norm": 7.173553943634033,
      "learning_rate": 7.066739124809151e-06,
      "loss": 4.8052,
      "step": 50000
    },
    {
      "epoch": 1.9602515332660508,
      "grad_norm": 5.093269348144531,
      "learning_rate": 6.937349584659576e-06,
      "loss": 4.7882,
      "step": 50500
    },
    {
      "epoch": 1.979659964288487,
      "grad_norm": 6.274416923522949,
      "learning_rate": 6.807960044510002e-06,
      "loss": 4.7739,
      "step": 51000
    },
    {
      "epoch": 1.999068395310923,
      "grad_norm": 5.963187217712402,
      "learning_rate": 6.678570504360427e-06,
      "loss": 4.8038,
      "step": 51500
    },
    {
      "epoch": 2.0,
      "eval_loss": 4.866925239562988,
      "eval_runtime": 488.2835,
      "eval_samples_per_second": 45.416,
      "eval_steps_per_second": 2.839,
      "step": 51524
    }
  ],
  "logging_steps": 500,
  "max_steps": 77286,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.1884558488942674e+18,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
