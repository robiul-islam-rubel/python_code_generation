{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9997780244173141,
  "eval_steps": 500,
  "global_step": 2252,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004439511653718091,
      "grad_norm": 2.6480212211608887,
      "learning_rate": 0.0004980017761989343,
      "loss": 1.9864,
      "step": 10
    },
    {
      "epoch": 0.008879023307436182,
      "grad_norm": 3.8814499378204346,
      "learning_rate": 0.0004957815275310834,
      "loss": 2.111,
      "step": 20
    },
    {
      "epoch": 0.013318534961154272,
      "grad_norm": 3.836819648742676,
      "learning_rate": 0.0004935612788632327,
      "loss": 2.026,
      "step": 30
    },
    {
      "epoch": 0.017758046614872364,
      "grad_norm": 4.906405448913574,
      "learning_rate": 0.0004913410301953819,
      "loss": 2.315,
      "step": 40
    },
    {
      "epoch": 0.022197558268590455,
      "grad_norm": 11.232091903686523,
      "learning_rate": 0.0004891207815275311,
      "loss": 2.5048,
      "step": 50
    },
    {
      "epoch": 0.026637069922308545,
      "grad_norm": 4.321173667907715,
      "learning_rate": 0.0004869005328596803,
      "loss": 2.0335,
      "step": 60
    },
    {
      "epoch": 0.03107658157602664,
      "grad_norm": 3.526768445968628,
      "learning_rate": 0.0004846802841918295,
      "loss": 1.6927,
      "step": 70
    },
    {
      "epoch": 0.03551609322974473,
      "grad_norm": 4.335197448730469,
      "learning_rate": 0.0004824600355239787,
      "loss": 1.9441,
      "step": 80
    },
    {
      "epoch": 0.03995560488346282,
      "grad_norm": 5.235602378845215,
      "learning_rate": 0.0004802397868561279,
      "loss": 2.0343,
      "step": 90
    },
    {
      "epoch": 0.04439511653718091,
      "grad_norm": 12.144674301147461,
      "learning_rate": 0.0004780195381882771,
      "loss": 2.553,
      "step": 100
    },
    {
      "epoch": 0.048834628190899,
      "grad_norm": 2.9235732555389404,
      "learning_rate": 0.0004757992895204263,
      "loss": 1.7132,
      "step": 110
    },
    {
      "epoch": 0.05327413984461709,
      "grad_norm": 2.640352249145508,
      "learning_rate": 0.0004738010657193606,
      "loss": 1.7306,
      "step": 120
    },
    {
      "epoch": 0.05771365149833518,
      "grad_norm": 4.020581245422363,
      "learning_rate": 0.0004715808170515098,
      "loss": 1.8579,
      "step": 130
    },
    {
      "epoch": 0.06215316315205328,
      "grad_norm": 5.956693172454834,
      "learning_rate": 0.000469360568383659,
      "loss": 1.9701,
      "step": 140
    },
    {
      "epoch": 0.06659267480577137,
      "grad_norm": 10.86117172241211,
      "learning_rate": 0.0004671403197158082,
      "loss": 2.4737,
      "step": 150
    },
    {
      "epoch": 0.07103218645948946,
      "grad_norm": 3.406299591064453,
      "learning_rate": 0.00046492007104795735,
      "loss": 1.7691,
      "step": 160
    },
    {
      "epoch": 0.07547169811320754,
      "grad_norm": 2.8631129264831543,
      "learning_rate": 0.0004626998223801066,
      "loss": 1.8153,
      "step": 170
    },
    {
      "epoch": 0.07991120976692564,
      "grad_norm": 3.7166526317596436,
      "learning_rate": 0.0004604795737122558,
      "loss": 1.7874,
      "step": 180
    },
    {
      "epoch": 0.08435072142064373,
      "grad_norm": 5.655150890350342,
      "learning_rate": 0.000458259325044405,
      "loss": 1.9541,
      "step": 190
    },
    {
      "epoch": 0.08879023307436182,
      "grad_norm": 14.101539611816406,
      "learning_rate": 0.00045603907637655416,
      "loss": 2.2468,
      "step": 200
    },
    {
      "epoch": 0.0932297447280799,
      "grad_norm": 2.777648448944092,
      "learning_rate": 0.0004538188277087034,
      "loss": 1.4692,
      "step": 210
    },
    {
      "epoch": 0.097669256381798,
      "grad_norm": 3.5041818618774414,
      "learning_rate": 0.0004515985790408526,
      "loss": 1.6826,
      "step": 220
    },
    {
      "epoch": 0.10210876803551609,
      "grad_norm": 3.113179922103882,
      "learning_rate": 0.0004493783303730018,
      "loss": 1.7892,
      "step": 230
    },
    {
      "epoch": 0.10654827968923418,
      "grad_norm": 5.085727691650391,
      "learning_rate": 0.00044715808170515096,
      "loss": 1.9955,
      "step": 240
    },
    {
      "epoch": 0.11098779134295228,
      "grad_norm": 11.823568344116211,
      "learning_rate": 0.0004449378330373002,
      "loss": 2.0594,
      "step": 250
    },
    {
      "epoch": 0.11542730299667037,
      "grad_norm": 3.5239861011505127,
      "learning_rate": 0.0004427175843694494,
      "loss": 1.5239,
      "step": 260
    },
    {
      "epoch": 0.11986681465038845,
      "grad_norm": 2.3924269676208496,
      "learning_rate": 0.0004404973357015986,
      "loss": 1.4782,
      "step": 270
    },
    {
      "epoch": 0.12430632630410655,
      "grad_norm": 2.890104055404663,
      "learning_rate": 0.0004382770870337478,
      "loss": 1.82,
      "step": 280
    },
    {
      "epoch": 0.12874583795782463,
      "grad_norm": 4.417105197906494,
      "learning_rate": 0.000436056838365897,
      "loss": 1.8403,
      "step": 290
    },
    {
      "epoch": 0.13318534961154274,
      "grad_norm": 18.158605575561523,
      "learning_rate": 0.0004338365896980462,
      "loss": 2.1923,
      "step": 300
    },
    {
      "epoch": 0.13762486126526083,
      "grad_norm": 2.554793357849121,
      "learning_rate": 0.00043161634103019536,
      "loss": 1.5482,
      "step": 310
    },
    {
      "epoch": 0.14206437291897892,
      "grad_norm": 2.6967556476593018,
      "learning_rate": 0.0004293960923623446,
      "loss": 1.523,
      "step": 320
    },
    {
      "epoch": 0.146503884572697,
      "grad_norm": 3.2753822803497314,
      "learning_rate": 0.0004271758436944938,
      "loss": 1.7703,
      "step": 330
    },
    {
      "epoch": 0.1509433962264151,
      "grad_norm": 4.702852725982666,
      "learning_rate": 0.000424955595026643,
      "loss": 1.9633,
      "step": 340
    },
    {
      "epoch": 0.15538290788013318,
      "grad_norm": 11.618288040161133,
      "learning_rate": 0.00042273534635879216,
      "loss": 2.0436,
      "step": 350
    },
    {
      "epoch": 0.1598224195338513,
      "grad_norm": 2.9904074668884277,
      "learning_rate": 0.0004205150976909414,
      "loss": 1.524,
      "step": 360
    },
    {
      "epoch": 0.16426193118756938,
      "grad_norm": 2.618682622909546,
      "learning_rate": 0.0004182948490230906,
      "loss": 1.5346,
      "step": 370
    },
    {
      "epoch": 0.16870144284128746,
      "grad_norm": 3.754326820373535,
      "learning_rate": 0.0004160746003552398,
      "loss": 1.6579,
      "step": 380
    },
    {
      "epoch": 0.17314095449500555,
      "grad_norm": 4.401376247406006,
      "learning_rate": 0.000413854351687389,
      "loss": 1.807,
      "step": 390
    },
    {
      "epoch": 0.17758046614872364,
      "grad_norm": 13.861041069030762,
      "learning_rate": 0.0004116341030195382,
      "loss": 2.0454,
      "step": 400
    },
    {
      "epoch": 0.18201997780244172,
      "grad_norm": 2.795679807662964,
      "learning_rate": 0.0004094138543516874,
      "loss": 1.4849,
      "step": 410
    },
    {
      "epoch": 0.1864594894561598,
      "grad_norm": 2.9945127964019775,
      "learning_rate": 0.0004071936056838366,
      "loss": 1.5372,
      "step": 420
    },
    {
      "epoch": 0.19089900110987792,
      "grad_norm": 2.9993138313293457,
      "learning_rate": 0.0004049733570159858,
      "loss": 1.6788,
      "step": 430
    },
    {
      "epoch": 0.195338512763596,
      "grad_norm": 4.126837730407715,
      "learning_rate": 0.000402753108348135,
      "loss": 1.8415,
      "step": 440
    },
    {
      "epoch": 0.1997780244173141,
      "grad_norm": 18.773263931274414,
      "learning_rate": 0.0004005328596802842,
      "loss": 2.1526,
      "step": 450
    },
    {
      "epoch": 0.20421753607103219,
      "grad_norm": 2.4826033115386963,
      "learning_rate": 0.00039831261101243336,
      "loss": 1.3834,
      "step": 460
    },
    {
      "epoch": 0.20865704772475027,
      "grad_norm": 2.375812530517578,
      "learning_rate": 0.00039609236234458263,
      "loss": 1.5174,
      "step": 470
    },
    {
      "epoch": 0.21309655937846836,
      "grad_norm": 2.4217655658721924,
      "learning_rate": 0.0003938721136767318,
      "loss": 1.6247,
      "step": 480
    },
    {
      "epoch": 0.21753607103218647,
      "grad_norm": 6.636941909790039,
      "learning_rate": 0.000391651865008881,
      "loss": 1.8015,
      "step": 490
    },
    {
      "epoch": 0.22197558268590456,
      "grad_norm": 8.263154029846191,
      "learning_rate": 0.00038943161634103017,
      "loss": 1.9807,
      "step": 500
    },
    {
      "epoch": 0.22641509433962265,
      "grad_norm": 2.9841692447662354,
      "learning_rate": 0.0003872113676731794,
      "loss": 1.472,
      "step": 510
    },
    {
      "epoch": 0.23085460599334073,
      "grad_norm": 2.6251726150512695,
      "learning_rate": 0.00038499111900532865,
      "loss": 1.6153,
      "step": 520
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 5.011847972869873,
      "learning_rate": 0.0003827708703374778,
      "loss": 1.6035,
      "step": 530
    },
    {
      "epoch": 0.2397336293007769,
      "grad_norm": 6.330869197845459,
      "learning_rate": 0.000380550621669627,
      "loss": 1.7316,
      "step": 540
    },
    {
      "epoch": 0.244173140954495,
      "grad_norm": 12.447341918945312,
      "learning_rate": 0.0003783303730017762,
      "loss": 1.9905,
      "step": 550
    },
    {
      "epoch": 0.2486126526082131,
      "grad_norm": 2.081214666366577,
      "learning_rate": 0.0003761101243339254,
      "loss": 1.336,
      "step": 560
    },
    {
      "epoch": 0.25305216426193117,
      "grad_norm": 2.662405252456665,
      "learning_rate": 0.0003738898756660746,
      "loss": 1.4031,
      "step": 570
    },
    {
      "epoch": 0.25749167591564925,
      "grad_norm": 3.315176486968994,
      "learning_rate": 0.00037166962699822383,
      "loss": 1.6729,
      "step": 580
    },
    {
      "epoch": 0.2619311875693674,
      "grad_norm": 6.071305274963379,
      "learning_rate": 0.000369449378330373,
      "loss": 1.8918,
      "step": 590
    },
    {
      "epoch": 0.2663706992230855,
      "grad_norm": 13.663119316101074,
      "learning_rate": 0.0003672291296625222,
      "loss": 2.1209,
      "step": 600
    },
    {
      "epoch": 0.27081021087680357,
      "grad_norm": 2.3198060989379883,
      "learning_rate": 0.00036500888099467137,
      "loss": 1.4062,
      "step": 610
    },
    {
      "epoch": 0.27524972253052166,
      "grad_norm": 2.818054437637329,
      "learning_rate": 0.00036278863232682063,
      "loss": 1.5829,
      "step": 620
    },
    {
      "epoch": 0.27968923418423974,
      "grad_norm": 3.2965891361236572,
      "learning_rate": 0.00036056838365896985,
      "loss": 1.5948,
      "step": 630
    },
    {
      "epoch": 0.28412874583795783,
      "grad_norm": 4.908566474914551,
      "learning_rate": 0.000358348134991119,
      "loss": 1.6749,
      "step": 640
    },
    {
      "epoch": 0.2885682574916759,
      "grad_norm": 12.173040390014648,
      "learning_rate": 0.0003561278863232682,
      "loss": 2.0575,
      "step": 650
    },
    {
      "epoch": 0.293007769145394,
      "grad_norm": 2.5288655757904053,
      "learning_rate": 0.0003539076376554174,
      "loss": 1.3692,
      "step": 660
    },
    {
      "epoch": 0.2974472807991121,
      "grad_norm": 3.6272215843200684,
      "learning_rate": 0.00035168738898756665,
      "loss": 1.3999,
      "step": 670
    },
    {
      "epoch": 0.3018867924528302,
      "grad_norm": 3.0021681785583496,
      "learning_rate": 0.0003494671403197158,
      "loss": 1.5021,
      "step": 680
    },
    {
      "epoch": 0.30632630410654826,
      "grad_norm": 4.833628177642822,
      "learning_rate": 0.00034724689165186503,
      "loss": 1.8385,
      "step": 690
    },
    {
      "epoch": 0.31076581576026635,
      "grad_norm": 9.886651039123535,
      "learning_rate": 0.0003450266429840142,
      "loss": 1.8232,
      "step": 700
    },
    {
      "epoch": 0.31520532741398444,
      "grad_norm": 2.959299325942993,
      "learning_rate": 0.0003428063943161634,
      "loss": 1.3905,
      "step": 710
    },
    {
      "epoch": 0.3196448390677026,
      "grad_norm": 2.7469334602355957,
      "learning_rate": 0.0003405861456483126,
      "loss": 1.3487,
      "step": 720
    },
    {
      "epoch": 0.32408435072142067,
      "grad_norm": 3.0091640949249268,
      "learning_rate": 0.00033836589698046183,
      "loss": 1.6422,
      "step": 730
    },
    {
      "epoch": 0.32852386237513875,
      "grad_norm": 4.956626892089844,
      "learning_rate": 0.000336145648312611,
      "loss": 1.7637,
      "step": 740
    },
    {
      "epoch": 0.33296337402885684,
      "grad_norm": 11.147844314575195,
      "learning_rate": 0.0003339253996447602,
      "loss": 1.9705,
      "step": 750
    },
    {
      "epoch": 0.3374028856825749,
      "grad_norm": 2.4156601428985596,
      "learning_rate": 0.0003317051509769094,
      "loss": 1.3207,
      "step": 760
    },
    {
      "epoch": 0.341842397336293,
      "grad_norm": 3.154998302459717,
      "learning_rate": 0.00032948490230905864,
      "loss": 1.5489,
      "step": 770
    },
    {
      "epoch": 0.3462819089900111,
      "grad_norm": 3.8287041187286377,
      "learning_rate": 0.00032726465364120785,
      "loss": 1.7162,
      "step": 780
    },
    {
      "epoch": 0.3507214206437292,
      "grad_norm": 7.401037693023682,
      "learning_rate": 0.000325044404973357,
      "loss": 1.8037,
      "step": 790
    },
    {
      "epoch": 0.3551609322974473,
      "grad_norm": 7.548885822296143,
      "learning_rate": 0.00032282415630550623,
      "loss": 1.9896,
      "step": 800
    },
    {
      "epoch": 0.35960044395116536,
      "grad_norm": 2.4291045665740967,
      "learning_rate": 0.0003206039076376554,
      "loss": 1.2883,
      "step": 810
    },
    {
      "epoch": 0.36403995560488345,
      "grad_norm": 2.5042500495910645,
      "learning_rate": 0.00031838365896980466,
      "loss": 1.4843,
      "step": 820
    },
    {
      "epoch": 0.36847946725860153,
      "grad_norm": 3.0613596439361572,
      "learning_rate": 0.0003161634103019538,
      "loss": 1.6377,
      "step": 830
    },
    {
      "epoch": 0.3729189789123196,
      "grad_norm": 4.791126728057861,
      "learning_rate": 0.00031394316163410303,
      "loss": 1.6451,
      "step": 840
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 12.54843807220459,
      "learning_rate": 0.0003117229129662522,
      "loss": 2.0207,
      "step": 850
    },
    {
      "epoch": 0.38179800221975585,
      "grad_norm": 2.500140905380249,
      "learning_rate": 0.0003095026642984014,
      "loss": 1.3542,
      "step": 860
    },
    {
      "epoch": 0.38623751387347394,
      "grad_norm": 2.897068500518799,
      "learning_rate": 0.0003072824156305506,
      "loss": 1.3988,
      "step": 870
    },
    {
      "epoch": 0.390677025527192,
      "grad_norm": 3.7682976722717285,
      "learning_rate": 0.00030506216696269984,
      "loss": 1.6722,
      "step": 880
    },
    {
      "epoch": 0.3951165371809101,
      "grad_norm": 5.111352920532227,
      "learning_rate": 0.00030284191829484905,
      "loss": 1.5998,
      "step": 890
    },
    {
      "epoch": 0.3995560488346282,
      "grad_norm": 14.146927833557129,
      "learning_rate": 0.0003006216696269982,
      "loss": 1.9545,
      "step": 900
    },
    {
      "epoch": 0.4039955604883463,
      "grad_norm": 2.6750071048736572,
      "learning_rate": 0.00029840142095914743,
      "loss": 1.254,
      "step": 910
    },
    {
      "epoch": 0.40843507214206437,
      "grad_norm": 2.234377145767212,
      "learning_rate": 0.00029618117229129664,
      "loss": 1.4234,
      "step": 920
    },
    {
      "epoch": 0.41287458379578246,
      "grad_norm": 3.256488561630249,
      "learning_rate": 0.00029396092362344586,
      "loss": 1.5793,
      "step": 930
    },
    {
      "epoch": 0.41731409544950054,
      "grad_norm": 4.795558452606201,
      "learning_rate": 0.000291740674955595,
      "loss": 1.6761,
      "step": 940
    },
    {
      "epoch": 0.42175360710321863,
      "grad_norm": 12.133206367492676,
      "learning_rate": 0.00028952042628774423,
      "loss": 2.0815,
      "step": 950
    },
    {
      "epoch": 0.4261931187569367,
      "grad_norm": 2.639509439468384,
      "learning_rate": 0.0002873001776198934,
      "loss": 1.338,
      "step": 960
    },
    {
      "epoch": 0.4306326304106548,
      "grad_norm": 3.0771937370300293,
      "learning_rate": 0.00028507992895204266,
      "loss": 1.4036,
      "step": 970
    },
    {
      "epoch": 0.43507214206437295,
      "grad_norm": 3.7259538173675537,
      "learning_rate": 0.0002828596802841918,
      "loss": 1.4765,
      "step": 980
    },
    {
      "epoch": 0.43951165371809103,
      "grad_norm": 4.704418182373047,
      "learning_rate": 0.00028063943161634104,
      "loss": 1.7446,
      "step": 990
    },
    {
      "epoch": 0.4439511653718091,
      "grad_norm": 10.983227729797363,
      "learning_rate": 0.0002784191829484902,
      "loss": 1.9901,
      "step": 1000
    },
    {
      "epoch": 0.4483906770255272,
      "grad_norm": 1.8925683498382568,
      "learning_rate": 0.0002761989342806394,
      "loss": 1.2102,
      "step": 1010
    },
    {
      "epoch": 0.4528301886792453,
      "grad_norm": 2.8974149227142334,
      "learning_rate": 0.0002739786856127887,
      "loss": 1.3562,
      "step": 1020
    },
    {
      "epoch": 0.4572697003329634,
      "grad_norm": 3.106844902038574,
      "learning_rate": 0.00027175843694493784,
      "loss": 1.5626,
      "step": 1030
    },
    {
      "epoch": 0.46170921198668147,
      "grad_norm": 6.168566703796387,
      "learning_rate": 0.00026953818827708706,
      "loss": 1.5893,
      "step": 1040
    },
    {
      "epoch": 0.46614872364039955,
      "grad_norm": 13.987650871276855,
      "learning_rate": 0.0002673179396092362,
      "loss": 2.0509,
      "step": 1050
    },
    {
      "epoch": 0.47058823529411764,
      "grad_norm": 2.7695271968841553,
      "learning_rate": 0.00026509769094138543,
      "loss": 1.2186,
      "step": 1060
    },
    {
      "epoch": 0.4750277469478357,
      "grad_norm": 3.01590633392334,
      "learning_rate": 0.00026287744227353465,
      "loss": 1.4949,
      "step": 1070
    },
    {
      "epoch": 0.4794672586015538,
      "grad_norm": 3.823798656463623,
      "learning_rate": 0.00026065719360568386,
      "loss": 1.5392,
      "step": 1080
    },
    {
      "epoch": 0.4839067702552719,
      "grad_norm": 4.6711745262146,
      "learning_rate": 0.000258436944937833,
      "loss": 1.6714,
      "step": 1090
    },
    {
      "epoch": 0.48834628190899,
      "grad_norm": 15.098380088806152,
      "learning_rate": 0.00025621669626998224,
      "loss": 2.0448,
      "step": 1100
    },
    {
      "epoch": 0.49278579356270813,
      "grad_norm": 2.3212554454803467,
      "learning_rate": 0.0002539964476021314,
      "loss": 1.1863,
      "step": 1110
    },
    {
      "epoch": 0.4972253052164262,
      "grad_norm": 3.3617687225341797,
      "learning_rate": 0.00025177619893428067,
      "loss": 1.4862,
      "step": 1120
    },
    {
      "epoch": 0.5016648168701443,
      "grad_norm": 3.3814823627471924,
      "learning_rate": 0.0002495559502664299,
      "loss": 1.6049,
      "step": 1130
    },
    {
      "epoch": 0.5061043285238623,
      "grad_norm": 4.782328128814697,
      "learning_rate": 0.00024733570159857904,
      "loss": 1.5822,
      "step": 1140
    },
    {
      "epoch": 0.5105438401775805,
      "grad_norm": 12.58549690246582,
      "learning_rate": 0.00024511545293072826,
      "loss": 1.8591,
      "step": 1150
    },
    {
      "epoch": 0.5149833518312985,
      "grad_norm": 2.5071122646331787,
      "learning_rate": 0.00024289520426287744,
      "loss": 1.2148,
      "step": 1160
    },
    {
      "epoch": 0.5194228634850167,
      "grad_norm": 2.7144827842712402,
      "learning_rate": 0.00024067495559502663,
      "loss": 1.3941,
      "step": 1170
    },
    {
      "epoch": 0.5238623751387348,
      "grad_norm": 3.405857563018799,
      "learning_rate": 0.00023845470692717585,
      "loss": 1.5388,
      "step": 1180
    },
    {
      "epoch": 0.5283018867924528,
      "grad_norm": 4.614385604858398,
      "learning_rate": 0.00023623445825932503,
      "loss": 1.5973,
      "step": 1190
    },
    {
      "epoch": 0.532741398446171,
      "grad_norm": 11.308721542358398,
      "learning_rate": 0.00023401420959147425,
      "loss": 2.0129,
      "step": 1200
    },
    {
      "epoch": 0.537180910099889,
      "grad_norm": 2.23142409324646,
      "learning_rate": 0.00023179396092362346,
      "loss": 1.3548,
      "step": 1210
    },
    {
      "epoch": 0.5416204217536071,
      "grad_norm": 2.3598549365997314,
      "learning_rate": 0.00022957371225577265,
      "loss": 1.3573,
      "step": 1220
    },
    {
      "epoch": 0.5460599334073252,
      "grad_norm": 4.465252876281738,
      "learning_rate": 0.00022735346358792187,
      "loss": 1.516,
      "step": 1230
    },
    {
      "epoch": 0.5504994450610433,
      "grad_norm": 4.624565124511719,
      "learning_rate": 0.00022513321492007105,
      "loss": 1.5995,
      "step": 1240
    },
    {
      "epoch": 0.5549389567147613,
      "grad_norm": 11.717595100402832,
      "learning_rate": 0.00022291296625222024,
      "loss": 2.1437,
      "step": 1250
    },
    {
      "epoch": 0.5593784683684795,
      "grad_norm": 2.5792629718780518,
      "learning_rate": 0.00022069271758436946,
      "loss": 1.1981,
      "step": 1260
    },
    {
      "epoch": 0.5638179800221975,
      "grad_norm": 2.56390643119812,
      "learning_rate": 0.00021847246891651864,
      "loss": 1.3531,
      "step": 1270
    },
    {
      "epoch": 0.5682574916759157,
      "grad_norm": 3.5103867053985596,
      "learning_rate": 0.00021625222024866786,
      "loss": 1.4552,
      "step": 1280
    },
    {
      "epoch": 0.5726970033296337,
      "grad_norm": 5.6287360191345215,
      "learning_rate": 0.00021403197158081705,
      "loss": 1.619,
      "step": 1290
    },
    {
      "epoch": 0.5771365149833518,
      "grad_norm": 13.241456031799316,
      "learning_rate": 0.00021181172291296626,
      "loss": 1.842,
      "step": 1300
    },
    {
      "epoch": 0.58157602663707,
      "grad_norm": 1.707026481628418,
      "learning_rate": 0.00020959147424511545,
      "loss": 1.2328,
      "step": 1310
    },
    {
      "epoch": 0.586015538290788,
      "grad_norm": 2.5454134941101074,
      "learning_rate": 0.00020737122557726466,
      "loss": 1.4556,
      "step": 1320
    },
    {
      "epoch": 0.5904550499445061,
      "grad_norm": 4.51999044418335,
      "learning_rate": 0.00020515097690941388,
      "loss": 1.5829,
      "step": 1330
    },
    {
      "epoch": 0.5948945615982242,
      "grad_norm": 5.803974628448486,
      "learning_rate": 0.00020293072824156306,
      "loss": 1.7158,
      "step": 1340
    },
    {
      "epoch": 0.5993340732519423,
      "grad_norm": 8.635396003723145,
      "learning_rate": 0.00020071047957371228,
      "loss": 2.0305,
      "step": 1350
    },
    {
      "epoch": 0.6037735849056604,
      "grad_norm": 2.5897269248962402,
      "learning_rate": 0.00019849023090586147,
      "loss": 1.2957,
      "step": 1360
    },
    {
      "epoch": 0.6082130965593785,
      "grad_norm": 2.8564116954803467,
      "learning_rate": 0.00019626998223801065,
      "loss": 1.4518,
      "step": 1370
    },
    {
      "epoch": 0.6126526082130965,
      "grad_norm": 4.025502681732178,
      "learning_rate": 0.00019404973357015987,
      "loss": 1.4896,
      "step": 1380
    },
    {
      "epoch": 0.6170921198668147,
      "grad_norm": 6.2419328689575195,
      "learning_rate": 0.00019182948490230906,
      "loss": 1.6442,
      "step": 1390
    },
    {
      "epoch": 0.6215316315205327,
      "grad_norm": 18.21343994140625,
      "learning_rate": 0.00018960923623445827,
      "loss": 2.1172,
      "step": 1400
    },
    {
      "epoch": 0.6259711431742508,
      "grad_norm": 2.538667917251587,
      "learning_rate": 0.00018738898756660746,
      "loss": 1.2513,
      "step": 1410
    },
    {
      "epoch": 0.6304106548279689,
      "grad_norm": 3.0883214473724365,
      "learning_rate": 0.00018516873889875665,
      "loss": 1.489,
      "step": 1420
    },
    {
      "epoch": 0.634850166481687,
      "grad_norm": 3.5508062839508057,
      "learning_rate": 0.00018294849023090586,
      "loss": 1.498,
      "step": 1430
    },
    {
      "epoch": 0.6392896781354052,
      "grad_norm": 4.443967819213867,
      "learning_rate": 0.00018072824156305505,
      "loss": 1.6467,
      "step": 1440
    },
    {
      "epoch": 0.6437291897891232,
      "grad_norm": 13.478910446166992,
      "learning_rate": 0.00017873001776198935,
      "loss": 2.1138,
      "step": 1450
    },
    {
      "epoch": 0.6481687014428413,
      "grad_norm": 1.944088339805603,
      "learning_rate": 0.00017650976909413854,
      "loss": 1.2682,
      "step": 1460
    },
    {
      "epoch": 0.6526082130965594,
      "grad_norm": 2.456782579421997,
      "learning_rate": 0.00017428952042628773,
      "loss": 1.4148,
      "step": 1470
    },
    {
      "epoch": 0.6570477247502775,
      "grad_norm": 3.025043487548828,
      "learning_rate": 0.00017206927175843694,
      "loss": 1.3909,
      "step": 1480
    },
    {
      "epoch": 0.6614872364039955,
      "grad_norm": 6.078201770782471,
      "learning_rate": 0.00016984902309058613,
      "loss": 1.678,
      "step": 1490
    },
    {
      "epoch": 0.6659267480577137,
      "grad_norm": 10.49463176727295,
      "learning_rate": 0.00016762877442273535,
      "loss": 1.9187,
      "step": 1500
    },
    {
      "epoch": 0.6703662597114317,
      "grad_norm": 2.662195920944214,
      "learning_rate": 0.00016540852575488456,
      "loss": 1.2996,
      "step": 1510
    },
    {
      "epoch": 0.6748057713651499,
      "grad_norm": 2.835592031478882,
      "learning_rate": 0.00016318827708703375,
      "loss": 1.5221,
      "step": 1520
    },
    {
      "epoch": 0.6792452830188679,
      "grad_norm": 4.037736415863037,
      "learning_rate": 0.00016096802841918296,
      "loss": 1.5633,
      "step": 1530
    },
    {
      "epoch": 0.683684794672586,
      "grad_norm": 5.402946949005127,
      "learning_rate": 0.00015874777975133215,
      "loss": 1.6176,
      "step": 1540
    },
    {
      "epoch": 0.6881243063263041,
      "grad_norm": 14.478370666503906,
      "learning_rate": 0.00015652753108348136,
      "loss": 1.9173,
      "step": 1550
    },
    {
      "epoch": 0.6925638179800222,
      "grad_norm": 2.58990216255188,
      "learning_rate": 0.00015430728241563055,
      "loss": 1.2484,
      "step": 1560
    },
    {
      "epoch": 0.6970033296337403,
      "grad_norm": 3.612370729446411,
      "learning_rate": 0.00015208703374777977,
      "loss": 1.3537,
      "step": 1570
    },
    {
      "epoch": 0.7014428412874584,
      "grad_norm": 3.987487554550171,
      "learning_rate": 0.00014986678507992895,
      "loss": 1.5682,
      "step": 1580
    },
    {
      "epoch": 0.7058823529411765,
      "grad_norm": 5.03774881362915,
      "learning_rate": 0.00014764653641207814,
      "loss": 1.6406,
      "step": 1590
    },
    {
      "epoch": 0.7103218645948945,
      "grad_norm": 14.200817108154297,
      "learning_rate": 0.00014542628774422736,
      "loss": 2.0936,
      "step": 1600
    },
    {
      "epoch": 0.7147613762486127,
      "grad_norm": 2.2695741653442383,
      "learning_rate": 0.00014320603907637654,
      "loss": 1.2791,
      "step": 1610
    },
    {
      "epoch": 0.7192008879023307,
      "grad_norm": 2.3212890625,
      "learning_rate": 0.00014098579040852576,
      "loss": 1.2358,
      "step": 1620
    },
    {
      "epoch": 0.7236403995560489,
      "grad_norm": 3.371938467025757,
      "learning_rate": 0.00013876554174067497,
      "loss": 1.4945,
      "step": 1630
    },
    {
      "epoch": 0.7280799112097669,
      "grad_norm": 5.500016689300537,
      "learning_rate": 0.00013654529307282416,
      "loss": 1.6147,
      "step": 1640
    },
    {
      "epoch": 0.732519422863485,
      "grad_norm": 13.485428810119629,
      "learning_rate": 0.00013432504440497338,
      "loss": 2.0337,
      "step": 1650
    },
    {
      "epoch": 0.7369589345172031,
      "grad_norm": 2.9658217430114746,
      "learning_rate": 0.00013210479573712256,
      "loss": 1.2317,
      "step": 1660
    },
    {
      "epoch": 0.7413984461709212,
      "grad_norm": 2.9035165309906006,
      "learning_rate": 0.00012988454706927178,
      "loss": 1.2479,
      "step": 1670
    },
    {
      "epoch": 0.7458379578246392,
      "grad_norm": 3.2992308139801025,
      "learning_rate": 0.00012766429840142097,
      "loss": 1.4118,
      "step": 1680
    },
    {
      "epoch": 0.7502774694783574,
      "grad_norm": 4.49350118637085,
      "learning_rate": 0.00012544404973357015,
      "loss": 1.5465,
      "step": 1690
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 10.469914436340332,
      "learning_rate": 0.00012322380106571937,
      "loss": 1.9008,
      "step": 1700
    },
    {
      "epoch": 0.7591564927857936,
      "grad_norm": 2.011507987976074,
      "learning_rate": 0.00012100355239786856,
      "loss": 1.202,
      "step": 1710
    },
    {
      "epoch": 0.7635960044395117,
      "grad_norm": 2.3160221576690674,
      "learning_rate": 0.00011878330373001776,
      "loss": 1.3729,
      "step": 1720
    },
    {
      "epoch": 0.7680355160932297,
      "grad_norm": 3.580491065979004,
      "learning_rate": 0.00011656305506216697,
      "loss": 1.3546,
      "step": 1730
    },
    {
      "epoch": 0.7724750277469479,
      "grad_norm": 5.41868782043457,
      "learning_rate": 0.00011434280639431617,
      "loss": 1.4872,
      "step": 1740
    },
    {
      "epoch": 0.7769145394006659,
      "grad_norm": 12.580742835998535,
      "learning_rate": 0.00011212255772646536,
      "loss": 2.1207,
      "step": 1750
    },
    {
      "epoch": 0.781354051054384,
      "grad_norm": 1.9727038145065308,
      "learning_rate": 0.00010990230905861456,
      "loss": 1.2415,
      "step": 1760
    },
    {
      "epoch": 0.7857935627081021,
      "grad_norm": 2.682058811187744,
      "learning_rate": 0.00010768206039076376,
      "loss": 1.3905,
      "step": 1770
    },
    {
      "epoch": 0.7902330743618202,
      "grad_norm": 3.479135513305664,
      "learning_rate": 0.00010546181172291296,
      "loss": 1.3614,
      "step": 1780
    },
    {
      "epoch": 0.7946725860155383,
      "grad_norm": 4.732255935668945,
      "learning_rate": 0.00010324156305506218,
      "loss": 1.5566,
      "step": 1790
    },
    {
      "epoch": 0.7991120976692564,
      "grad_norm": 16.04266357421875,
      "learning_rate": 0.00010102131438721137,
      "loss": 1.996,
      "step": 1800
    },
    {
      "epoch": 0.8035516093229744,
      "grad_norm": 1.9168890714645386,
      "learning_rate": 9.880106571936057e-05,
      "loss": 1.2599,
      "step": 1810
    },
    {
      "epoch": 0.8079911209766926,
      "grad_norm": 4.307586669921875,
      "learning_rate": 9.658081705150977e-05,
      "loss": 1.4392,
      "step": 1820
    },
    {
      "epoch": 0.8124306326304107,
      "grad_norm": 4.147525787353516,
      "learning_rate": 9.436056838365897e-05,
      "loss": 1.4515,
      "step": 1830
    },
    {
      "epoch": 0.8168701442841287,
      "grad_norm": 5.004342079162598,
      "learning_rate": 9.214031971580817e-05,
      "loss": 1.4454,
      "step": 1840
    },
    {
      "epoch": 0.8213096559378469,
      "grad_norm": 11.836891174316406,
      "learning_rate": 8.992007104795737e-05,
      "loss": 2.0717,
      "step": 1850
    },
    {
      "epoch": 0.8257491675915649,
      "grad_norm": 2.081458806991577,
      "learning_rate": 8.769982238010657e-05,
      "loss": 1.2711,
      "step": 1860
    },
    {
      "epoch": 0.8301886792452831,
      "grad_norm": 3.2301530838012695,
      "learning_rate": 8.547957371225578e-05,
      "loss": 1.3933,
      "step": 1870
    },
    {
      "epoch": 0.8346281908990011,
      "grad_norm": 4.215748310089111,
      "learning_rate": 8.325932504440498e-05,
      "loss": 1.492,
      "step": 1880
    },
    {
      "epoch": 0.8390677025527192,
      "grad_norm": 5.146585941314697,
      "learning_rate": 8.103907637655418e-05,
      "loss": 1.8075,
      "step": 1890
    },
    {
      "epoch": 0.8435072142064373,
      "grad_norm": 17.597797393798828,
      "learning_rate": 7.881882770870337e-05,
      "loss": 2.1816,
      "step": 1900
    },
    {
      "epoch": 0.8479467258601554,
      "grad_norm": 2.1561050415039062,
      "learning_rate": 7.659857904085257e-05,
      "loss": 1.2316,
      "step": 1910
    },
    {
      "epoch": 0.8523862375138734,
      "grad_norm": 2.979154348373413,
      "learning_rate": 7.437833037300178e-05,
      "loss": 1.3182,
      "step": 1920
    },
    {
      "epoch": 0.8568257491675916,
      "grad_norm": 4.0174407958984375,
      "learning_rate": 7.215808170515098e-05,
      "loss": 1.5184,
      "step": 1930
    },
    {
      "epoch": 0.8612652608213096,
      "grad_norm": 5.249705791473389,
      "learning_rate": 6.993783303730018e-05,
      "loss": 1.6518,
      "step": 1940
    },
    {
      "epoch": 0.8657047724750278,
      "grad_norm": 18.494525909423828,
      "learning_rate": 6.771758436944938e-05,
      "loss": 1.6454,
      "step": 1950
    },
    {
      "epoch": 0.8701442841287459,
      "grad_norm": 2.3823788166046143,
      "learning_rate": 6.549733570159857e-05,
      "loss": 1.1649,
      "step": 1960
    },
    {
      "epoch": 0.8745837957824639,
      "grad_norm": 2.5704667568206787,
      "learning_rate": 6.327708703374777e-05,
      "loss": 1.3537,
      "step": 1970
    },
    {
      "epoch": 0.8790233074361821,
      "grad_norm": 3.684246301651001,
      "learning_rate": 6.105683836589699e-05,
      "loss": 1.4923,
      "step": 1980
    },
    {
      "epoch": 0.8834628190899001,
      "grad_norm": 6.052124977111816,
      "learning_rate": 5.883658969804618e-05,
      "loss": 1.5947,
      "step": 1990
    },
    {
      "epoch": 0.8879023307436182,
      "grad_norm": 10.06444263458252,
      "learning_rate": 5.6616341030195384e-05,
      "loss": 1.6872,
      "step": 2000
    },
    {
      "epoch": 0.8923418423973363,
      "grad_norm": 2.449098825454712,
      "learning_rate": 5.4396092362344585e-05,
      "loss": 1.2291,
      "step": 2010
    },
    {
      "epoch": 0.8967813540510544,
      "grad_norm": 3.3503260612487793,
      "learning_rate": 5.217584369449378e-05,
      "loss": 1.4097,
      "step": 2020
    },
    {
      "epoch": 0.9012208657047724,
      "grad_norm": 4.615697860717773,
      "learning_rate": 4.995559502664299e-05,
      "loss": 1.5036,
      "step": 2030
    },
    {
      "epoch": 0.9056603773584906,
      "grad_norm": 4.478034496307373,
      "learning_rate": 4.773534635879218e-05,
      "loss": 1.6367,
      "step": 2040
    },
    {
      "epoch": 0.9100998890122086,
      "grad_norm": 11.398510932922363,
      "learning_rate": 4.551509769094138e-05,
      "loss": 1.8832,
      "step": 2050
    },
    {
      "epoch": 0.9145394006659268,
      "grad_norm": 2.5301389694213867,
      "learning_rate": 4.329484902309059e-05,
      "loss": 1.2636,
      "step": 2060
    },
    {
      "epoch": 0.9189789123196448,
      "grad_norm": 2.599909782409668,
      "learning_rate": 4.1074600355239785e-05,
      "loss": 1.3657,
      "step": 2070
    },
    {
      "epoch": 0.9234184239733629,
      "grad_norm": 2.961329936981201,
      "learning_rate": 3.8854351687388986e-05,
      "loss": 1.5784,
      "step": 2080
    },
    {
      "epoch": 0.9278579356270811,
      "grad_norm": 5.4108686447143555,
      "learning_rate": 3.6634103019538194e-05,
      "loss": 1.5303,
      "step": 2090
    },
    {
      "epoch": 0.9322974472807991,
      "grad_norm": 10.046679496765137,
      "learning_rate": 3.441385435168739e-05,
      "loss": 1.7771,
      "step": 2100
    },
    {
      "epoch": 0.9367369589345172,
      "grad_norm": 2.524421215057373,
      "learning_rate": 3.219360568383659e-05,
      "loss": 1.2326,
      "step": 2110
    },
    {
      "epoch": 0.9411764705882353,
      "grad_norm": 2.4826345443725586,
      "learning_rate": 2.997335701598579e-05,
      "loss": 1.4405,
      "step": 2120
    },
    {
      "epoch": 0.9456159822419534,
      "grad_norm": 3.585848093032837,
      "learning_rate": 2.7753108348134992e-05,
      "loss": 1.3667,
      "step": 2130
    },
    {
      "epoch": 0.9500554938956715,
      "grad_norm": 6.881542682647705,
      "learning_rate": 2.5532859680284193e-05,
      "loss": 1.7215,
      "step": 2140
    },
    {
      "epoch": 0.9544950055493896,
      "grad_norm": 23.762577056884766,
      "learning_rate": 2.331261101243339e-05,
      "loss": 1.9808,
      "step": 2150
    },
    {
      "epoch": 0.9589345172031076,
      "grad_norm": 2.2526614665985107,
      "learning_rate": 2.1092362344582592e-05,
      "loss": 1.2045,
      "step": 2160
    },
    {
      "epoch": 0.9633740288568258,
      "grad_norm": 2.5937604904174805,
      "learning_rate": 1.8872113676731797e-05,
      "loss": 1.4083,
      "step": 2170
    },
    {
      "epoch": 0.9678135405105438,
      "grad_norm": 2.7855405807495117,
      "learning_rate": 1.6651865008880995e-05,
      "loss": 1.612,
      "step": 2180
    },
    {
      "epoch": 0.9722530521642619,
      "grad_norm": 5.190046787261963,
      "learning_rate": 1.4431616341030196e-05,
      "loss": 1.4839,
      "step": 2190
    },
    {
      "epoch": 0.97669256381798,
      "grad_norm": 11.090227127075195,
      "learning_rate": 1.2211367673179397e-05,
      "loss": 1.9021,
      "step": 2200
    },
    {
      "epoch": 0.9811320754716981,
      "grad_norm": 2.865304470062256,
      "learning_rate": 9.991119005328596e-06,
      "loss": 1.1503,
      "step": 2210
    },
    {
      "epoch": 0.9855715871254163,
      "grad_norm": 2.6737253665924072,
      "learning_rate": 7.770870337477798e-06,
      "loss": 1.4061,
      "step": 2220
    },
    {
      "epoch": 0.9900110987791343,
      "grad_norm": 5.334693431854248,
      "learning_rate": 5.550621669626999e-06,
      "loss": 1.5951,
      "step": 2230
    },
    {
      "epoch": 0.9944506104328524,
      "grad_norm": 5.061669826507568,
      "learning_rate": 3.3303730017761987e-06,
      "loss": 1.5569,
      "step": 2240
    },
    {
      "epoch": 0.9988901220865705,
      "grad_norm": 12.654797554016113,
      "learning_rate": 1.1101243339253996e-06,
      "loss": 1.832,
      "step": 2250
    }
  ],
  "logging_steps": 10,
  "max_steps": 2252,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3539759032958976.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
