{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.908990011098779,
  "eval_steps": 500,
  "global_step": 4300,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004439511653718091,
      "grad_norm": 4.054643630981445,
      "learning_rate": 0.0004992599171107164,
      "loss": 2.352,
      "step": 10
    },
    {
      "epoch": 0.008879023307436182,
      "grad_norm": 2.477839469909668,
      "learning_rate": 0.0004985198342214328,
      "loss": 1.9176,
      "step": 20
    },
    {
      "epoch": 0.013318534961154272,
      "grad_norm": 2.748054027557373,
      "learning_rate": 0.0004977797513321492,
      "loss": 2.0735,
      "step": 30
    },
    {
      "epoch": 0.017758046614872364,
      "grad_norm": 2.6057891845703125,
      "learning_rate": 0.0004970396684428656,
      "loss": 1.7736,
      "step": 40
    },
    {
      "epoch": 0.022197558268590455,
      "grad_norm": 1.843123197555542,
      "learning_rate": 0.000496299585553582,
      "loss": 1.822,
      "step": 50
    },
    {
      "epoch": 0.026637069922308545,
      "grad_norm": 1.7726880311965942,
      "learning_rate": 0.0004956335109532268,
      "loss": 1.7889,
      "step": 60
    },
    {
      "epoch": 0.03107658157602664,
      "grad_norm": 2.047724485397339,
      "learning_rate": 0.0004948934280639432,
      "loss": 1.6535,
      "step": 70
    },
    {
      "epoch": 0.03551609322974473,
      "grad_norm": 1.8672618865966797,
      "learning_rate": 0.0004941533451746596,
      "loss": 1.6607,
      "step": 80
    },
    {
      "epoch": 0.03995560488346282,
      "grad_norm": 2.528557538986206,
      "learning_rate": 0.000493413262285376,
      "loss": 1.7816,
      "step": 90
    },
    {
      "epoch": 0.04439511653718091,
      "grad_norm": 2.168644905090332,
      "learning_rate": 0.0004926731793960924,
      "loss": 1.5275,
      "step": 100
    },
    {
      "epoch": 0.048834628190899,
      "grad_norm": 2.151695728302002,
      "learning_rate": 0.0004919330965068088,
      "loss": 1.7346,
      "step": 110
    },
    {
      "epoch": 0.05327413984461709,
      "grad_norm": 2.0462753772735596,
      "learning_rate": 0.0004911930136175252,
      "loss": 1.5574,
      "step": 120
    },
    {
      "epoch": 0.05771365149833518,
      "grad_norm": 1.5268481969833374,
      "learning_rate": 0.0004904529307282416,
      "loss": 1.6383,
      "step": 130
    },
    {
      "epoch": 0.06215316315205328,
      "grad_norm": 2.150617837905884,
      "learning_rate": 0.000489712847838958,
      "loss": 1.6846,
      "step": 140
    },
    {
      "epoch": 0.06659267480577137,
      "grad_norm": 2.586848020553589,
      "learning_rate": 0.0004889727649496743,
      "loss": 1.5786,
      "step": 150
    },
    {
      "epoch": 0.07103218645948946,
      "grad_norm": 1.9333423376083374,
      "learning_rate": 0.00048823268206039076,
      "loss": 1.7571,
      "step": 160
    },
    {
      "epoch": 0.07547169811320754,
      "grad_norm": 3.1580400466918945,
      "learning_rate": 0.00048749259917110717,
      "loss": 1.6413,
      "step": 170
    },
    {
      "epoch": 0.07991120976692564,
      "grad_norm": 2.201777458190918,
      "learning_rate": 0.00048675251628182357,
      "loss": 1.6637,
      "step": 180
    },
    {
      "epoch": 0.08435072142064373,
      "grad_norm": 1.5520122051239014,
      "learning_rate": 0.00048601243339254,
      "loss": 1.5949,
      "step": 190
    },
    {
      "epoch": 0.08879023307436182,
      "grad_norm": 3.2420663833618164,
      "learning_rate": 0.0004852723505032564,
      "loss": 1.5658,
      "step": 200
    },
    {
      "epoch": 0.0932297447280799,
      "grad_norm": 2.1900508403778076,
      "learning_rate": 0.0004845322676139728,
      "loss": 1.5863,
      "step": 210
    },
    {
      "epoch": 0.097669256381798,
      "grad_norm": 2.21549391746521,
      "learning_rate": 0.0004837921847246892,
      "loss": 1.4758,
      "step": 220
    },
    {
      "epoch": 0.10210876803551609,
      "grad_norm": 2.064615488052368,
      "learning_rate": 0.0004830521018354056,
      "loss": 1.55,
      "step": 230
    },
    {
      "epoch": 0.10654827968923418,
      "grad_norm": 2.525146961212158,
      "learning_rate": 0.000482312018946122,
      "loss": 1.5399,
      "step": 240
    },
    {
      "epoch": 0.11098779134295228,
      "grad_norm": 2.0821399688720703,
      "learning_rate": 0.00048157193605683835,
      "loss": 1.4429,
      "step": 250
    },
    {
      "epoch": 0.11542730299667037,
      "grad_norm": 1.58427894115448,
      "learning_rate": 0.0004808318531675548,
      "loss": 1.4108,
      "step": 260
    },
    {
      "epoch": 0.11986681465038845,
      "grad_norm": 2.192683696746826,
      "learning_rate": 0.00048009177027827116,
      "loss": 1.4006,
      "step": 270
    },
    {
      "epoch": 0.12430632630410655,
      "grad_norm": 1.5377377271652222,
      "learning_rate": 0.0004793516873889876,
      "loss": 1.3984,
      "step": 280
    },
    {
      "epoch": 0.12874583795782463,
      "grad_norm": 2.260063409805298,
      "learning_rate": 0.00047861160449970397,
      "loss": 1.4997,
      "step": 290
    },
    {
      "epoch": 0.13318534961154274,
      "grad_norm": 3.4104886054992676,
      "learning_rate": 0.0004778715216104204,
      "loss": 1.4686,
      "step": 300
    },
    {
      "epoch": 0.13762486126526083,
      "grad_norm": 2.8780131340026855,
      "learning_rate": 0.0004771314387211368,
      "loss": 1.6059,
      "step": 310
    },
    {
      "epoch": 0.14206437291897892,
      "grad_norm": 2.854306221008301,
      "learning_rate": 0.0004763913558318532,
      "loss": 1.6074,
      "step": 320
    },
    {
      "epoch": 0.146503884572697,
      "grad_norm": 1.7294703722000122,
      "learning_rate": 0.0004756512729425696,
      "loss": 1.5092,
      "step": 330
    },
    {
      "epoch": 0.1509433962264151,
      "grad_norm": 2.494455099105835,
      "learning_rate": 0.000474911190053286,
      "loss": 1.4524,
      "step": 340
    },
    {
      "epoch": 0.15538290788013318,
      "grad_norm": 2.055124521255493,
      "learning_rate": 0.00047417110716400235,
      "loss": 1.4831,
      "step": 350
    },
    {
      "epoch": 0.1598224195338513,
      "grad_norm": 2.6806280612945557,
      "learning_rate": 0.0004734310242747188,
      "loss": 1.5829,
      "step": 360
    },
    {
      "epoch": 0.16426193118756938,
      "grad_norm": 2.334710121154785,
      "learning_rate": 0.00047269094138543516,
      "loss": 1.5037,
      "step": 370
    },
    {
      "epoch": 0.16870144284128746,
      "grad_norm": 3.8841795921325684,
      "learning_rate": 0.0004719508584961516,
      "loss": 1.4424,
      "step": 380
    },
    {
      "epoch": 0.17314095449500555,
      "grad_norm": 3.0745744705200195,
      "learning_rate": 0.00047121077560686797,
      "loss": 1.4201,
      "step": 390
    },
    {
      "epoch": 0.17758046614872364,
      "grad_norm": 2.2438924312591553,
      "learning_rate": 0.00047047069271758437,
      "loss": 1.4589,
      "step": 400
    },
    {
      "epoch": 0.18201997780244172,
      "grad_norm": 2.606391429901123,
      "learning_rate": 0.0004697306098283008,
      "loss": 1.5147,
      "step": 410
    },
    {
      "epoch": 0.1864594894561598,
      "grad_norm": 1.9846000671386719,
      "learning_rate": 0.0004689905269390172,
      "loss": 1.6754,
      "step": 420
    },
    {
      "epoch": 0.19089900110987792,
      "grad_norm": 2.0650007724761963,
      "learning_rate": 0.0004682504440497336,
      "loss": 1.4663,
      "step": 430
    },
    {
      "epoch": 0.195338512763596,
      "grad_norm": 1.969778060913086,
      "learning_rate": 0.00046751036116045,
      "loss": 1.4181,
      "step": 440
    },
    {
      "epoch": 0.1997780244173141,
      "grad_norm": 2.866069793701172,
      "learning_rate": 0.00046677027827116634,
      "loss": 1.3834,
      "step": 450
    },
    {
      "epoch": 0.20421753607103219,
      "grad_norm": 2.133826494216919,
      "learning_rate": 0.0004660301953818828,
      "loss": 1.4518,
      "step": 460
    },
    {
      "epoch": 0.20865704772475027,
      "grad_norm": 2.4633429050445557,
      "learning_rate": 0.00046529011249259915,
      "loss": 1.4814,
      "step": 470
    },
    {
      "epoch": 0.21309655937846836,
      "grad_norm": 1.6882880926132202,
      "learning_rate": 0.0004645500296033156,
      "loss": 1.4354,
      "step": 480
    },
    {
      "epoch": 0.21753607103218647,
      "grad_norm": 2.161064624786377,
      "learning_rate": 0.00046380994671403196,
      "loss": 1.4254,
      "step": 490
    },
    {
      "epoch": 0.22197558268590456,
      "grad_norm": 3.765519142150879,
      "learning_rate": 0.00046306986382474837,
      "loss": 1.4999,
      "step": 500
    },
    {
      "epoch": 0.22641509433962265,
      "grad_norm": 2.3685739040374756,
      "learning_rate": 0.00046232978093546477,
      "loss": 1.4368,
      "step": 510
    },
    {
      "epoch": 0.23085460599334073,
      "grad_norm": 1.8080672025680542,
      "learning_rate": 0.0004615896980461812,
      "loss": 1.4922,
      "step": 520
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 3.117081880569458,
      "learning_rate": 0.0004608496151568976,
      "loss": 1.4003,
      "step": 530
    },
    {
      "epoch": 0.2397336293007769,
      "grad_norm": 1.7682117223739624,
      "learning_rate": 0.000460109532267614,
      "loss": 1.489,
      "step": 540
    },
    {
      "epoch": 0.244173140954495,
      "grad_norm": 2.2536675930023193,
      "learning_rate": 0.00045936944937833034,
      "loss": 1.5295,
      "step": 550
    },
    {
      "epoch": 0.2486126526082131,
      "grad_norm": 2.2390410900115967,
      "learning_rate": 0.0004586293664890468,
      "loss": 1.3979,
      "step": 560
    },
    {
      "epoch": 0.25305216426193117,
      "grad_norm": 2.209352731704712,
      "learning_rate": 0.0004578892835997632,
      "loss": 1.4194,
      "step": 570
    },
    {
      "epoch": 0.25749167591564925,
      "grad_norm": 2.129152536392212,
      "learning_rate": 0.0004571492007104796,
      "loss": 1.394,
      "step": 580
    },
    {
      "epoch": 0.2619311875693674,
      "grad_norm": 2.003588914871216,
      "learning_rate": 0.000456409117821196,
      "loss": 1.2182,
      "step": 590
    },
    {
      "epoch": 0.2663706992230855,
      "grad_norm": 1.967622995376587,
      "learning_rate": 0.00045566903493191236,
      "loss": 1.5414,
      "step": 600
    },
    {
      "epoch": 0.27081021087680357,
      "grad_norm": 2.4503138065338135,
      "learning_rate": 0.0004549289520426288,
      "loss": 1.3881,
      "step": 610
    },
    {
      "epoch": 0.27524972253052166,
      "grad_norm": 2.1452744007110596,
      "learning_rate": 0.00045418886915334517,
      "loss": 1.3729,
      "step": 620
    },
    {
      "epoch": 0.27968923418423974,
      "grad_norm": 2.611724615097046,
      "learning_rate": 0.00045344878626406163,
      "loss": 1.4539,
      "step": 630
    },
    {
      "epoch": 0.28412874583795783,
      "grad_norm": 2.1509506702423096,
      "learning_rate": 0.000452708703374778,
      "loss": 1.4869,
      "step": 640
    },
    {
      "epoch": 0.2885682574916759,
      "grad_norm": 1.7710063457489014,
      "learning_rate": 0.0004519686204854944,
      "loss": 1.4636,
      "step": 650
    },
    {
      "epoch": 0.293007769145394,
      "grad_norm": 2.103294610977173,
      "learning_rate": 0.0004512285375962108,
      "loss": 1.3811,
      "step": 660
    },
    {
      "epoch": 0.2974472807991121,
      "grad_norm": 2.388345956802368,
      "learning_rate": 0.0004504884547069272,
      "loss": 1.3846,
      "step": 670
    },
    {
      "epoch": 0.3018867924528302,
      "grad_norm": 2.7555673122406006,
      "learning_rate": 0.0004497483718176436,
      "loss": 1.4782,
      "step": 680
    },
    {
      "epoch": 0.30632630410654826,
      "grad_norm": 2.0017216205596924,
      "learning_rate": 0.00044900828892836,
      "loss": 1.3683,
      "step": 690
    },
    {
      "epoch": 0.31076581576026635,
      "grad_norm": 2.90942120552063,
      "learning_rate": 0.00044826820603907636,
      "loss": 1.3179,
      "step": 700
    },
    {
      "epoch": 0.31520532741398444,
      "grad_norm": 2.5046520233154297,
      "learning_rate": 0.0004475281231497928,
      "loss": 1.4616,
      "step": 710
    },
    {
      "epoch": 0.3196448390677026,
      "grad_norm": 1.8346855640411377,
      "learning_rate": 0.00044678804026050917,
      "loss": 1.2704,
      "step": 720
    },
    {
      "epoch": 0.32408435072142067,
      "grad_norm": 1.8805220127105713,
      "learning_rate": 0.0004460479573712256,
      "loss": 1.3124,
      "step": 730
    },
    {
      "epoch": 0.32852386237513875,
      "grad_norm": 2.064824342727661,
      "learning_rate": 0.000445307874481942,
      "loss": 1.4841,
      "step": 740
    },
    {
      "epoch": 0.33296337402885684,
      "grad_norm": 2.6950631141662598,
      "learning_rate": 0.0004445677915926584,
      "loss": 1.4189,
      "step": 750
    },
    {
      "epoch": 0.3374028856825749,
      "grad_norm": 2.8344624042510986,
      "learning_rate": 0.0004438277087033748,
      "loss": 1.6122,
      "step": 760
    },
    {
      "epoch": 0.341842397336293,
      "grad_norm": 2.25856351852417,
      "learning_rate": 0.0004430876258140912,
      "loss": 1.3434,
      "step": 770
    },
    {
      "epoch": 0.3462819089900111,
      "grad_norm": 3.7481534481048584,
      "learning_rate": 0.0004423475429248076,
      "loss": 1.4331,
      "step": 780
    },
    {
      "epoch": 0.3507214206437292,
      "grad_norm": 2.2983579635620117,
      "learning_rate": 0.000441607460035524,
      "loss": 1.389,
      "step": 790
    },
    {
      "epoch": 0.3551609322974473,
      "grad_norm": 1.9178669452667236,
      "learning_rate": 0.00044086737714624035,
      "loss": 1.3596,
      "step": 800
    },
    {
      "epoch": 0.35960044395116536,
      "grad_norm": 2.607564926147461,
      "learning_rate": 0.0004401272942569568,
      "loss": 1.3677,
      "step": 810
    },
    {
      "epoch": 0.36403995560488345,
      "grad_norm": 1.7786167860031128,
      "learning_rate": 0.00043938721136767316,
      "loss": 1.3581,
      "step": 820
    },
    {
      "epoch": 0.36847946725860153,
      "grad_norm": 1.9138953685760498,
      "learning_rate": 0.0004386471284783896,
      "loss": 1.3118,
      "step": 830
    },
    {
      "epoch": 0.3729189789123196,
      "grad_norm": 2.3295493125915527,
      "learning_rate": 0.00043790704558910597,
      "loss": 1.425,
      "step": 840
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 2.2294840812683105,
      "learning_rate": 0.0004371669626998224,
      "loss": 1.4387,
      "step": 850
    },
    {
      "epoch": 0.38179800221975585,
      "grad_norm": 2.571744918823242,
      "learning_rate": 0.0004364268798105388,
      "loss": 1.1943,
      "step": 860
    },
    {
      "epoch": 0.38623751387347394,
      "grad_norm": 3.9955742359161377,
      "learning_rate": 0.0004356867969212552,
      "loss": 1.5068,
      "step": 870
    },
    {
      "epoch": 0.390677025527192,
      "grad_norm": 1.9013206958770752,
      "learning_rate": 0.0004349467140319716,
      "loss": 1.3918,
      "step": 880
    },
    {
      "epoch": 0.3951165371809101,
      "grad_norm": 2.1233904361724854,
      "learning_rate": 0.000434206631142688,
      "loss": 1.3743,
      "step": 890
    },
    {
      "epoch": 0.3995560488346282,
      "grad_norm": 2.040383815765381,
      "learning_rate": 0.00043346654825340435,
      "loss": 1.3621,
      "step": 900
    },
    {
      "epoch": 0.4039955604883463,
      "grad_norm": 3.0131123065948486,
      "learning_rate": 0.0004327264653641208,
      "loss": 1.4435,
      "step": 910
    },
    {
      "epoch": 0.40843507214206437,
      "grad_norm": 3.830791711807251,
      "learning_rate": 0.00043198638247483716,
      "loss": 1.3214,
      "step": 920
    },
    {
      "epoch": 0.41287458379578246,
      "grad_norm": 2.083017349243164,
      "learning_rate": 0.0004312462995855536,
      "loss": 1.4802,
      "step": 930
    },
    {
      "epoch": 0.41731409544950054,
      "grad_norm": 2.2863805294036865,
      "learning_rate": 0.00043050621669627,
      "loss": 1.35,
      "step": 940
    },
    {
      "epoch": 0.42175360710321863,
      "grad_norm": 3.8747406005859375,
      "learning_rate": 0.00042976613380698637,
      "loss": 1.2948,
      "step": 950
    },
    {
      "epoch": 0.4261931187569367,
      "grad_norm": 2.510215997695923,
      "learning_rate": 0.00042902605091770283,
      "loss": 1.3056,
      "step": 960
    },
    {
      "epoch": 0.4306326304106548,
      "grad_norm": 2.4046311378479004,
      "learning_rate": 0.0004282859680284192,
      "loss": 1.3578,
      "step": 970
    },
    {
      "epoch": 0.43507214206437295,
      "grad_norm": 1.803960919380188,
      "learning_rate": 0.00042754588513913564,
      "loss": 1.4015,
      "step": 980
    },
    {
      "epoch": 0.43951165371809103,
      "grad_norm": 1.5232865810394287,
      "learning_rate": 0.000426805802249852,
      "loss": 1.4134,
      "step": 990
    },
    {
      "epoch": 0.4439511653718091,
      "grad_norm": 1.850748062133789,
      "learning_rate": 0.0004260657193605684,
      "loss": 1.2966,
      "step": 1000
    },
    {
      "epoch": 0.4483906770255272,
      "grad_norm": 3.298250198364258,
      "learning_rate": 0.0004253256364712848,
      "loss": 1.301,
      "step": 1010
    },
    {
      "epoch": 0.4528301886792453,
      "grad_norm": 2.6895077228546143,
      "learning_rate": 0.0004246595618709296,
      "loss": 1.2867,
      "step": 1020
    },
    {
      "epoch": 0.4572697003329634,
      "grad_norm": 2.4650585651397705,
      "learning_rate": 0.00042391947898164594,
      "loss": 1.3447,
      "step": 1030
    },
    {
      "epoch": 0.46170921198668147,
      "grad_norm": 2.47660756111145,
      "learning_rate": 0.00042317939609236234,
      "loss": 1.3688,
      "step": 1040
    },
    {
      "epoch": 0.46614872364039955,
      "grad_norm": 2.0048694610595703,
      "learning_rate": 0.00042243931320307875,
      "loss": 1.3055,
      "step": 1050
    },
    {
      "epoch": 0.47058823529411764,
      "grad_norm": 2.7688956260681152,
      "learning_rate": 0.00042169923031379515,
      "loss": 1.3676,
      "step": 1060
    },
    {
      "epoch": 0.4750277469478357,
      "grad_norm": 2.5110268592834473,
      "learning_rate": 0.00042095914742451156,
      "loss": 1.3522,
      "step": 1070
    },
    {
      "epoch": 0.4794672586015538,
      "grad_norm": 2.1339023113250732,
      "learning_rate": 0.00042021906453522796,
      "loss": 1.239,
      "step": 1080
    },
    {
      "epoch": 0.4839067702552719,
      "grad_norm": 2.849062204360962,
      "learning_rate": 0.0004194789816459443,
      "loss": 1.3378,
      "step": 1090
    },
    {
      "epoch": 0.48834628190899,
      "grad_norm": 2.0617716312408447,
      "learning_rate": 0.00041873889875666077,
      "loss": 1.2474,
      "step": 1100
    },
    {
      "epoch": 0.49278579356270813,
      "grad_norm": 2.1087727546691895,
      "learning_rate": 0.0004179988158673771,
      "loss": 1.2378,
      "step": 1110
    },
    {
      "epoch": 0.4972253052164262,
      "grad_norm": 2.3770649433135986,
      "learning_rate": 0.0004172587329780936,
      "loss": 1.2649,
      "step": 1120
    },
    {
      "epoch": 0.5016648168701443,
      "grad_norm": 2.892982244491577,
      "learning_rate": 0.00041651865008881,
      "loss": 1.3096,
      "step": 1130
    },
    {
      "epoch": 0.5061043285238623,
      "grad_norm": 3.1324703693389893,
      "learning_rate": 0.00041577856719952634,
      "loss": 1.3468,
      "step": 1140
    },
    {
      "epoch": 0.5105438401775805,
      "grad_norm": 2.302788019180298,
      "learning_rate": 0.0004150384843102428,
      "loss": 1.5017,
      "step": 1150
    },
    {
      "epoch": 0.5149833518312985,
      "grad_norm": 2.7934770584106445,
      "learning_rate": 0.00041429840142095915,
      "loss": 1.1984,
      "step": 1160
    },
    {
      "epoch": 0.5194228634850167,
      "grad_norm": 2.149888038635254,
      "learning_rate": 0.0004135583185316756,
      "loss": 1.4468,
      "step": 1170
    },
    {
      "epoch": 0.5238623751387348,
      "grad_norm": 2.188237428665161,
      "learning_rate": 0.00041281823564239196,
      "loss": 1.2426,
      "step": 1180
    },
    {
      "epoch": 0.5283018867924528,
      "grad_norm": 2.5798327922821045,
      "learning_rate": 0.00041207815275310836,
      "loss": 1.3429,
      "step": 1190
    },
    {
      "epoch": 0.532741398446171,
      "grad_norm": 2.389024019241333,
      "learning_rate": 0.00041133806986382477,
      "loss": 1.349,
      "step": 1200
    },
    {
      "epoch": 0.537180910099889,
      "grad_norm": 1.64403235912323,
      "learning_rate": 0.00041059798697454117,
      "loss": 1.3302,
      "step": 1210
    },
    {
      "epoch": 0.5416204217536071,
      "grad_norm": 1.824286699295044,
      "learning_rate": 0.0004098579040852576,
      "loss": 1.3638,
      "step": 1220
    },
    {
      "epoch": 0.5460599334073252,
      "grad_norm": 1.6353033781051636,
      "learning_rate": 0.000409117821195974,
      "loss": 1.47,
      "step": 1230
    },
    {
      "epoch": 0.5504994450610433,
      "grad_norm": 1.9310777187347412,
      "learning_rate": 0.00040837773830669033,
      "loss": 1.3305,
      "step": 1240
    },
    {
      "epoch": 0.5549389567147613,
      "grad_norm": 2.2481529712677,
      "learning_rate": 0.0004076376554174068,
      "loss": 1.4085,
      "step": 1250
    },
    {
      "epoch": 0.5593784683684795,
      "grad_norm": 2.4630343914031982,
      "learning_rate": 0.00040689757252812314,
      "loss": 1.2791,
      "step": 1260
    },
    {
      "epoch": 0.5638179800221975,
      "grad_norm": 1.8748359680175781,
      "learning_rate": 0.0004061574896388396,
      "loss": 1.1222,
      "step": 1270
    },
    {
      "epoch": 0.5682574916759157,
      "grad_norm": 2.4667835235595703,
      "learning_rate": 0.00040541740674955595,
      "loss": 1.2666,
      "step": 1280
    },
    {
      "epoch": 0.5726970033296337,
      "grad_norm": 2.185810089111328,
      "learning_rate": 0.00040467732386027236,
      "loss": 1.2362,
      "step": 1290
    },
    {
      "epoch": 0.5771365149833518,
      "grad_norm": 2.8877182006835938,
      "learning_rate": 0.00040393724097098876,
      "loss": 1.4745,
      "step": 1300
    },
    {
      "epoch": 0.58157602663707,
      "grad_norm": 2.3007447719573975,
      "learning_rate": 0.00040319715808170517,
      "loss": 1.4565,
      "step": 1310
    },
    {
      "epoch": 0.586015538290788,
      "grad_norm": 2.2806577682495117,
      "learning_rate": 0.00040245707519242157,
      "loss": 1.2963,
      "step": 1320
    },
    {
      "epoch": 0.5904550499445061,
      "grad_norm": 2.3302412033081055,
      "learning_rate": 0.000401716992303138,
      "loss": 1.2644,
      "step": 1330
    },
    {
      "epoch": 0.5948945615982242,
      "grad_norm": 2.422548532485962,
      "learning_rate": 0.0004009769094138543,
      "loss": 1.3935,
      "step": 1340
    },
    {
      "epoch": 0.5993340732519423,
      "grad_norm": 2.364901542663574,
      "learning_rate": 0.0004002368265245708,
      "loss": 1.2859,
      "step": 1350
    },
    {
      "epoch": 0.6037735849056604,
      "grad_norm": 2.2721052169799805,
      "learning_rate": 0.00039949674363528714,
      "loss": 1.2598,
      "step": 1360
    },
    {
      "epoch": 0.6082130965593785,
      "grad_norm": 2.7273976802825928,
      "learning_rate": 0.0003987566607460036,
      "loss": 1.4341,
      "step": 1370
    },
    {
      "epoch": 0.6126526082130965,
      "grad_norm": 2.987922191619873,
      "learning_rate": 0.00039801657785671995,
      "loss": 1.357,
      "step": 1380
    },
    {
      "epoch": 0.6170921198668147,
      "grad_norm": 2.6135194301605225,
      "learning_rate": 0.00039727649496743635,
      "loss": 1.3516,
      "step": 1390
    },
    {
      "epoch": 0.6215316315205327,
      "grad_norm": 2.7479171752929688,
      "learning_rate": 0.00039653641207815276,
      "loss": 1.3379,
      "step": 1400
    },
    {
      "epoch": 0.6259711431742508,
      "grad_norm": 2.23116135597229,
      "learning_rate": 0.00039579632918886916,
      "loss": 1.2953,
      "step": 1410
    },
    {
      "epoch": 0.6304106548279689,
      "grad_norm": 2.293957471847534,
      "learning_rate": 0.00039505624629958556,
      "loss": 1.3825,
      "step": 1420
    },
    {
      "epoch": 0.634850166481687,
      "grad_norm": 2.1172266006469727,
      "learning_rate": 0.00039431616341030197,
      "loss": 1.3515,
      "step": 1430
    },
    {
      "epoch": 0.6392896781354052,
      "grad_norm": 2.479458808898926,
      "learning_rate": 0.0003935760805210183,
      "loss": 1.1927,
      "step": 1440
    },
    {
      "epoch": 0.6437291897891232,
      "grad_norm": 2.385388135910034,
      "learning_rate": 0.0003928359976317348,
      "loss": 1.3524,
      "step": 1450
    },
    {
      "epoch": 0.6481687014428413,
      "grad_norm": 2.0675551891326904,
      "learning_rate": 0.00039209591474245113,
      "loss": 1.3568,
      "step": 1460
    },
    {
      "epoch": 0.6526082130965594,
      "grad_norm": 2.1671926975250244,
      "learning_rate": 0.0003913558318531676,
      "loss": 1.3033,
      "step": 1470
    },
    {
      "epoch": 0.6570477247502775,
      "grad_norm": 2.717130661010742,
      "learning_rate": 0.00039061574896388394,
      "loss": 1.3575,
      "step": 1480
    },
    {
      "epoch": 0.6614872364039955,
      "grad_norm": 2.779754638671875,
      "learning_rate": 0.00038987566607460035,
      "loss": 1.3663,
      "step": 1490
    },
    {
      "epoch": 0.6659267480577137,
      "grad_norm": 1.8778971433639526,
      "learning_rate": 0.0003891355831853168,
      "loss": 1.2533,
      "step": 1500
    },
    {
      "epoch": 0.6703662597114317,
      "grad_norm": 3.0039737224578857,
      "learning_rate": 0.00038839550029603315,
      "loss": 1.4432,
      "step": 1510
    },
    {
      "epoch": 0.6748057713651499,
      "grad_norm": 2.170186758041382,
      "learning_rate": 0.0003876554174067496,
      "loss": 1.378,
      "step": 1520
    },
    {
      "epoch": 0.6792452830188679,
      "grad_norm": 3.1594653129577637,
      "learning_rate": 0.00038691533451746596,
      "loss": 1.4479,
      "step": 1530
    },
    {
      "epoch": 0.683684794672586,
      "grad_norm": 3.039705514907837,
      "learning_rate": 0.00038617525162818237,
      "loss": 1.2748,
      "step": 1540
    },
    {
      "epoch": 0.6881243063263041,
      "grad_norm": 3.3899080753326416,
      "learning_rate": 0.0003854351687388988,
      "loss": 1.3297,
      "step": 1550
    },
    {
      "epoch": 0.6925638179800222,
      "grad_norm": 2.6969168186187744,
      "learning_rate": 0.0003846950858496152,
      "loss": 1.3423,
      "step": 1560
    },
    {
      "epoch": 0.6970033296337403,
      "grad_norm": 2.2130165100097656,
      "learning_rate": 0.0003839550029603316,
      "loss": 1.3877,
      "step": 1570
    },
    {
      "epoch": 0.7014428412874584,
      "grad_norm": 2.1677567958831787,
      "learning_rate": 0.000383214920071048,
      "loss": 1.265,
      "step": 1580
    },
    {
      "epoch": 0.7058823529411765,
      "grad_norm": 2.1968941688537598,
      "learning_rate": 0.00038247483718176434,
      "loss": 1.2592,
      "step": 1590
    },
    {
      "epoch": 0.7103218645948945,
      "grad_norm": 2.5119307041168213,
      "learning_rate": 0.0003817347542924808,
      "loss": 1.2195,
      "step": 1600
    },
    {
      "epoch": 0.7147613762486127,
      "grad_norm": 1.9480350017547607,
      "learning_rate": 0.00038099467140319715,
      "loss": 1.303,
      "step": 1610
    },
    {
      "epoch": 0.7192008879023307,
      "grad_norm": 2.907348155975342,
      "learning_rate": 0.0003802545885139136,
      "loss": 1.3237,
      "step": 1620
    },
    {
      "epoch": 0.7236403995560489,
      "grad_norm": 2.109365224838257,
      "learning_rate": 0.00037951450562462996,
      "loss": 1.2038,
      "step": 1630
    },
    {
      "epoch": 0.7280799112097669,
      "grad_norm": 2.029604434967041,
      "learning_rate": 0.00037877442273534636,
      "loss": 1.2806,
      "step": 1640
    },
    {
      "epoch": 0.732519422863485,
      "grad_norm": 3.7805356979370117,
      "learning_rate": 0.00037803433984606277,
      "loss": 1.3228,
      "step": 1650
    },
    {
      "epoch": 0.7369589345172031,
      "grad_norm": 3.477368116378784,
      "learning_rate": 0.0003772942569567792,
      "loss": 1.2331,
      "step": 1660
    },
    {
      "epoch": 0.7413984461709212,
      "grad_norm": 2.4978432655334473,
      "learning_rate": 0.0003765541740674956,
      "loss": 1.1916,
      "step": 1670
    },
    {
      "epoch": 0.7458379578246392,
      "grad_norm": 2.6162259578704834,
      "learning_rate": 0.000375814091178212,
      "loss": 1.2123,
      "step": 1680
    },
    {
      "epoch": 0.7502774694783574,
      "grad_norm": 2.777238607406616,
      "learning_rate": 0.00037507400828892833,
      "loss": 1.2888,
      "step": 1690
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 2.3157105445861816,
      "learning_rate": 0.0003743339253996448,
      "loss": 1.2471,
      "step": 1700
    },
    {
      "epoch": 0.7591564927857936,
      "grad_norm": 1.8402167558670044,
      "learning_rate": 0.00037359384251036114,
      "loss": 1.3752,
      "step": 1710
    },
    {
      "epoch": 0.7635960044395117,
      "grad_norm": 2.027571439743042,
      "learning_rate": 0.0003728537596210776,
      "loss": 1.2647,
      "step": 1720
    },
    {
      "epoch": 0.7680355160932297,
      "grad_norm": 1.9094743728637695,
      "learning_rate": 0.00037211367673179395,
      "loss": 1.2921,
      "step": 1730
    },
    {
      "epoch": 0.7724750277469479,
      "grad_norm": 2.897094249725342,
      "learning_rate": 0.00037137359384251036,
      "loss": 1.1782,
      "step": 1740
    },
    {
      "epoch": 0.7769145394006659,
      "grad_norm": 2.5221755504608154,
      "learning_rate": 0.00037063351095322676,
      "loss": 1.2724,
      "step": 1750
    },
    {
      "epoch": 0.781354051054384,
      "grad_norm": 2.4435060024261475,
      "learning_rate": 0.00036989342806394317,
      "loss": 1.2431,
      "step": 1760
    },
    {
      "epoch": 0.7857935627081021,
      "grad_norm": 3.433166027069092,
      "learning_rate": 0.0003691533451746596,
      "loss": 1.3318,
      "step": 1770
    },
    {
      "epoch": 0.7902330743618202,
      "grad_norm": 2.768627166748047,
      "learning_rate": 0.000368413262285376,
      "loss": 1.3737,
      "step": 1780
    },
    {
      "epoch": 0.7946725860155383,
      "grad_norm": 3.1287553310394287,
      "learning_rate": 0.00036767317939609233,
      "loss": 1.2251,
      "step": 1790
    },
    {
      "epoch": 0.7991120976692564,
      "grad_norm": 2.128648042678833,
      "learning_rate": 0.0003669330965068088,
      "loss": 1.3406,
      "step": 1800
    },
    {
      "epoch": 0.8035516093229744,
      "grad_norm": 2.511744976043701,
      "learning_rate": 0.00036619301361752514,
      "loss": 1.3364,
      "step": 1810
    },
    {
      "epoch": 0.8079911209766926,
      "grad_norm": 2.834439277648926,
      "learning_rate": 0.0003654529307282416,
      "loss": 1.2263,
      "step": 1820
    },
    {
      "epoch": 0.8124306326304107,
      "grad_norm": 1.8574966192245483,
      "learning_rate": 0.00036471284783895795,
      "loss": 1.2755,
      "step": 1830
    },
    {
      "epoch": 0.8168701442841287,
      "grad_norm": 1.9493478536605835,
      "learning_rate": 0.00036397276494967435,
      "loss": 1.3086,
      "step": 1840
    },
    {
      "epoch": 0.8213096559378469,
      "grad_norm": 3.059087038040161,
      "learning_rate": 0.00036323268206039076,
      "loss": 1.2578,
      "step": 1850
    },
    {
      "epoch": 0.8257491675915649,
      "grad_norm": 3.091503620147705,
      "learning_rate": 0.00036249259917110716,
      "loss": 1.2783,
      "step": 1860
    },
    {
      "epoch": 0.8301886792452831,
      "grad_norm": 2.5705952644348145,
      "learning_rate": 0.00036175251628182357,
      "loss": 1.4447,
      "step": 1870
    },
    {
      "epoch": 0.8346281908990011,
      "grad_norm": 2.274764060974121,
      "learning_rate": 0.00036101243339254,
      "loss": 1.3539,
      "step": 1880
    },
    {
      "epoch": 0.8390677025527192,
      "grad_norm": 3.1099283695220947,
      "learning_rate": 0.0003602723505032564,
      "loss": 1.4636,
      "step": 1890
    },
    {
      "epoch": 0.8435072142064373,
      "grad_norm": 3.4586181640625,
      "learning_rate": 0.0003595322676139728,
      "loss": 1.1836,
      "step": 1900
    },
    {
      "epoch": 0.8479467258601554,
      "grad_norm": 2.310471534729004,
      "learning_rate": 0.0003587921847246892,
      "loss": 1.2528,
      "step": 1910
    },
    {
      "epoch": 0.8523862375138734,
      "grad_norm": 2.177295207977295,
      "learning_rate": 0.0003580521018354056,
      "loss": 1.2325,
      "step": 1920
    },
    {
      "epoch": 0.8568257491675916,
      "grad_norm": 2.4178223609924316,
      "learning_rate": 0.000357312018946122,
      "loss": 1.2742,
      "step": 1930
    },
    {
      "epoch": 0.8612652608213096,
      "grad_norm": 2.4221882820129395,
      "learning_rate": 0.00035657193605683835,
      "loss": 1.2754,
      "step": 1940
    },
    {
      "epoch": 0.8657047724750278,
      "grad_norm": 1.860136866569519,
      "learning_rate": 0.0003558318531675548,
      "loss": 1.2677,
      "step": 1950
    },
    {
      "epoch": 0.8701442841287459,
      "grad_norm": 2.4410409927368164,
      "learning_rate": 0.00035509177027827116,
      "loss": 1.3772,
      "step": 1960
    },
    {
      "epoch": 0.8745837957824639,
      "grad_norm": 2.259046792984009,
      "learning_rate": 0.0003543516873889876,
      "loss": 1.2543,
      "step": 1970
    },
    {
      "epoch": 0.8790233074361821,
      "grad_norm": 1.9376980066299438,
      "learning_rate": 0.00035361160449970397,
      "loss": 1.2474,
      "step": 1980
    },
    {
      "epoch": 0.8834628190899001,
      "grad_norm": 2.565645694732666,
      "learning_rate": 0.0003528715216104204,
      "loss": 1.0465,
      "step": 1990
    },
    {
      "epoch": 0.8879023307436182,
      "grad_norm": 4.752220630645752,
      "learning_rate": 0.0003521314387211368,
      "loss": 1.2269,
      "step": 2000
    },
    {
      "epoch": 0.8923418423973363,
      "grad_norm": 2.784287691116333,
      "learning_rate": 0.0003513913558318532,
      "loss": 1.3076,
      "step": 2010
    },
    {
      "epoch": 0.8967813540510544,
      "grad_norm": 4.195347309112549,
      "learning_rate": 0.0003506512729425696,
      "loss": 1.4301,
      "step": 2020
    },
    {
      "epoch": 0.9012208657047724,
      "grad_norm": 2.292226552963257,
      "learning_rate": 0.000349911190053286,
      "loss": 1.3365,
      "step": 2030
    },
    {
      "epoch": 0.9056603773584906,
      "grad_norm": 2.43320631980896,
      "learning_rate": 0.00034917110716400234,
      "loss": 1.3202,
      "step": 2040
    },
    {
      "epoch": 0.9100998890122086,
      "grad_norm": 2.402198314666748,
      "learning_rate": 0.0003484310242747188,
      "loss": 1.1725,
      "step": 2050
    },
    {
      "epoch": 0.9145394006659268,
      "grad_norm": 2.1881065368652344,
      "learning_rate": 0.00034769094138543515,
      "loss": 1.2839,
      "step": 2060
    },
    {
      "epoch": 0.9189789123196448,
      "grad_norm": 1.8475335836410522,
      "learning_rate": 0.0003469508584961516,
      "loss": 1.2011,
      "step": 2070
    },
    {
      "epoch": 0.9234184239733629,
      "grad_norm": 2.3030402660369873,
      "learning_rate": 0.00034621077560686796,
      "loss": 1.3943,
      "step": 2080
    },
    {
      "epoch": 0.9278579356270811,
      "grad_norm": 2.8198013305664062,
      "learning_rate": 0.00034547069271758437,
      "loss": 1.4321,
      "step": 2090
    },
    {
      "epoch": 0.9322974472807991,
      "grad_norm": 2.1051223278045654,
      "learning_rate": 0.0003447306098283008,
      "loss": 1.2649,
      "step": 2100
    },
    {
      "epoch": 0.9367369589345172,
      "grad_norm": 2.481344699859619,
      "learning_rate": 0.0003439905269390172,
      "loss": 1.2455,
      "step": 2110
    },
    {
      "epoch": 0.9411764705882353,
      "grad_norm": 2.976132392883301,
      "learning_rate": 0.0003432504440497336,
      "loss": 1.2647,
      "step": 2120
    },
    {
      "epoch": 0.9456159822419534,
      "grad_norm": 3.210503339767456,
      "learning_rate": 0.00034251036116045,
      "loss": 1.3676,
      "step": 2130
    },
    {
      "epoch": 0.9500554938956715,
      "grad_norm": 2.400081157684326,
      "learning_rate": 0.00034177027827116634,
      "loss": 1.3116,
      "step": 2140
    },
    {
      "epoch": 0.9544950055493896,
      "grad_norm": 1.8725590705871582,
      "learning_rate": 0.0003410301953818828,
      "loss": 1.1833,
      "step": 2150
    },
    {
      "epoch": 0.9589345172031076,
      "grad_norm": 2.828796863555908,
      "learning_rate": 0.00034029011249259915,
      "loss": 1.176,
      "step": 2160
    },
    {
      "epoch": 0.9633740288568258,
      "grad_norm": 2.2701761722564697,
      "learning_rate": 0.0003395500296033156,
      "loss": 1.325,
      "step": 2170
    },
    {
      "epoch": 0.9678135405105438,
      "grad_norm": 2.2489287853240967,
      "learning_rate": 0.00033880994671403196,
      "loss": 1.3181,
      "step": 2180
    },
    {
      "epoch": 0.9722530521642619,
      "grad_norm": 1.8751778602600098,
      "learning_rate": 0.00033806986382474836,
      "loss": 1.2499,
      "step": 2190
    },
    {
      "epoch": 0.97669256381798,
      "grad_norm": 2.2308638095855713,
      "learning_rate": 0.00033732978093546477,
      "loss": 1.2912,
      "step": 2200
    },
    {
      "epoch": 0.9811320754716981,
      "grad_norm": 2.251229763031006,
      "learning_rate": 0.0003365896980461812,
      "loss": 1.2541,
      "step": 2210
    },
    {
      "epoch": 0.9855715871254163,
      "grad_norm": 2.0617949962615967,
      "learning_rate": 0.0003358496151568976,
      "loss": 1.3543,
      "step": 2220
    },
    {
      "epoch": 0.9900110987791343,
      "grad_norm": 2.9683804512023926,
      "learning_rate": 0.000335109532267614,
      "loss": 1.29,
      "step": 2230
    },
    {
      "epoch": 0.9944506104328524,
      "grad_norm": 3.065110683441162,
      "learning_rate": 0.00033436944937833033,
      "loss": 1.4118,
      "step": 2240
    },
    {
      "epoch": 0.9988901220865705,
      "grad_norm": 2.382269859313965,
      "learning_rate": 0.0003336293664890468,
      "loss": 1.0963,
      "step": 2250
    },
    {
      "epoch": 1.0033296337402886,
      "grad_norm": 1.8833093643188477,
      "learning_rate": 0.0003328892835997632,
      "loss": 1.2489,
      "step": 2260
    },
    {
      "epoch": 1.0077691453940067,
      "grad_norm": 1.7492579221725464,
      "learning_rate": 0.0003321492007104796,
      "loss": 1.2988,
      "step": 2270
    },
    {
      "epoch": 1.0122086570477247,
      "grad_norm": 2.5458309650421143,
      "learning_rate": 0.000331409117821196,
      "loss": 1.2664,
      "step": 2280
    },
    {
      "epoch": 1.0166481687014428,
      "grad_norm": 2.1255764961242676,
      "learning_rate": 0.00033066903493191236,
      "loss": 1.122,
      "step": 2290
    },
    {
      "epoch": 1.021087680355161,
      "grad_norm": 2.2985191345214844,
      "learning_rate": 0.0003299289520426288,
      "loss": 1.1377,
      "step": 2300
    },
    {
      "epoch": 1.025527192008879,
      "grad_norm": 2.472223997116089,
      "learning_rate": 0.00032918886915334517,
      "loss": 1.2435,
      "step": 2310
    },
    {
      "epoch": 1.029966703662597,
      "grad_norm": 2.3725171089172363,
      "learning_rate": 0.00032844878626406163,
      "loss": 1.2156,
      "step": 2320
    },
    {
      "epoch": 1.0344062153163152,
      "grad_norm": 2.1236491203308105,
      "learning_rate": 0.000327708703374778,
      "loss": 1.0609,
      "step": 2330
    },
    {
      "epoch": 1.0388457269700333,
      "grad_norm": 2.015129566192627,
      "learning_rate": 0.0003269686204854944,
      "loss": 1.2746,
      "step": 2340
    },
    {
      "epoch": 1.0432852386237514,
      "grad_norm": 2.806223154067993,
      "learning_rate": 0.0003262285375962108,
      "loss": 1.2457,
      "step": 2350
    },
    {
      "epoch": 1.0477247502774696,
      "grad_norm": 2.469069004058838,
      "learning_rate": 0.0003254884547069272,
      "loss": 1.2986,
      "step": 2360
    },
    {
      "epoch": 1.0521642619311875,
      "grad_norm": 2.2171850204467773,
      "learning_rate": 0.0003247483718176436,
      "loss": 1.2312,
      "step": 2370
    },
    {
      "epoch": 1.0566037735849056,
      "grad_norm": 2.004631996154785,
      "learning_rate": 0.00032400828892836,
      "loss": 1.2888,
      "step": 2380
    },
    {
      "epoch": 1.0610432852386238,
      "grad_norm": 3.481259822845459,
      "learning_rate": 0.00032326820603907635,
      "loss": 1.1646,
      "step": 2390
    },
    {
      "epoch": 1.065482796892342,
      "grad_norm": 3.5468332767486572,
      "learning_rate": 0.0003225281231497928,
      "loss": 1.2545,
      "step": 2400
    },
    {
      "epoch": 1.0699223085460599,
      "grad_norm": 2.489809274673462,
      "learning_rate": 0.00032178804026050916,
      "loss": 1.1666,
      "step": 2410
    },
    {
      "epoch": 1.074361820199778,
      "grad_norm": 1.7560049295425415,
      "learning_rate": 0.0003210479573712256,
      "loss": 1.1918,
      "step": 2420
    },
    {
      "epoch": 1.0788013318534961,
      "grad_norm": 2.6162755489349365,
      "learning_rate": 0.00032030787448194197,
      "loss": 1.1915,
      "step": 2430
    },
    {
      "epoch": 1.0832408435072143,
      "grad_norm": 2.567081928253174,
      "learning_rate": 0.0003195677915926584,
      "loss": 1.2986,
      "step": 2440
    },
    {
      "epoch": 1.0876803551609322,
      "grad_norm": 2.2984960079193115,
      "learning_rate": 0.0003188277087033748,
      "loss": 1.1703,
      "step": 2450
    },
    {
      "epoch": 1.0921198668146503,
      "grad_norm": 2.108612537384033,
      "learning_rate": 0.0003180876258140912,
      "loss": 1.2901,
      "step": 2460
    },
    {
      "epoch": 1.0965593784683685,
      "grad_norm": 2.3637850284576416,
      "learning_rate": 0.0003173475429248076,
      "loss": 1.1323,
      "step": 2470
    },
    {
      "epoch": 1.1009988901220866,
      "grad_norm": 1.8329159021377563,
      "learning_rate": 0.000316607460035524,
      "loss": 1.3127,
      "step": 2480
    },
    {
      "epoch": 1.1054384017758045,
      "grad_norm": 2.1613948345184326,
      "learning_rate": 0.00031586737714624035,
      "loss": 1.3645,
      "step": 2490
    },
    {
      "epoch": 1.1098779134295227,
      "grad_norm": 3.2343435287475586,
      "learning_rate": 0.0003151272942569568,
      "loss": 1.3153,
      "step": 2500
    },
    {
      "epoch": 1.1143174250832408,
      "grad_norm": 2.944899797439575,
      "learning_rate": 0.00031438721136767316,
      "loss": 1.211,
      "step": 2510
    },
    {
      "epoch": 1.118756936736959,
      "grad_norm": 2.2123000621795654,
      "learning_rate": 0.0003136471284783896,
      "loss": 1.2605,
      "step": 2520
    },
    {
      "epoch": 1.1231964483906771,
      "grad_norm": 2.521188735961914,
      "learning_rate": 0.00031290704558910597,
      "loss": 1.1525,
      "step": 2530
    },
    {
      "epoch": 1.127635960044395,
      "grad_norm": 2.9518613815307617,
      "learning_rate": 0.00031216696269982237,
      "loss": 1.4673,
      "step": 2540
    },
    {
      "epoch": 1.1320754716981132,
      "grad_norm": 2.6925246715545654,
      "learning_rate": 0.0003114268798105388,
      "loss": 1.3116,
      "step": 2550
    },
    {
      "epoch": 1.1365149833518313,
      "grad_norm": 2.416891574859619,
      "learning_rate": 0.0003106867969212552,
      "loss": 1.367,
      "step": 2560
    },
    {
      "epoch": 1.1409544950055495,
      "grad_norm": 2.159923553466797,
      "learning_rate": 0.0003099467140319716,
      "loss": 1.1389,
      "step": 2570
    },
    {
      "epoch": 1.1453940066592674,
      "grad_norm": 2.171494245529175,
      "learning_rate": 0.000309206631142688,
      "loss": 1.1552,
      "step": 2580
    },
    {
      "epoch": 1.1498335183129855,
      "grad_norm": 2.370046377182007,
      "learning_rate": 0.00030846654825340434,
      "loss": 1.2822,
      "step": 2590
    },
    {
      "epoch": 1.1542730299667037,
      "grad_norm": 2.736478567123413,
      "learning_rate": 0.0003077264653641208,
      "loss": 1.2089,
      "step": 2600
    },
    {
      "epoch": 1.1587125416204218,
      "grad_norm": 3.6072492599487305,
      "learning_rate": 0.00030698638247483715,
      "loss": 1.2394,
      "step": 2610
    },
    {
      "epoch": 1.16315205327414,
      "grad_norm": 2.3369784355163574,
      "learning_rate": 0.0003062462995855536,
      "loss": 1.4634,
      "step": 2620
    },
    {
      "epoch": 1.1675915649278579,
      "grad_norm": 2.442995071411133,
      "learning_rate": 0.00030550621669627,
      "loss": 1.213,
      "step": 2630
    },
    {
      "epoch": 1.172031076581576,
      "grad_norm": 2.9687016010284424,
      "learning_rate": 0.00030476613380698637,
      "loss": 1.1588,
      "step": 2640
    },
    {
      "epoch": 1.1764705882352942,
      "grad_norm": 3.002044677734375,
      "learning_rate": 0.0003040260509177028,
      "loss": 1.1027,
      "step": 2650
    },
    {
      "epoch": 1.1809100998890123,
      "grad_norm": 2.1949100494384766,
      "learning_rate": 0.0003032859680284192,
      "loss": 1.227,
      "step": 2660
    },
    {
      "epoch": 1.1853496115427302,
      "grad_norm": 2.2456674575805664,
      "learning_rate": 0.00030254588513913564,
      "loss": 1.0935,
      "step": 2670
    },
    {
      "epoch": 1.1897891231964484,
      "grad_norm": 3.1037633419036865,
      "learning_rate": 0.000301805802249852,
      "loss": 1.2114,
      "step": 2680
    },
    {
      "epoch": 1.1942286348501665,
      "grad_norm": 1.8266792297363281,
      "learning_rate": 0.0003010657193605684,
      "loss": 1.1689,
      "step": 2690
    },
    {
      "epoch": 1.1986681465038846,
      "grad_norm": 2.212174415588379,
      "learning_rate": 0.0003003256364712848,
      "loss": 1.2804,
      "step": 2700
    },
    {
      "epoch": 1.2031076581576028,
      "grad_norm": 2.7373647689819336,
      "learning_rate": 0.0002995855535820012,
      "loss": 1.3174,
      "step": 2710
    },
    {
      "epoch": 1.2075471698113207,
      "grad_norm": 2.871727228164673,
      "learning_rate": 0.0002988454706927176,
      "loss": 1.2202,
      "step": 2720
    },
    {
      "epoch": 1.2119866814650389,
      "grad_norm": 2.1525402069091797,
      "learning_rate": 0.000298105387803434,
      "loss": 1.1276,
      "step": 2730
    },
    {
      "epoch": 1.216426193118757,
      "grad_norm": 2.4317424297332764,
      "learning_rate": 0.00029736530491415036,
      "loss": 1.2066,
      "step": 2740
    },
    {
      "epoch": 1.220865704772475,
      "grad_norm": 2.238224506378174,
      "learning_rate": 0.0002966252220248668,
      "loss": 1.1431,
      "step": 2750
    },
    {
      "epoch": 1.225305216426193,
      "grad_norm": 2.328303813934326,
      "learning_rate": 0.00029588513913558317,
      "loss": 1.207,
      "step": 2760
    },
    {
      "epoch": 1.2297447280799112,
      "grad_norm": 2.8975861072540283,
      "learning_rate": 0.00029514505624629963,
      "loss": 1.2341,
      "step": 2770
    },
    {
      "epoch": 1.2341842397336293,
      "grad_norm": 1.982830286026001,
      "learning_rate": 0.000294404973357016,
      "loss": 1.18,
      "step": 2780
    },
    {
      "epoch": 1.2386237513873475,
      "grad_norm": 2.5605311393737793,
      "learning_rate": 0.0002936648904677324,
      "loss": 1.0867,
      "step": 2790
    },
    {
      "epoch": 1.2430632630410654,
      "grad_norm": 1.880393624305725,
      "learning_rate": 0.0002929248075784488,
      "loss": 1.1795,
      "step": 2800
    },
    {
      "epoch": 1.2475027746947835,
      "grad_norm": 2.493835210800171,
      "learning_rate": 0.0002921847246891652,
      "loss": 1.1725,
      "step": 2810
    },
    {
      "epoch": 1.2519422863485017,
      "grad_norm": 3.806032419204712,
      "learning_rate": 0.0002914446417998816,
      "loss": 1.1731,
      "step": 2820
    },
    {
      "epoch": 1.2563817980022198,
      "grad_norm": 1.874725580215454,
      "learning_rate": 0.000290704558910598,
      "loss": 1.1509,
      "step": 2830
    },
    {
      "epoch": 1.2608213096559377,
      "grad_norm": 2.4433417320251465,
      "learning_rate": 0.00028996447602131436,
      "loss": 1.3995,
      "step": 2840
    },
    {
      "epoch": 1.265260821309656,
      "grad_norm": 2.454763174057007,
      "learning_rate": 0.0002892243931320308,
      "loss": 1.186,
      "step": 2850
    },
    {
      "epoch": 1.269700332963374,
      "grad_norm": 2.5560622215270996,
      "learning_rate": 0.00028848431024274717,
      "loss": 1.2642,
      "step": 2860
    },
    {
      "epoch": 1.2741398446170922,
      "grad_norm": 2.6806108951568604,
      "learning_rate": 0.0002877442273534636,
      "loss": 1.1983,
      "step": 2870
    },
    {
      "epoch": 1.2785793562708103,
      "grad_norm": 2.1315486431121826,
      "learning_rate": 0.00028700414446418,
      "loss": 1.18,
      "step": 2880
    },
    {
      "epoch": 1.2830188679245282,
      "grad_norm": 2.376688003540039,
      "learning_rate": 0.0002862640615748964,
      "loss": 1.2564,
      "step": 2890
    },
    {
      "epoch": 1.2874583795782464,
      "grad_norm": 3.168863534927368,
      "learning_rate": 0.0002855239786856128,
      "loss": 1.2495,
      "step": 2900
    },
    {
      "epoch": 1.2918978912319645,
      "grad_norm": 2.698871374130249,
      "learning_rate": 0.0002847838957963292,
      "loss": 1.1881,
      "step": 2910
    },
    {
      "epoch": 1.2963374028856824,
      "grad_norm": 2.199425458908081,
      "learning_rate": 0.0002840438129070456,
      "loss": 1.3054,
      "step": 2920
    },
    {
      "epoch": 1.3007769145394006,
      "grad_norm": 2.5704493522644043,
      "learning_rate": 0.000283303730017762,
      "loss": 1.2326,
      "step": 2930
    },
    {
      "epoch": 1.3052164261931187,
      "grad_norm": 1.8353842496871948,
      "learning_rate": 0.00028256364712847835,
      "loss": 1.1732,
      "step": 2940
    },
    {
      "epoch": 1.3096559378468369,
      "grad_norm": 2.8656632900238037,
      "learning_rate": 0.0002818235642391948,
      "loss": 1.2007,
      "step": 2950
    },
    {
      "epoch": 1.314095449500555,
      "grad_norm": 2.2096683979034424,
      "learning_rate": 0.00028108348134991116,
      "loss": 1.0575,
      "step": 2960
    },
    {
      "epoch": 1.3185349611542732,
      "grad_norm": 1.6169360876083374,
      "learning_rate": 0.0002803433984606276,
      "loss": 1.2676,
      "step": 2970
    },
    {
      "epoch": 1.322974472807991,
      "grad_norm": 2.2629027366638184,
      "learning_rate": 0.00027960331557134397,
      "loss": 1.105,
      "step": 2980
    },
    {
      "epoch": 1.3274139844617092,
      "grad_norm": 2.7655019760131836,
      "learning_rate": 0.0002788632326820604,
      "loss": 1.415,
      "step": 2990
    },
    {
      "epoch": 1.3318534961154274,
      "grad_norm": 3.3843798637390137,
      "learning_rate": 0.0002781231497927768,
      "loss": 1.2296,
      "step": 3000
    },
    {
      "epoch": 1.3362930077691453,
      "grad_norm": 2.173914670944214,
      "learning_rate": 0.0002773830669034932,
      "loss": 1.2744,
      "step": 3010
    },
    {
      "epoch": 1.3407325194228634,
      "grad_norm": 2.6042745113372803,
      "learning_rate": 0.00027664298401420965,
      "loss": 1.1579,
      "step": 3020
    },
    {
      "epoch": 1.3451720310765816,
      "grad_norm": 2.8949265480041504,
      "learning_rate": 0.000275902901124926,
      "loss": 1.1775,
      "step": 3030
    },
    {
      "epoch": 1.3496115427302997,
      "grad_norm": 2.3167853355407715,
      "learning_rate": 0.0002751628182356424,
      "loss": 1.1783,
      "step": 3040
    },
    {
      "epoch": 1.3540510543840178,
      "grad_norm": 1.7227991819381714,
      "learning_rate": 0.0002744227353463588,
      "loss": 1.2564,
      "step": 3050
    },
    {
      "epoch": 1.3584905660377358,
      "grad_norm": 2.2688581943511963,
      "learning_rate": 0.0002736826524570752,
      "loss": 1.1192,
      "step": 3060
    },
    {
      "epoch": 1.362930077691454,
      "grad_norm": 2.0362861156463623,
      "learning_rate": 0.0002729425695677916,
      "loss": 1.198,
      "step": 3070
    },
    {
      "epoch": 1.367369589345172,
      "grad_norm": 2.449152946472168,
      "learning_rate": 0.000272202486678508,
      "loss": 1.1914,
      "step": 3080
    },
    {
      "epoch": 1.3718091009988902,
      "grad_norm": 1.9924858808517456,
      "learning_rate": 0.00027146240378922437,
      "loss": 1.1452,
      "step": 3090
    },
    {
      "epoch": 1.3762486126526081,
      "grad_norm": 2.1075618267059326,
      "learning_rate": 0.00027072232089994083,
      "loss": 1.2138,
      "step": 3100
    },
    {
      "epoch": 1.3806881243063263,
      "grad_norm": 2.448345184326172,
      "learning_rate": 0.0002699822380106572,
      "loss": 1.2074,
      "step": 3110
    },
    {
      "epoch": 1.3851276359600444,
      "grad_norm": 3.132848024368286,
      "learning_rate": 0.00026924215512137364,
      "loss": 1.2549,
      "step": 3120
    },
    {
      "epoch": 1.3895671476137625,
      "grad_norm": 2.8190388679504395,
      "learning_rate": 0.00026850207223209,
      "loss": 1.092,
      "step": 3130
    },
    {
      "epoch": 1.3940066592674807,
      "grad_norm": 3.3462393283843994,
      "learning_rate": 0.0002677619893428064,
      "loss": 1.2195,
      "step": 3140
    },
    {
      "epoch": 1.3984461709211986,
      "grad_norm": 2.1941757202148438,
      "learning_rate": 0.0002670219064535228,
      "loss": 1.2653,
      "step": 3150
    },
    {
      "epoch": 1.4028856825749167,
      "grad_norm": 2.6809446811676025,
      "learning_rate": 0.0002662818235642392,
      "loss": 1.2041,
      "step": 3160
    },
    {
      "epoch": 1.407325194228635,
      "grad_norm": 2.210653305053711,
      "learning_rate": 0.0002655417406749556,
      "loss": 1.1725,
      "step": 3170
    },
    {
      "epoch": 1.4117647058823528,
      "grad_norm": 2.1755423545837402,
      "learning_rate": 0.000264801657785672,
      "loss": 1.2657,
      "step": 3180
    },
    {
      "epoch": 1.416204217536071,
      "grad_norm": 3.3633124828338623,
      "learning_rate": 0.00026406157489638837,
      "loss": 1.07,
      "step": 3190
    },
    {
      "epoch": 1.420643729189789,
      "grad_norm": 2.594770908355713,
      "learning_rate": 0.0002633214920071048,
      "loss": 1.1718,
      "step": 3200
    },
    {
      "epoch": 1.4250832408435072,
      "grad_norm": 3.1605348587036133,
      "learning_rate": 0.0002625814091178212,
      "loss": 1.2052,
      "step": 3210
    },
    {
      "epoch": 1.4295227524972254,
      "grad_norm": 2.416339874267578,
      "learning_rate": 0.00026184132622853764,
      "loss": 1.2276,
      "step": 3220
    },
    {
      "epoch": 1.4339622641509435,
      "grad_norm": 1.6244300603866577,
      "learning_rate": 0.000261101243339254,
      "loss": 1.2202,
      "step": 3230
    },
    {
      "epoch": 1.4384017758046614,
      "grad_norm": 1.882071614265442,
      "learning_rate": 0.0002603611604499704,
      "loss": 1.2609,
      "step": 3240
    },
    {
      "epoch": 1.4428412874583796,
      "grad_norm": 2.7860050201416016,
      "learning_rate": 0.0002596210775606868,
      "loss": 1.1403,
      "step": 3250
    },
    {
      "epoch": 1.4472807991120977,
      "grad_norm": 3.4615440368652344,
      "learning_rate": 0.0002588809946714032,
      "loss": 1.1831,
      "step": 3260
    },
    {
      "epoch": 1.4517203107658156,
      "grad_norm": 2.7832460403442383,
      "learning_rate": 0.0002581409117821196,
      "loss": 1.1472,
      "step": 3270
    },
    {
      "epoch": 1.4561598224195338,
      "grad_norm": 3.3921632766723633,
      "learning_rate": 0.000257400828892836,
      "loss": 1.1578,
      "step": 3280
    },
    {
      "epoch": 1.460599334073252,
      "grad_norm": 2.404139995574951,
      "learning_rate": 0.00025666074600355236,
      "loss": 1.237,
      "step": 3290
    },
    {
      "epoch": 1.46503884572697,
      "grad_norm": 1.8011175394058228,
      "learning_rate": 0.0002559206631142688,
      "loss": 1.2906,
      "step": 3300
    },
    {
      "epoch": 1.4694783573806882,
      "grad_norm": 2.6748876571655273,
      "learning_rate": 0.00025518058022498517,
      "loss": 1.1754,
      "step": 3310
    },
    {
      "epoch": 1.4739178690344061,
      "grad_norm": 2.1977102756500244,
      "learning_rate": 0.00025444049733570163,
      "loss": 1.229,
      "step": 3320
    },
    {
      "epoch": 1.4783573806881243,
      "grad_norm": 2.5084972381591797,
      "learning_rate": 0.000253700414446418,
      "loss": 1.0321,
      "step": 3330
    },
    {
      "epoch": 1.4827968923418424,
      "grad_norm": 3.0747833251953125,
      "learning_rate": 0.0002529603315571344,
      "loss": 1.2771,
      "step": 3340
    },
    {
      "epoch": 1.4872364039955606,
      "grad_norm": 3.102553129196167,
      "learning_rate": 0.0002522202486678508,
      "loss": 1.1329,
      "step": 3350
    },
    {
      "epoch": 1.4916759156492785,
      "grad_norm": 2.4768261909484863,
      "learning_rate": 0.0002514801657785672,
      "loss": 1.2519,
      "step": 3360
    },
    {
      "epoch": 1.4961154273029966,
      "grad_norm": 2.7730607986450195,
      "learning_rate": 0.0002507400828892836,
      "loss": 1.0882,
      "step": 3370
    },
    {
      "epoch": 1.5005549389567148,
      "grad_norm": 2.8421099185943604,
      "learning_rate": 0.00025,
      "loss": 1.1584,
      "step": 3380
    },
    {
      "epoch": 1.504994450610433,
      "grad_norm": 5.1660075187683105,
      "learning_rate": 0.0002492599171107164,
      "loss": 1.3548,
      "step": 3390
    },
    {
      "epoch": 1.509433962264151,
      "grad_norm": 2.1417887210845947,
      "learning_rate": 0.0002485198342214328,
      "loss": 1.2616,
      "step": 3400
    },
    {
      "epoch": 1.5138734739178692,
      "grad_norm": 3.2966387271881104,
      "learning_rate": 0.0002477797513321492,
      "loss": 1.1468,
      "step": 3410
    },
    {
      "epoch": 1.5183129855715871,
      "grad_norm": 2.6271238327026367,
      "learning_rate": 0.0002470396684428656,
      "loss": 1.3465,
      "step": 3420
    },
    {
      "epoch": 1.5227524972253053,
      "grad_norm": 2.494246006011963,
      "learning_rate": 0.00024629958555358203,
      "loss": 1.2754,
      "step": 3430
    },
    {
      "epoch": 1.5271920088790232,
      "grad_norm": 2.260831117630005,
      "learning_rate": 0.00024555950266429843,
      "loss": 1.213,
      "step": 3440
    },
    {
      "epoch": 1.5316315205327413,
      "grad_norm": 2.9933383464813232,
      "learning_rate": 0.00024481941977501484,
      "loss": 1.1168,
      "step": 3450
    },
    {
      "epoch": 1.5360710321864595,
      "grad_norm": 2.6057305335998535,
      "learning_rate": 0.0002440793368857312,
      "loss": 1.1243,
      "step": 3460
    },
    {
      "epoch": 1.5405105438401776,
      "grad_norm": 3.2585434913635254,
      "learning_rate": 0.0002433392539964476,
      "loss": 1.2683,
      "step": 3470
    },
    {
      "epoch": 1.5449500554938957,
      "grad_norm": 3.0457262992858887,
      "learning_rate": 0.000242599171107164,
      "loss": 1.1589,
      "step": 3480
    },
    {
      "epoch": 1.5493895671476139,
      "grad_norm": 2.8825504779815674,
      "learning_rate": 0.0002418590882178804,
      "loss": 1.1673,
      "step": 3490
    },
    {
      "epoch": 1.5538290788013318,
      "grad_norm": 2.139880418777466,
      "learning_rate": 0.0002411190053285968,
      "loss": 1.0695,
      "step": 3500
    },
    {
      "epoch": 1.55826859045505,
      "grad_norm": 3.508735179901123,
      "learning_rate": 0.0002403789224393132,
      "loss": 1.1191,
      "step": 3510
    },
    {
      "epoch": 1.5627081021087679,
      "grad_norm": 3.306018352508545,
      "learning_rate": 0.0002396388395500296,
      "loss": 1.2047,
      "step": 3520
    },
    {
      "epoch": 1.567147613762486,
      "grad_norm": 1.835242509841919,
      "learning_rate": 0.000238898756660746,
      "loss": 1.2258,
      "step": 3530
    },
    {
      "epoch": 1.5715871254162042,
      "grad_norm": 3.1522581577301025,
      "learning_rate": 0.0002381586737714624,
      "loss": 1.2185,
      "step": 3540
    },
    {
      "epoch": 1.5760266370699223,
      "grad_norm": 2.3730084896087646,
      "learning_rate": 0.0002374185908821788,
      "loss": 1.2038,
      "step": 3550
    },
    {
      "epoch": 1.5804661487236404,
      "grad_norm": 1.805886149406433,
      "learning_rate": 0.00023667850799289519,
      "loss": 1.1907,
      "step": 3560
    },
    {
      "epoch": 1.5849056603773586,
      "grad_norm": 2.676323413848877,
      "learning_rate": 0.00023593842510361162,
      "loss": 1.0997,
      "step": 3570
    },
    {
      "epoch": 1.5893451720310767,
      "grad_norm": 3.6150553226470947,
      "learning_rate": 0.00023519834221432802,
      "loss": 1.1677,
      "step": 3580
    },
    {
      "epoch": 1.5937846836847946,
      "grad_norm": 2.5337932109832764,
      "learning_rate": 0.00023445825932504443,
      "loss": 1.327,
      "step": 3590
    },
    {
      "epoch": 1.5982241953385128,
      "grad_norm": 3.848266363143921,
      "learning_rate": 0.00023371817643576083,
      "loss": 1.2452,
      "step": 3600
    },
    {
      "epoch": 1.6026637069922307,
      "grad_norm": 2.7990570068359375,
      "learning_rate": 0.0002329780935464772,
      "loss": 1.1405,
      "step": 3610
    },
    {
      "epoch": 1.6071032186459488,
      "grad_norm": 2.877617120742798,
      "learning_rate": 0.00023223801065719361,
      "loss": 1.2704,
      "step": 3620
    },
    {
      "epoch": 1.611542730299667,
      "grad_norm": 2.4765634536743164,
      "learning_rate": 0.00023149792776791002,
      "loss": 1.3649,
      "step": 3630
    },
    {
      "epoch": 1.6159822419533851,
      "grad_norm": 3.8984432220458984,
      "learning_rate": 0.00023075784487862642,
      "loss": 1.1041,
      "step": 3640
    },
    {
      "epoch": 1.6204217536071033,
      "grad_norm": 2.9763686656951904,
      "learning_rate": 0.00023001776198934283,
      "loss": 1.2616,
      "step": 3650
    },
    {
      "epoch": 1.6248612652608214,
      "grad_norm": 1.8666826486587524,
      "learning_rate": 0.0002292776791000592,
      "loss": 1.1995,
      "step": 3660
    },
    {
      "epoch": 1.6293007769145396,
      "grad_norm": 3.087594509124756,
      "learning_rate": 0.0002285375962107756,
      "loss": 1.1611,
      "step": 3670
    },
    {
      "epoch": 1.6337402885682575,
      "grad_norm": 2.4261507987976074,
      "learning_rate": 0.00022779751332149202,
      "loss": 1.0556,
      "step": 3680
    },
    {
      "epoch": 1.6381798002219756,
      "grad_norm": 2.1468873023986816,
      "learning_rate": 0.00022705743043220842,
      "loss": 1.1779,
      "step": 3690
    },
    {
      "epoch": 1.6426193118756935,
      "grad_norm": 2.674466371536255,
      "learning_rate": 0.00022631734754292483,
      "loss": 1.1405,
      "step": 3700
    },
    {
      "epoch": 1.6470588235294117,
      "grad_norm": 2.8991189002990723,
      "learning_rate": 0.0002255772646536412,
      "loss": 1.1332,
      "step": 3710
    },
    {
      "epoch": 1.6514983351831298,
      "grad_norm": 2.403418779373169,
      "learning_rate": 0.0002248371817643576,
      "loss": 1.2624,
      "step": 3720
    },
    {
      "epoch": 1.655937846836848,
      "grad_norm": 2.594874858856201,
      "learning_rate": 0.00022409709887507401,
      "loss": 1.1595,
      "step": 3730
    },
    {
      "epoch": 1.6603773584905661,
      "grad_norm": 3.1170547008514404,
      "learning_rate": 0.00022335701598579042,
      "loss": 1.2147,
      "step": 3740
    },
    {
      "epoch": 1.6648168701442843,
      "grad_norm": 2.4602553844451904,
      "learning_rate": 0.00022261693309650682,
      "loss": 1.0584,
      "step": 3750
    },
    {
      "epoch": 1.6692563817980022,
      "grad_norm": 2.9225375652313232,
      "learning_rate": 0.0002218768502072232,
      "loss": 1.2781,
      "step": 3760
    },
    {
      "epoch": 1.6736958934517203,
      "grad_norm": 2.0052387714385986,
      "learning_rate": 0.0002211367673179396,
      "loss": 1.0892,
      "step": 3770
    },
    {
      "epoch": 1.6781354051054382,
      "grad_norm": 2.296525239944458,
      "learning_rate": 0.000220396684428656,
      "loss": 1.1342,
      "step": 3780
    },
    {
      "epoch": 1.6825749167591564,
      "grad_norm": 2.481069564819336,
      "learning_rate": 0.00021965660153937242,
      "loss": 1.1496,
      "step": 3790
    },
    {
      "epoch": 1.6870144284128745,
      "grad_norm": 2.5501298904418945,
      "learning_rate": 0.00021891651865008882,
      "loss": 1.076,
      "step": 3800
    },
    {
      "epoch": 1.6914539400665927,
      "grad_norm": 3.4054605960845947,
      "learning_rate": 0.0002181764357608052,
      "loss": 1.0062,
      "step": 3810
    },
    {
      "epoch": 1.6958934517203108,
      "grad_norm": 2.15496563911438,
      "learning_rate": 0.0002174363528715216,
      "loss": 1.182,
      "step": 3820
    },
    {
      "epoch": 1.700332963374029,
      "grad_norm": 2.4078776836395264,
      "learning_rate": 0.000216696269982238,
      "loss": 1.116,
      "step": 3830
    },
    {
      "epoch": 1.704772475027747,
      "grad_norm": 2.1361124515533447,
      "learning_rate": 0.00021595618709295441,
      "loss": 1.0938,
      "step": 3840
    },
    {
      "epoch": 1.709211986681465,
      "grad_norm": 3.150621175765991,
      "learning_rate": 0.00021521610420367082,
      "loss": 1.1961,
      "step": 3850
    },
    {
      "epoch": 1.7136514983351832,
      "grad_norm": 3.9479739665985107,
      "learning_rate": 0.0002144760213143872,
      "loss": 1.2683,
      "step": 3860
    },
    {
      "epoch": 1.718091009988901,
      "grad_norm": 3.4793198108673096,
      "learning_rate": 0.0002137359384251036,
      "loss": 1.1636,
      "step": 3870
    },
    {
      "epoch": 1.7225305216426192,
      "grad_norm": 2.4920384883880615,
      "learning_rate": 0.00021299585553582,
      "loss": 1.1928,
      "step": 3880
    },
    {
      "epoch": 1.7269700332963374,
      "grad_norm": 2.0845539569854736,
      "learning_rate": 0.0002122557726465364,
      "loss": 1.2978,
      "step": 3890
    },
    {
      "epoch": 1.7314095449500555,
      "grad_norm": 2.4409728050231934,
      "learning_rate": 0.00021151568975725282,
      "loss": 1.2076,
      "step": 3900
    },
    {
      "epoch": 1.7358490566037736,
      "grad_norm": 2.287454128265381,
      "learning_rate": 0.0002107756068679692,
      "loss": 1.1856,
      "step": 3910
    },
    {
      "epoch": 1.7402885682574918,
      "grad_norm": 2.82369327545166,
      "learning_rate": 0.0002100355239786856,
      "loss": 1.2573,
      "step": 3920
    },
    {
      "epoch": 1.74472807991121,
      "grad_norm": 2.726433277130127,
      "learning_rate": 0.000209295441089402,
      "loss": 1.3373,
      "step": 3930
    },
    {
      "epoch": 1.7491675915649278,
      "grad_norm": 3.3801662921905518,
      "learning_rate": 0.0002085553582001184,
      "loss": 1.2339,
      "step": 3940
    },
    {
      "epoch": 1.753607103218646,
      "grad_norm": 1.9525073766708374,
      "learning_rate": 0.00020781527531083484,
      "loss": 1.203,
      "step": 3950
    },
    {
      "epoch": 1.758046614872364,
      "grad_norm": 2.1085376739501953,
      "learning_rate": 0.00020707519242155122,
      "loss": 1.3084,
      "step": 3960
    },
    {
      "epoch": 1.762486126526082,
      "grad_norm": 1.9295835494995117,
      "learning_rate": 0.00020633510953226762,
      "loss": 1.0961,
      "step": 3970
    },
    {
      "epoch": 1.7669256381798002,
      "grad_norm": 2.250570058822632,
      "learning_rate": 0.00020559502664298403,
      "loss": 1.1518,
      "step": 3980
    },
    {
      "epoch": 1.7713651498335183,
      "grad_norm": 3.264716148376465,
      "learning_rate": 0.00020485494375370043,
      "loss": 1.2206,
      "step": 3990
    },
    {
      "epoch": 1.7758046614872365,
      "grad_norm": 2.40153431892395,
      "learning_rate": 0.00020411486086441684,
      "loss": 1.2106,
      "step": 4000
    },
    {
      "epoch": 1.7802441731409546,
      "grad_norm": 3.3480634689331055,
      "learning_rate": 0.00020337477797513322,
      "loss": 1.1763,
      "step": 4010
    },
    {
      "epoch": 1.7846836847946725,
      "grad_norm": 2.0051910877227783,
      "learning_rate": 0.00020263469508584962,
      "loss": 1.233,
      "step": 4020
    },
    {
      "epoch": 1.7891231964483907,
      "grad_norm": 1.9596502780914307,
      "learning_rate": 0.00020189461219656603,
      "loss": 1.1247,
      "step": 4030
    },
    {
      "epoch": 1.7935627081021086,
      "grad_norm": 2.500321865081787,
      "learning_rate": 0.00020115452930728243,
      "loss": 1.315,
      "step": 4040
    },
    {
      "epoch": 1.7980022197558267,
      "grad_norm": 2.8283376693725586,
      "learning_rate": 0.00020041444641799884,
      "loss": 1.1395,
      "step": 4050
    },
    {
      "epoch": 1.802441731409545,
      "grad_norm": 2.67047381401062,
      "learning_rate": 0.00019967436352871521,
      "loss": 1.2022,
      "step": 4060
    },
    {
      "epoch": 1.806881243063263,
      "grad_norm": 2.2395167350769043,
      "learning_rate": 0.00019893428063943162,
      "loss": 1.2477,
      "step": 4070
    },
    {
      "epoch": 1.8113207547169812,
      "grad_norm": 3.7520008087158203,
      "learning_rate": 0.00019819419775014802,
      "loss": 1.1859,
      "step": 4080
    },
    {
      "epoch": 1.8157602663706993,
      "grad_norm": 2.4978578090667725,
      "learning_rate": 0.00019745411486086443,
      "loss": 1.0266,
      "step": 4090
    },
    {
      "epoch": 1.8201997780244175,
      "grad_norm": 2.326854944229126,
      "learning_rate": 0.00019671403197158083,
      "loss": 1.3114,
      "step": 4100
    },
    {
      "epoch": 1.8246392896781354,
      "grad_norm": 3.3812456130981445,
      "learning_rate": 0.0001959739490822972,
      "loss": 1.2458,
      "step": 4110
    },
    {
      "epoch": 1.8290788013318535,
      "grad_norm": 3.0731539726257324,
      "learning_rate": 0.00019523386619301362,
      "loss": 1.0785,
      "step": 4120
    },
    {
      "epoch": 1.8335183129855714,
      "grad_norm": 2.4971518516540527,
      "learning_rate": 0.00019449378330373002,
      "loss": 1.2514,
      "step": 4130
    },
    {
      "epoch": 1.8379578246392896,
      "grad_norm": 2.1898913383483887,
      "learning_rate": 0.00019375370041444643,
      "loss": 1.3365,
      "step": 4140
    },
    {
      "epoch": 1.8423973362930077,
      "grad_norm": 2.2339117527008057,
      "learning_rate": 0.00019301361752516283,
      "loss": 1.2654,
      "step": 4150
    },
    {
      "epoch": 1.8468368479467259,
      "grad_norm": 1.9162166118621826,
      "learning_rate": 0.0001922735346358792,
      "loss": 1.2141,
      "step": 4160
    },
    {
      "epoch": 1.851276359600444,
      "grad_norm": 3.4643778800964355,
      "learning_rate": 0.0001915334517465956,
      "loss": 1.1086,
      "step": 4170
    },
    {
      "epoch": 1.8557158712541622,
      "grad_norm": 2.533435583114624,
      "learning_rate": 0.00019079336885731202,
      "loss": 1.2319,
      "step": 4180
    },
    {
      "epoch": 1.8601553829078803,
      "grad_norm": 2.9026618003845215,
      "learning_rate": 0.00019005328596802842,
      "loss": 1.2592,
      "step": 4190
    },
    {
      "epoch": 1.8645948945615982,
      "grad_norm": 2.042279005050659,
      "learning_rate": 0.00018931320307874483,
      "loss": 1.2121,
      "step": 4200
    },
    {
      "epoch": 1.8690344062153164,
      "grad_norm": 2.8764638900756836,
      "learning_rate": 0.0001885731201894612,
      "loss": 1.1081,
      "step": 4210
    },
    {
      "epoch": 1.8734739178690343,
      "grad_norm": 2.06687068939209,
      "learning_rate": 0.0001878330373001776,
      "loss": 1.2417,
      "step": 4220
    },
    {
      "epoch": 1.8779134295227524,
      "grad_norm": 2.00785231590271,
      "learning_rate": 0.00018709295441089402,
      "loss": 1.2131,
      "step": 4230
    },
    {
      "epoch": 1.8823529411764706,
      "grad_norm": 3.1687676906585693,
      "learning_rate": 0.00018635287152161042,
      "loss": 1.1776,
      "step": 4240
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 2.8466646671295166,
      "learning_rate": 0.00018561278863232683,
      "loss": 1.1209,
      "step": 4250
    },
    {
      "epoch": 1.8912319644839068,
      "grad_norm": 1.9301031827926636,
      "learning_rate": 0.0001848727057430432,
      "loss": 1.2573,
      "step": 4260
    },
    {
      "epoch": 1.895671476137625,
      "grad_norm": 2.328272581100464,
      "learning_rate": 0.0001841326228537596,
      "loss": 1.0608,
      "step": 4270
    },
    {
      "epoch": 1.900110987791343,
      "grad_norm": 2.2757699489593506,
      "learning_rate": 0.000183392539964476,
      "loss": 1.2308,
      "step": 4280
    },
    {
      "epoch": 1.904550499445061,
      "grad_norm": 2.6472411155700684,
      "learning_rate": 0.00018265245707519242,
      "loss": 1.0846,
      "step": 4290
    },
    {
      "epoch": 1.908990011098779,
      "grad_norm": 2.0756540298461914,
      "learning_rate": 0.00018191237418590882,
      "loss": 1.2288,
      "step": 4300
    }
  ],
  "logging_steps": 10,
  "max_steps": 6756,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6811258470137856.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
