{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.2874583795782464,
  "eval_steps": 500,
  "global_step": 2900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004439511653718091,
      "grad_norm": 4.054643630981445,
      "learning_rate": 0.0004992599171107164,
      "loss": 2.352,
      "step": 10
    },
    {
      "epoch": 0.008879023307436182,
      "grad_norm": 2.477839469909668,
      "learning_rate": 0.0004985198342214328,
      "loss": 1.9176,
      "step": 20
    },
    {
      "epoch": 0.013318534961154272,
      "grad_norm": 2.748054027557373,
      "learning_rate": 0.0004977797513321492,
      "loss": 2.0735,
      "step": 30
    },
    {
      "epoch": 0.017758046614872364,
      "grad_norm": 2.6057891845703125,
      "learning_rate": 0.0004970396684428656,
      "loss": 1.7736,
      "step": 40
    },
    {
      "epoch": 0.022197558268590455,
      "grad_norm": 1.843123197555542,
      "learning_rate": 0.000496299585553582,
      "loss": 1.822,
      "step": 50
    },
    {
      "epoch": 0.026637069922308545,
      "grad_norm": 1.7726880311965942,
      "learning_rate": 0.0004956335109532268,
      "loss": 1.7889,
      "step": 60
    },
    {
      "epoch": 0.03107658157602664,
      "grad_norm": 2.047724485397339,
      "learning_rate": 0.0004948934280639432,
      "loss": 1.6535,
      "step": 70
    },
    {
      "epoch": 0.03551609322974473,
      "grad_norm": 1.8672618865966797,
      "learning_rate": 0.0004941533451746596,
      "loss": 1.6607,
      "step": 80
    },
    {
      "epoch": 0.03995560488346282,
      "grad_norm": 2.528557538986206,
      "learning_rate": 0.000493413262285376,
      "loss": 1.7816,
      "step": 90
    },
    {
      "epoch": 0.04439511653718091,
      "grad_norm": 2.168644905090332,
      "learning_rate": 0.0004926731793960924,
      "loss": 1.5275,
      "step": 100
    },
    {
      "epoch": 0.048834628190899,
      "grad_norm": 2.151695728302002,
      "learning_rate": 0.0004919330965068088,
      "loss": 1.7346,
      "step": 110
    },
    {
      "epoch": 0.05327413984461709,
      "grad_norm": 2.0462753772735596,
      "learning_rate": 0.0004911930136175252,
      "loss": 1.5574,
      "step": 120
    },
    {
      "epoch": 0.05771365149833518,
      "grad_norm": 1.5268481969833374,
      "learning_rate": 0.0004904529307282416,
      "loss": 1.6383,
      "step": 130
    },
    {
      "epoch": 0.06215316315205328,
      "grad_norm": 2.150617837905884,
      "learning_rate": 0.000489712847838958,
      "loss": 1.6846,
      "step": 140
    },
    {
      "epoch": 0.06659267480577137,
      "grad_norm": 2.586848020553589,
      "learning_rate": 0.0004889727649496743,
      "loss": 1.5786,
      "step": 150
    },
    {
      "epoch": 0.07103218645948946,
      "grad_norm": 1.9333423376083374,
      "learning_rate": 0.00048823268206039076,
      "loss": 1.7571,
      "step": 160
    },
    {
      "epoch": 0.07547169811320754,
      "grad_norm": 3.1580400466918945,
      "learning_rate": 0.00048749259917110717,
      "loss": 1.6413,
      "step": 170
    },
    {
      "epoch": 0.07991120976692564,
      "grad_norm": 2.201777458190918,
      "learning_rate": 0.00048675251628182357,
      "loss": 1.6637,
      "step": 180
    },
    {
      "epoch": 0.08435072142064373,
      "grad_norm": 1.5520122051239014,
      "learning_rate": 0.00048601243339254,
      "loss": 1.5949,
      "step": 190
    },
    {
      "epoch": 0.08879023307436182,
      "grad_norm": 3.2420663833618164,
      "learning_rate": 0.0004852723505032564,
      "loss": 1.5658,
      "step": 200
    },
    {
      "epoch": 0.0932297447280799,
      "grad_norm": 2.1900508403778076,
      "learning_rate": 0.0004845322676139728,
      "loss": 1.5863,
      "step": 210
    },
    {
      "epoch": 0.097669256381798,
      "grad_norm": 2.21549391746521,
      "learning_rate": 0.0004837921847246892,
      "loss": 1.4758,
      "step": 220
    },
    {
      "epoch": 0.10210876803551609,
      "grad_norm": 2.064615488052368,
      "learning_rate": 0.0004830521018354056,
      "loss": 1.55,
      "step": 230
    },
    {
      "epoch": 0.10654827968923418,
      "grad_norm": 2.525146961212158,
      "learning_rate": 0.000482312018946122,
      "loss": 1.5399,
      "step": 240
    },
    {
      "epoch": 0.11098779134295228,
      "grad_norm": 2.0821399688720703,
      "learning_rate": 0.00048157193605683835,
      "loss": 1.4429,
      "step": 250
    },
    {
      "epoch": 0.11542730299667037,
      "grad_norm": 1.58427894115448,
      "learning_rate": 0.0004808318531675548,
      "loss": 1.4108,
      "step": 260
    },
    {
      "epoch": 0.11986681465038845,
      "grad_norm": 2.192683696746826,
      "learning_rate": 0.00048009177027827116,
      "loss": 1.4006,
      "step": 270
    },
    {
      "epoch": 0.12430632630410655,
      "grad_norm": 1.5377377271652222,
      "learning_rate": 0.0004793516873889876,
      "loss": 1.3984,
      "step": 280
    },
    {
      "epoch": 0.12874583795782463,
      "grad_norm": 2.260063409805298,
      "learning_rate": 0.00047861160449970397,
      "loss": 1.4997,
      "step": 290
    },
    {
      "epoch": 0.13318534961154274,
      "grad_norm": 3.4104886054992676,
      "learning_rate": 0.0004778715216104204,
      "loss": 1.4686,
      "step": 300
    },
    {
      "epoch": 0.13762486126526083,
      "grad_norm": 2.8780131340026855,
      "learning_rate": 0.0004771314387211368,
      "loss": 1.6059,
      "step": 310
    },
    {
      "epoch": 0.14206437291897892,
      "grad_norm": 2.854306221008301,
      "learning_rate": 0.0004763913558318532,
      "loss": 1.6074,
      "step": 320
    },
    {
      "epoch": 0.146503884572697,
      "grad_norm": 1.7294703722000122,
      "learning_rate": 0.0004756512729425696,
      "loss": 1.5092,
      "step": 330
    },
    {
      "epoch": 0.1509433962264151,
      "grad_norm": 2.494455099105835,
      "learning_rate": 0.000474911190053286,
      "loss": 1.4524,
      "step": 340
    },
    {
      "epoch": 0.15538290788013318,
      "grad_norm": 2.055124521255493,
      "learning_rate": 0.00047417110716400235,
      "loss": 1.4831,
      "step": 350
    },
    {
      "epoch": 0.1598224195338513,
      "grad_norm": 2.6806280612945557,
      "learning_rate": 0.0004734310242747188,
      "loss": 1.5829,
      "step": 360
    },
    {
      "epoch": 0.16426193118756938,
      "grad_norm": 2.334710121154785,
      "learning_rate": 0.00047269094138543516,
      "loss": 1.5037,
      "step": 370
    },
    {
      "epoch": 0.16870144284128746,
      "grad_norm": 3.8841795921325684,
      "learning_rate": 0.0004719508584961516,
      "loss": 1.4424,
      "step": 380
    },
    {
      "epoch": 0.17314095449500555,
      "grad_norm": 3.0745744705200195,
      "learning_rate": 0.00047121077560686797,
      "loss": 1.4201,
      "step": 390
    },
    {
      "epoch": 0.17758046614872364,
      "grad_norm": 2.2438924312591553,
      "learning_rate": 0.00047047069271758437,
      "loss": 1.4589,
      "step": 400
    },
    {
      "epoch": 0.18201997780244172,
      "grad_norm": 2.606391429901123,
      "learning_rate": 0.0004697306098283008,
      "loss": 1.5147,
      "step": 410
    },
    {
      "epoch": 0.1864594894561598,
      "grad_norm": 1.9846000671386719,
      "learning_rate": 0.0004689905269390172,
      "loss": 1.6754,
      "step": 420
    },
    {
      "epoch": 0.19089900110987792,
      "grad_norm": 2.0650007724761963,
      "learning_rate": 0.0004682504440497336,
      "loss": 1.4663,
      "step": 430
    },
    {
      "epoch": 0.195338512763596,
      "grad_norm": 1.969778060913086,
      "learning_rate": 0.00046751036116045,
      "loss": 1.4181,
      "step": 440
    },
    {
      "epoch": 0.1997780244173141,
      "grad_norm": 2.866069793701172,
      "learning_rate": 0.00046677027827116634,
      "loss": 1.3834,
      "step": 450
    },
    {
      "epoch": 0.20421753607103219,
      "grad_norm": 2.133826494216919,
      "learning_rate": 0.0004660301953818828,
      "loss": 1.4518,
      "step": 460
    },
    {
      "epoch": 0.20865704772475027,
      "grad_norm": 2.4633429050445557,
      "learning_rate": 0.00046529011249259915,
      "loss": 1.4814,
      "step": 470
    },
    {
      "epoch": 0.21309655937846836,
      "grad_norm": 1.6882880926132202,
      "learning_rate": 0.0004645500296033156,
      "loss": 1.4354,
      "step": 480
    },
    {
      "epoch": 0.21753607103218647,
      "grad_norm": 2.161064624786377,
      "learning_rate": 0.00046380994671403196,
      "loss": 1.4254,
      "step": 490
    },
    {
      "epoch": 0.22197558268590456,
      "grad_norm": 3.765519142150879,
      "learning_rate": 0.00046306986382474837,
      "loss": 1.4999,
      "step": 500
    },
    {
      "epoch": 0.22641509433962265,
      "grad_norm": 2.3685739040374756,
      "learning_rate": 0.00046232978093546477,
      "loss": 1.4368,
      "step": 510
    },
    {
      "epoch": 0.23085460599334073,
      "grad_norm": 1.8080672025680542,
      "learning_rate": 0.0004615896980461812,
      "loss": 1.4922,
      "step": 520
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 3.117081880569458,
      "learning_rate": 0.0004608496151568976,
      "loss": 1.4003,
      "step": 530
    },
    {
      "epoch": 0.2397336293007769,
      "grad_norm": 1.7682117223739624,
      "learning_rate": 0.000460109532267614,
      "loss": 1.489,
      "step": 540
    },
    {
      "epoch": 0.244173140954495,
      "grad_norm": 2.2536675930023193,
      "learning_rate": 0.00045936944937833034,
      "loss": 1.5295,
      "step": 550
    },
    {
      "epoch": 0.2486126526082131,
      "grad_norm": 2.2390410900115967,
      "learning_rate": 0.0004586293664890468,
      "loss": 1.3979,
      "step": 560
    },
    {
      "epoch": 0.25305216426193117,
      "grad_norm": 2.209352731704712,
      "learning_rate": 0.0004578892835997632,
      "loss": 1.4194,
      "step": 570
    },
    {
      "epoch": 0.25749167591564925,
      "grad_norm": 2.129152536392212,
      "learning_rate": 0.0004571492007104796,
      "loss": 1.394,
      "step": 580
    },
    {
      "epoch": 0.2619311875693674,
      "grad_norm": 2.003588914871216,
      "learning_rate": 0.000456409117821196,
      "loss": 1.2182,
      "step": 590
    },
    {
      "epoch": 0.2663706992230855,
      "grad_norm": 1.967622995376587,
      "learning_rate": 0.00045566903493191236,
      "loss": 1.5414,
      "step": 600
    },
    {
      "epoch": 0.27081021087680357,
      "grad_norm": 2.4503138065338135,
      "learning_rate": 0.0004549289520426288,
      "loss": 1.3881,
      "step": 610
    },
    {
      "epoch": 0.27524972253052166,
      "grad_norm": 2.1452744007110596,
      "learning_rate": 0.00045418886915334517,
      "loss": 1.3729,
      "step": 620
    },
    {
      "epoch": 0.27968923418423974,
      "grad_norm": 2.611724615097046,
      "learning_rate": 0.00045344878626406163,
      "loss": 1.4539,
      "step": 630
    },
    {
      "epoch": 0.28412874583795783,
      "grad_norm": 2.1509506702423096,
      "learning_rate": 0.000452708703374778,
      "loss": 1.4869,
      "step": 640
    },
    {
      "epoch": 0.2885682574916759,
      "grad_norm": 1.7710063457489014,
      "learning_rate": 0.0004519686204854944,
      "loss": 1.4636,
      "step": 650
    },
    {
      "epoch": 0.293007769145394,
      "grad_norm": 2.103294610977173,
      "learning_rate": 0.0004512285375962108,
      "loss": 1.3811,
      "step": 660
    },
    {
      "epoch": 0.2974472807991121,
      "grad_norm": 2.388345956802368,
      "learning_rate": 0.0004504884547069272,
      "loss": 1.3846,
      "step": 670
    },
    {
      "epoch": 0.3018867924528302,
      "grad_norm": 2.7555673122406006,
      "learning_rate": 0.0004497483718176436,
      "loss": 1.4782,
      "step": 680
    },
    {
      "epoch": 0.30632630410654826,
      "grad_norm": 2.0017216205596924,
      "learning_rate": 0.00044900828892836,
      "loss": 1.3683,
      "step": 690
    },
    {
      "epoch": 0.31076581576026635,
      "grad_norm": 2.90942120552063,
      "learning_rate": 0.00044826820603907636,
      "loss": 1.3179,
      "step": 700
    },
    {
      "epoch": 0.31520532741398444,
      "grad_norm": 2.5046520233154297,
      "learning_rate": 0.0004475281231497928,
      "loss": 1.4616,
      "step": 710
    },
    {
      "epoch": 0.3196448390677026,
      "grad_norm": 1.8346855640411377,
      "learning_rate": 0.00044678804026050917,
      "loss": 1.2704,
      "step": 720
    },
    {
      "epoch": 0.32408435072142067,
      "grad_norm": 1.8805220127105713,
      "learning_rate": 0.0004460479573712256,
      "loss": 1.3124,
      "step": 730
    },
    {
      "epoch": 0.32852386237513875,
      "grad_norm": 2.064824342727661,
      "learning_rate": 0.000445307874481942,
      "loss": 1.4841,
      "step": 740
    },
    {
      "epoch": 0.33296337402885684,
      "grad_norm": 2.6950631141662598,
      "learning_rate": 0.0004445677915926584,
      "loss": 1.4189,
      "step": 750
    },
    {
      "epoch": 0.3374028856825749,
      "grad_norm": 2.8344624042510986,
      "learning_rate": 0.0004438277087033748,
      "loss": 1.6122,
      "step": 760
    },
    {
      "epoch": 0.341842397336293,
      "grad_norm": 2.25856351852417,
      "learning_rate": 0.0004430876258140912,
      "loss": 1.3434,
      "step": 770
    },
    {
      "epoch": 0.3462819089900111,
      "grad_norm": 3.7481534481048584,
      "learning_rate": 0.0004423475429248076,
      "loss": 1.4331,
      "step": 780
    },
    {
      "epoch": 0.3507214206437292,
      "grad_norm": 2.2983579635620117,
      "learning_rate": 0.000441607460035524,
      "loss": 1.389,
      "step": 790
    },
    {
      "epoch": 0.3551609322974473,
      "grad_norm": 1.9178669452667236,
      "learning_rate": 0.00044086737714624035,
      "loss": 1.3596,
      "step": 800
    },
    {
      "epoch": 0.35960044395116536,
      "grad_norm": 2.607564926147461,
      "learning_rate": 0.0004401272942569568,
      "loss": 1.3677,
      "step": 810
    },
    {
      "epoch": 0.36403995560488345,
      "grad_norm": 1.7786167860031128,
      "learning_rate": 0.00043938721136767316,
      "loss": 1.3581,
      "step": 820
    },
    {
      "epoch": 0.36847946725860153,
      "grad_norm": 1.9138953685760498,
      "learning_rate": 0.0004386471284783896,
      "loss": 1.3118,
      "step": 830
    },
    {
      "epoch": 0.3729189789123196,
      "grad_norm": 2.3295493125915527,
      "learning_rate": 0.00043790704558910597,
      "loss": 1.425,
      "step": 840
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 2.2294840812683105,
      "learning_rate": 0.0004371669626998224,
      "loss": 1.4387,
      "step": 850
    },
    {
      "epoch": 0.38179800221975585,
      "grad_norm": 2.571744918823242,
      "learning_rate": 0.0004364268798105388,
      "loss": 1.1943,
      "step": 860
    },
    {
      "epoch": 0.38623751387347394,
      "grad_norm": 3.9955742359161377,
      "learning_rate": 0.0004356867969212552,
      "loss": 1.5068,
      "step": 870
    },
    {
      "epoch": 0.390677025527192,
      "grad_norm": 1.9013206958770752,
      "learning_rate": 0.0004349467140319716,
      "loss": 1.3918,
      "step": 880
    },
    {
      "epoch": 0.3951165371809101,
      "grad_norm": 2.1233904361724854,
      "learning_rate": 0.000434206631142688,
      "loss": 1.3743,
      "step": 890
    },
    {
      "epoch": 0.3995560488346282,
      "grad_norm": 2.040383815765381,
      "learning_rate": 0.00043346654825340435,
      "loss": 1.3621,
      "step": 900
    },
    {
      "epoch": 0.4039955604883463,
      "grad_norm": 3.0131123065948486,
      "learning_rate": 0.0004327264653641208,
      "loss": 1.4435,
      "step": 910
    },
    {
      "epoch": 0.40843507214206437,
      "grad_norm": 3.830791711807251,
      "learning_rate": 0.00043198638247483716,
      "loss": 1.3214,
      "step": 920
    },
    {
      "epoch": 0.41287458379578246,
      "grad_norm": 2.083017349243164,
      "learning_rate": 0.0004312462995855536,
      "loss": 1.4802,
      "step": 930
    },
    {
      "epoch": 0.41731409544950054,
      "grad_norm": 2.2863805294036865,
      "learning_rate": 0.00043050621669627,
      "loss": 1.35,
      "step": 940
    },
    {
      "epoch": 0.42175360710321863,
      "grad_norm": 3.8747406005859375,
      "learning_rate": 0.00042976613380698637,
      "loss": 1.2948,
      "step": 950
    },
    {
      "epoch": 0.4261931187569367,
      "grad_norm": 2.510215997695923,
      "learning_rate": 0.00042902605091770283,
      "loss": 1.3056,
      "step": 960
    },
    {
      "epoch": 0.4306326304106548,
      "grad_norm": 2.4046311378479004,
      "learning_rate": 0.0004282859680284192,
      "loss": 1.3578,
      "step": 970
    },
    {
      "epoch": 0.43507214206437295,
      "grad_norm": 1.803960919380188,
      "learning_rate": 0.00042754588513913564,
      "loss": 1.4015,
      "step": 980
    },
    {
      "epoch": 0.43951165371809103,
      "grad_norm": 1.5232865810394287,
      "learning_rate": 0.000426805802249852,
      "loss": 1.4134,
      "step": 990
    },
    {
      "epoch": 0.4439511653718091,
      "grad_norm": 1.850748062133789,
      "learning_rate": 0.0004260657193605684,
      "loss": 1.2966,
      "step": 1000
    },
    {
      "epoch": 0.4483906770255272,
      "grad_norm": 3.298250198364258,
      "learning_rate": 0.0004253256364712848,
      "loss": 1.301,
      "step": 1010
    },
    {
      "epoch": 0.4528301886792453,
      "grad_norm": 2.6895077228546143,
      "learning_rate": 0.0004246595618709296,
      "loss": 1.2867,
      "step": 1020
    },
    {
      "epoch": 0.4572697003329634,
      "grad_norm": 2.4650585651397705,
      "learning_rate": 0.00042391947898164594,
      "loss": 1.3447,
      "step": 1030
    },
    {
      "epoch": 0.46170921198668147,
      "grad_norm": 2.47660756111145,
      "learning_rate": 0.00042317939609236234,
      "loss": 1.3688,
      "step": 1040
    },
    {
      "epoch": 0.46614872364039955,
      "grad_norm": 2.0048694610595703,
      "learning_rate": 0.00042243931320307875,
      "loss": 1.3055,
      "step": 1050
    },
    {
      "epoch": 0.47058823529411764,
      "grad_norm": 2.7688956260681152,
      "learning_rate": 0.00042169923031379515,
      "loss": 1.3676,
      "step": 1060
    },
    {
      "epoch": 0.4750277469478357,
      "grad_norm": 2.5110268592834473,
      "learning_rate": 0.00042095914742451156,
      "loss": 1.3522,
      "step": 1070
    },
    {
      "epoch": 0.4794672586015538,
      "grad_norm": 2.1339023113250732,
      "learning_rate": 0.00042021906453522796,
      "loss": 1.239,
      "step": 1080
    },
    {
      "epoch": 0.4839067702552719,
      "grad_norm": 2.849062204360962,
      "learning_rate": 0.0004194789816459443,
      "loss": 1.3378,
      "step": 1090
    },
    {
      "epoch": 0.48834628190899,
      "grad_norm": 2.0617716312408447,
      "learning_rate": 0.00041873889875666077,
      "loss": 1.2474,
      "step": 1100
    },
    {
      "epoch": 0.49278579356270813,
      "grad_norm": 2.1087727546691895,
      "learning_rate": 0.0004179988158673771,
      "loss": 1.2378,
      "step": 1110
    },
    {
      "epoch": 0.4972253052164262,
      "grad_norm": 2.3770649433135986,
      "learning_rate": 0.0004172587329780936,
      "loss": 1.2649,
      "step": 1120
    },
    {
      "epoch": 0.5016648168701443,
      "grad_norm": 2.892982244491577,
      "learning_rate": 0.00041651865008881,
      "loss": 1.3096,
      "step": 1130
    },
    {
      "epoch": 0.5061043285238623,
      "grad_norm": 3.1324703693389893,
      "learning_rate": 0.00041577856719952634,
      "loss": 1.3468,
      "step": 1140
    },
    {
      "epoch": 0.5105438401775805,
      "grad_norm": 2.302788019180298,
      "learning_rate": 0.0004150384843102428,
      "loss": 1.5017,
      "step": 1150
    },
    {
      "epoch": 0.5149833518312985,
      "grad_norm": 2.7934770584106445,
      "learning_rate": 0.00041429840142095915,
      "loss": 1.1984,
      "step": 1160
    },
    {
      "epoch": 0.5194228634850167,
      "grad_norm": 2.149888038635254,
      "learning_rate": 0.0004135583185316756,
      "loss": 1.4468,
      "step": 1170
    },
    {
      "epoch": 0.5238623751387348,
      "grad_norm": 2.188237428665161,
      "learning_rate": 0.00041281823564239196,
      "loss": 1.2426,
      "step": 1180
    },
    {
      "epoch": 0.5283018867924528,
      "grad_norm": 2.5798327922821045,
      "learning_rate": 0.00041207815275310836,
      "loss": 1.3429,
      "step": 1190
    },
    {
      "epoch": 0.532741398446171,
      "grad_norm": 2.389024019241333,
      "learning_rate": 0.00041133806986382477,
      "loss": 1.349,
      "step": 1200
    },
    {
      "epoch": 0.537180910099889,
      "grad_norm": 1.64403235912323,
      "learning_rate": 0.00041059798697454117,
      "loss": 1.3302,
      "step": 1210
    },
    {
      "epoch": 0.5416204217536071,
      "grad_norm": 1.824286699295044,
      "learning_rate": 0.0004098579040852576,
      "loss": 1.3638,
      "step": 1220
    },
    {
      "epoch": 0.5460599334073252,
      "grad_norm": 1.6353033781051636,
      "learning_rate": 0.000409117821195974,
      "loss": 1.47,
      "step": 1230
    },
    {
      "epoch": 0.5504994450610433,
      "grad_norm": 1.9310777187347412,
      "learning_rate": 0.00040837773830669033,
      "loss": 1.3305,
      "step": 1240
    },
    {
      "epoch": 0.5549389567147613,
      "grad_norm": 2.2481529712677,
      "learning_rate": 0.0004076376554174068,
      "loss": 1.4085,
      "step": 1250
    },
    {
      "epoch": 0.5593784683684795,
      "grad_norm": 2.4630343914031982,
      "learning_rate": 0.00040689757252812314,
      "loss": 1.2791,
      "step": 1260
    },
    {
      "epoch": 0.5638179800221975,
      "grad_norm": 1.8748359680175781,
      "learning_rate": 0.0004061574896388396,
      "loss": 1.1222,
      "step": 1270
    },
    {
      "epoch": 0.5682574916759157,
      "grad_norm": 2.4667835235595703,
      "learning_rate": 0.00040541740674955595,
      "loss": 1.2666,
      "step": 1280
    },
    {
      "epoch": 0.5726970033296337,
      "grad_norm": 2.185810089111328,
      "learning_rate": 0.00040467732386027236,
      "loss": 1.2362,
      "step": 1290
    },
    {
      "epoch": 0.5771365149833518,
      "grad_norm": 2.8877182006835938,
      "learning_rate": 0.00040393724097098876,
      "loss": 1.4745,
      "step": 1300
    },
    {
      "epoch": 0.58157602663707,
      "grad_norm": 2.3007447719573975,
      "learning_rate": 0.00040319715808170517,
      "loss": 1.4565,
      "step": 1310
    },
    {
      "epoch": 0.586015538290788,
      "grad_norm": 2.2806577682495117,
      "learning_rate": 0.00040245707519242157,
      "loss": 1.2963,
      "step": 1320
    },
    {
      "epoch": 0.5904550499445061,
      "grad_norm": 2.3302412033081055,
      "learning_rate": 0.000401716992303138,
      "loss": 1.2644,
      "step": 1330
    },
    {
      "epoch": 0.5948945615982242,
      "grad_norm": 2.422548532485962,
      "learning_rate": 0.0004009769094138543,
      "loss": 1.3935,
      "step": 1340
    },
    {
      "epoch": 0.5993340732519423,
      "grad_norm": 2.364901542663574,
      "learning_rate": 0.0004002368265245708,
      "loss": 1.2859,
      "step": 1350
    },
    {
      "epoch": 0.6037735849056604,
      "grad_norm": 2.2721052169799805,
      "learning_rate": 0.00039949674363528714,
      "loss": 1.2598,
      "step": 1360
    },
    {
      "epoch": 0.6082130965593785,
      "grad_norm": 2.7273976802825928,
      "learning_rate": 0.0003987566607460036,
      "loss": 1.4341,
      "step": 1370
    },
    {
      "epoch": 0.6126526082130965,
      "grad_norm": 2.987922191619873,
      "learning_rate": 0.00039801657785671995,
      "loss": 1.357,
      "step": 1380
    },
    {
      "epoch": 0.6170921198668147,
      "grad_norm": 2.6135194301605225,
      "learning_rate": 0.00039727649496743635,
      "loss": 1.3516,
      "step": 1390
    },
    {
      "epoch": 0.6215316315205327,
      "grad_norm": 2.7479171752929688,
      "learning_rate": 0.00039653641207815276,
      "loss": 1.3379,
      "step": 1400
    },
    {
      "epoch": 0.6259711431742508,
      "grad_norm": 2.23116135597229,
      "learning_rate": 0.00039579632918886916,
      "loss": 1.2953,
      "step": 1410
    },
    {
      "epoch": 0.6304106548279689,
      "grad_norm": 2.293957471847534,
      "learning_rate": 0.00039505624629958556,
      "loss": 1.3825,
      "step": 1420
    },
    {
      "epoch": 0.634850166481687,
      "grad_norm": 2.1172266006469727,
      "learning_rate": 0.00039431616341030197,
      "loss": 1.3515,
      "step": 1430
    },
    {
      "epoch": 0.6392896781354052,
      "grad_norm": 2.479458808898926,
      "learning_rate": 0.0003935760805210183,
      "loss": 1.1927,
      "step": 1440
    },
    {
      "epoch": 0.6437291897891232,
      "grad_norm": 2.385388135910034,
      "learning_rate": 0.0003928359976317348,
      "loss": 1.3524,
      "step": 1450
    },
    {
      "epoch": 0.6481687014428413,
      "grad_norm": 2.0675551891326904,
      "learning_rate": 0.00039209591474245113,
      "loss": 1.3568,
      "step": 1460
    },
    {
      "epoch": 0.6526082130965594,
      "grad_norm": 2.1671926975250244,
      "learning_rate": 0.0003913558318531676,
      "loss": 1.3033,
      "step": 1470
    },
    {
      "epoch": 0.6570477247502775,
      "grad_norm": 2.717130661010742,
      "learning_rate": 0.00039061574896388394,
      "loss": 1.3575,
      "step": 1480
    },
    {
      "epoch": 0.6614872364039955,
      "grad_norm": 2.779754638671875,
      "learning_rate": 0.00038987566607460035,
      "loss": 1.3663,
      "step": 1490
    },
    {
      "epoch": 0.6659267480577137,
      "grad_norm": 1.8778971433639526,
      "learning_rate": 0.0003891355831853168,
      "loss": 1.2533,
      "step": 1500
    },
    {
      "epoch": 0.6703662597114317,
      "grad_norm": 3.0039737224578857,
      "learning_rate": 0.00038839550029603315,
      "loss": 1.4432,
      "step": 1510
    },
    {
      "epoch": 0.6748057713651499,
      "grad_norm": 2.170186758041382,
      "learning_rate": 0.0003876554174067496,
      "loss": 1.378,
      "step": 1520
    },
    {
      "epoch": 0.6792452830188679,
      "grad_norm": 3.1594653129577637,
      "learning_rate": 0.00038691533451746596,
      "loss": 1.4479,
      "step": 1530
    },
    {
      "epoch": 0.683684794672586,
      "grad_norm": 3.039705514907837,
      "learning_rate": 0.00038617525162818237,
      "loss": 1.2748,
      "step": 1540
    },
    {
      "epoch": 0.6881243063263041,
      "grad_norm": 3.3899080753326416,
      "learning_rate": 0.0003854351687388988,
      "loss": 1.3297,
      "step": 1550
    },
    {
      "epoch": 0.6925638179800222,
      "grad_norm": 2.6969168186187744,
      "learning_rate": 0.0003846950858496152,
      "loss": 1.3423,
      "step": 1560
    },
    {
      "epoch": 0.6970033296337403,
      "grad_norm": 2.2130165100097656,
      "learning_rate": 0.0003839550029603316,
      "loss": 1.3877,
      "step": 1570
    },
    {
      "epoch": 0.7014428412874584,
      "grad_norm": 2.1677567958831787,
      "learning_rate": 0.000383214920071048,
      "loss": 1.265,
      "step": 1580
    },
    {
      "epoch": 0.7058823529411765,
      "grad_norm": 2.1968941688537598,
      "learning_rate": 0.00038247483718176434,
      "loss": 1.2592,
      "step": 1590
    },
    {
      "epoch": 0.7103218645948945,
      "grad_norm": 2.5119307041168213,
      "learning_rate": 0.0003817347542924808,
      "loss": 1.2195,
      "step": 1600
    },
    {
      "epoch": 0.7147613762486127,
      "grad_norm": 1.9480350017547607,
      "learning_rate": 0.00038099467140319715,
      "loss": 1.303,
      "step": 1610
    },
    {
      "epoch": 0.7192008879023307,
      "grad_norm": 2.907348155975342,
      "learning_rate": 0.0003802545885139136,
      "loss": 1.3237,
      "step": 1620
    },
    {
      "epoch": 0.7236403995560489,
      "grad_norm": 2.109365224838257,
      "learning_rate": 0.00037951450562462996,
      "loss": 1.2038,
      "step": 1630
    },
    {
      "epoch": 0.7280799112097669,
      "grad_norm": 2.029604434967041,
      "learning_rate": 0.00037877442273534636,
      "loss": 1.2806,
      "step": 1640
    },
    {
      "epoch": 0.732519422863485,
      "grad_norm": 3.7805356979370117,
      "learning_rate": 0.00037803433984606277,
      "loss": 1.3228,
      "step": 1650
    },
    {
      "epoch": 0.7369589345172031,
      "grad_norm": 3.477368116378784,
      "learning_rate": 0.0003772942569567792,
      "loss": 1.2331,
      "step": 1660
    },
    {
      "epoch": 0.7413984461709212,
      "grad_norm": 2.4978432655334473,
      "learning_rate": 0.0003765541740674956,
      "loss": 1.1916,
      "step": 1670
    },
    {
      "epoch": 0.7458379578246392,
      "grad_norm": 2.6162259578704834,
      "learning_rate": 0.000375814091178212,
      "loss": 1.2123,
      "step": 1680
    },
    {
      "epoch": 0.7502774694783574,
      "grad_norm": 2.777238607406616,
      "learning_rate": 0.00037507400828892833,
      "loss": 1.2888,
      "step": 1690
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 2.3157105445861816,
      "learning_rate": 0.0003743339253996448,
      "loss": 1.2471,
      "step": 1700
    },
    {
      "epoch": 0.7591564927857936,
      "grad_norm": 1.8402167558670044,
      "learning_rate": 0.00037359384251036114,
      "loss": 1.3752,
      "step": 1710
    },
    {
      "epoch": 0.7635960044395117,
      "grad_norm": 2.027571439743042,
      "learning_rate": 0.0003728537596210776,
      "loss": 1.2647,
      "step": 1720
    },
    {
      "epoch": 0.7680355160932297,
      "grad_norm": 1.9094743728637695,
      "learning_rate": 0.00037211367673179395,
      "loss": 1.2921,
      "step": 1730
    },
    {
      "epoch": 0.7724750277469479,
      "grad_norm": 2.897094249725342,
      "learning_rate": 0.00037137359384251036,
      "loss": 1.1782,
      "step": 1740
    },
    {
      "epoch": 0.7769145394006659,
      "grad_norm": 2.5221755504608154,
      "learning_rate": 0.00037063351095322676,
      "loss": 1.2724,
      "step": 1750
    },
    {
      "epoch": 0.781354051054384,
      "grad_norm": 2.4435060024261475,
      "learning_rate": 0.00036989342806394317,
      "loss": 1.2431,
      "step": 1760
    },
    {
      "epoch": 0.7857935627081021,
      "grad_norm": 3.433166027069092,
      "learning_rate": 0.0003691533451746596,
      "loss": 1.3318,
      "step": 1770
    },
    {
      "epoch": 0.7902330743618202,
      "grad_norm": 2.768627166748047,
      "learning_rate": 0.000368413262285376,
      "loss": 1.3737,
      "step": 1780
    },
    {
      "epoch": 0.7946725860155383,
      "grad_norm": 3.1287553310394287,
      "learning_rate": 0.00036767317939609233,
      "loss": 1.2251,
      "step": 1790
    },
    {
      "epoch": 0.7991120976692564,
      "grad_norm": 2.128648042678833,
      "learning_rate": 0.0003669330965068088,
      "loss": 1.3406,
      "step": 1800
    },
    {
      "epoch": 0.8035516093229744,
      "grad_norm": 2.511744976043701,
      "learning_rate": 0.00036619301361752514,
      "loss": 1.3364,
      "step": 1810
    },
    {
      "epoch": 0.8079911209766926,
      "grad_norm": 2.834439277648926,
      "learning_rate": 0.0003654529307282416,
      "loss": 1.2263,
      "step": 1820
    },
    {
      "epoch": 0.8124306326304107,
      "grad_norm": 1.8574966192245483,
      "learning_rate": 0.00036471284783895795,
      "loss": 1.2755,
      "step": 1830
    },
    {
      "epoch": 0.8168701442841287,
      "grad_norm": 1.9493478536605835,
      "learning_rate": 0.00036397276494967435,
      "loss": 1.3086,
      "step": 1840
    },
    {
      "epoch": 0.8213096559378469,
      "grad_norm": 3.059087038040161,
      "learning_rate": 0.00036323268206039076,
      "loss": 1.2578,
      "step": 1850
    },
    {
      "epoch": 0.8257491675915649,
      "grad_norm": 3.091503620147705,
      "learning_rate": 0.00036249259917110716,
      "loss": 1.2783,
      "step": 1860
    },
    {
      "epoch": 0.8301886792452831,
      "grad_norm": 2.5705952644348145,
      "learning_rate": 0.00036175251628182357,
      "loss": 1.4447,
      "step": 1870
    },
    {
      "epoch": 0.8346281908990011,
      "grad_norm": 2.274764060974121,
      "learning_rate": 0.00036101243339254,
      "loss": 1.3539,
      "step": 1880
    },
    {
      "epoch": 0.8390677025527192,
      "grad_norm": 3.1099283695220947,
      "learning_rate": 0.0003602723505032564,
      "loss": 1.4636,
      "step": 1890
    },
    {
      "epoch": 0.8435072142064373,
      "grad_norm": 3.4586181640625,
      "learning_rate": 0.0003595322676139728,
      "loss": 1.1836,
      "step": 1900
    },
    {
      "epoch": 0.8479467258601554,
      "grad_norm": 2.310471534729004,
      "learning_rate": 0.0003587921847246892,
      "loss": 1.2528,
      "step": 1910
    },
    {
      "epoch": 0.8523862375138734,
      "grad_norm": 2.177295207977295,
      "learning_rate": 0.0003580521018354056,
      "loss": 1.2325,
      "step": 1920
    },
    {
      "epoch": 0.8568257491675916,
      "grad_norm": 2.4178223609924316,
      "learning_rate": 0.000357312018946122,
      "loss": 1.2742,
      "step": 1930
    },
    {
      "epoch": 0.8612652608213096,
      "grad_norm": 2.4221882820129395,
      "learning_rate": 0.00035657193605683835,
      "loss": 1.2754,
      "step": 1940
    },
    {
      "epoch": 0.8657047724750278,
      "grad_norm": 1.860136866569519,
      "learning_rate": 0.0003558318531675548,
      "loss": 1.2677,
      "step": 1950
    },
    {
      "epoch": 0.8701442841287459,
      "grad_norm": 2.4410409927368164,
      "learning_rate": 0.00035509177027827116,
      "loss": 1.3772,
      "step": 1960
    },
    {
      "epoch": 0.8745837957824639,
      "grad_norm": 2.259046792984009,
      "learning_rate": 0.0003543516873889876,
      "loss": 1.2543,
      "step": 1970
    },
    {
      "epoch": 0.8790233074361821,
      "grad_norm": 1.9376980066299438,
      "learning_rate": 0.00035361160449970397,
      "loss": 1.2474,
      "step": 1980
    },
    {
      "epoch": 0.8834628190899001,
      "grad_norm": 2.565645694732666,
      "learning_rate": 0.0003528715216104204,
      "loss": 1.0465,
      "step": 1990
    },
    {
      "epoch": 0.8879023307436182,
      "grad_norm": 4.752220630645752,
      "learning_rate": 0.0003521314387211368,
      "loss": 1.2269,
      "step": 2000
    },
    {
      "epoch": 0.8923418423973363,
      "grad_norm": 2.784287691116333,
      "learning_rate": 0.0003513913558318532,
      "loss": 1.3076,
      "step": 2010
    },
    {
      "epoch": 0.8967813540510544,
      "grad_norm": 4.195347309112549,
      "learning_rate": 0.0003506512729425696,
      "loss": 1.4301,
      "step": 2020
    },
    {
      "epoch": 0.9012208657047724,
      "grad_norm": 2.292226552963257,
      "learning_rate": 0.000349911190053286,
      "loss": 1.3365,
      "step": 2030
    },
    {
      "epoch": 0.9056603773584906,
      "grad_norm": 2.43320631980896,
      "learning_rate": 0.00034917110716400234,
      "loss": 1.3202,
      "step": 2040
    },
    {
      "epoch": 0.9100998890122086,
      "grad_norm": 2.402198314666748,
      "learning_rate": 0.0003484310242747188,
      "loss": 1.1725,
      "step": 2050
    },
    {
      "epoch": 0.9145394006659268,
      "grad_norm": 2.1881065368652344,
      "learning_rate": 0.00034769094138543515,
      "loss": 1.2839,
      "step": 2060
    },
    {
      "epoch": 0.9189789123196448,
      "grad_norm": 1.8475335836410522,
      "learning_rate": 0.0003469508584961516,
      "loss": 1.2011,
      "step": 2070
    },
    {
      "epoch": 0.9234184239733629,
      "grad_norm": 2.3030402660369873,
      "learning_rate": 0.00034621077560686796,
      "loss": 1.3943,
      "step": 2080
    },
    {
      "epoch": 0.9278579356270811,
      "grad_norm": 2.8198013305664062,
      "learning_rate": 0.00034547069271758437,
      "loss": 1.4321,
      "step": 2090
    },
    {
      "epoch": 0.9322974472807991,
      "grad_norm": 2.1051223278045654,
      "learning_rate": 0.0003447306098283008,
      "loss": 1.2649,
      "step": 2100
    },
    {
      "epoch": 0.9367369589345172,
      "grad_norm": 2.481344699859619,
      "learning_rate": 0.0003439905269390172,
      "loss": 1.2455,
      "step": 2110
    },
    {
      "epoch": 0.9411764705882353,
      "grad_norm": 2.976132392883301,
      "learning_rate": 0.0003432504440497336,
      "loss": 1.2647,
      "step": 2120
    },
    {
      "epoch": 0.9456159822419534,
      "grad_norm": 3.210503339767456,
      "learning_rate": 0.00034251036116045,
      "loss": 1.3676,
      "step": 2130
    },
    {
      "epoch": 0.9500554938956715,
      "grad_norm": 2.400081157684326,
      "learning_rate": 0.00034177027827116634,
      "loss": 1.3116,
      "step": 2140
    },
    {
      "epoch": 0.9544950055493896,
      "grad_norm": 1.8725590705871582,
      "learning_rate": 0.0003410301953818828,
      "loss": 1.1833,
      "step": 2150
    },
    {
      "epoch": 0.9589345172031076,
      "grad_norm": 2.828796863555908,
      "learning_rate": 0.00034029011249259915,
      "loss": 1.176,
      "step": 2160
    },
    {
      "epoch": 0.9633740288568258,
      "grad_norm": 2.2701761722564697,
      "learning_rate": 0.0003395500296033156,
      "loss": 1.325,
      "step": 2170
    },
    {
      "epoch": 0.9678135405105438,
      "grad_norm": 2.2489287853240967,
      "learning_rate": 0.00033880994671403196,
      "loss": 1.3181,
      "step": 2180
    },
    {
      "epoch": 0.9722530521642619,
      "grad_norm": 1.8751778602600098,
      "learning_rate": 0.00033806986382474836,
      "loss": 1.2499,
      "step": 2190
    },
    {
      "epoch": 0.97669256381798,
      "grad_norm": 2.2308638095855713,
      "learning_rate": 0.00033732978093546477,
      "loss": 1.2912,
      "step": 2200
    },
    {
      "epoch": 0.9811320754716981,
      "grad_norm": 2.251229763031006,
      "learning_rate": 0.0003365896980461812,
      "loss": 1.2541,
      "step": 2210
    },
    {
      "epoch": 0.9855715871254163,
      "grad_norm": 2.0617949962615967,
      "learning_rate": 0.0003358496151568976,
      "loss": 1.3543,
      "step": 2220
    },
    {
      "epoch": 0.9900110987791343,
      "grad_norm": 2.9683804512023926,
      "learning_rate": 0.000335109532267614,
      "loss": 1.29,
      "step": 2230
    },
    {
      "epoch": 0.9944506104328524,
      "grad_norm": 3.065110683441162,
      "learning_rate": 0.00033436944937833033,
      "loss": 1.4118,
      "step": 2240
    },
    {
      "epoch": 0.9988901220865705,
      "grad_norm": 2.382269859313965,
      "learning_rate": 0.0003336293664890468,
      "loss": 1.0963,
      "step": 2250
    },
    {
      "epoch": 1.0033296337402886,
      "grad_norm": 1.8833093643188477,
      "learning_rate": 0.0003328892835997632,
      "loss": 1.2489,
      "step": 2260
    },
    {
      "epoch": 1.0077691453940067,
      "grad_norm": 1.7492579221725464,
      "learning_rate": 0.0003321492007104796,
      "loss": 1.2988,
      "step": 2270
    },
    {
      "epoch": 1.0122086570477247,
      "grad_norm": 2.5458309650421143,
      "learning_rate": 0.000331409117821196,
      "loss": 1.2664,
      "step": 2280
    },
    {
      "epoch": 1.0166481687014428,
      "grad_norm": 2.1255764961242676,
      "learning_rate": 0.00033066903493191236,
      "loss": 1.122,
      "step": 2290
    },
    {
      "epoch": 1.021087680355161,
      "grad_norm": 2.2985191345214844,
      "learning_rate": 0.0003299289520426288,
      "loss": 1.1377,
      "step": 2300
    },
    {
      "epoch": 1.025527192008879,
      "grad_norm": 2.472223997116089,
      "learning_rate": 0.00032918886915334517,
      "loss": 1.2435,
      "step": 2310
    },
    {
      "epoch": 1.029966703662597,
      "grad_norm": 2.3725171089172363,
      "learning_rate": 0.00032844878626406163,
      "loss": 1.2156,
      "step": 2320
    },
    {
      "epoch": 1.0344062153163152,
      "grad_norm": 2.1236491203308105,
      "learning_rate": 0.000327708703374778,
      "loss": 1.0609,
      "step": 2330
    },
    {
      "epoch": 1.0388457269700333,
      "grad_norm": 2.015129566192627,
      "learning_rate": 0.0003269686204854944,
      "loss": 1.2746,
      "step": 2340
    },
    {
      "epoch": 1.0432852386237514,
      "grad_norm": 2.806223154067993,
      "learning_rate": 0.0003262285375962108,
      "loss": 1.2457,
      "step": 2350
    },
    {
      "epoch": 1.0477247502774696,
      "grad_norm": 2.469069004058838,
      "learning_rate": 0.0003254884547069272,
      "loss": 1.2986,
      "step": 2360
    },
    {
      "epoch": 1.0521642619311875,
      "grad_norm": 2.2171850204467773,
      "learning_rate": 0.0003247483718176436,
      "loss": 1.2312,
      "step": 2370
    },
    {
      "epoch": 1.0566037735849056,
      "grad_norm": 2.004631996154785,
      "learning_rate": 0.00032400828892836,
      "loss": 1.2888,
      "step": 2380
    },
    {
      "epoch": 1.0610432852386238,
      "grad_norm": 3.481259822845459,
      "learning_rate": 0.00032326820603907635,
      "loss": 1.1646,
      "step": 2390
    },
    {
      "epoch": 1.065482796892342,
      "grad_norm": 3.5468332767486572,
      "learning_rate": 0.0003225281231497928,
      "loss": 1.2545,
      "step": 2400
    },
    {
      "epoch": 1.0699223085460599,
      "grad_norm": 2.489809274673462,
      "learning_rate": 0.00032178804026050916,
      "loss": 1.1666,
      "step": 2410
    },
    {
      "epoch": 1.074361820199778,
      "grad_norm": 1.7560049295425415,
      "learning_rate": 0.0003210479573712256,
      "loss": 1.1918,
      "step": 2420
    },
    {
      "epoch": 1.0788013318534961,
      "grad_norm": 2.6162755489349365,
      "learning_rate": 0.00032030787448194197,
      "loss": 1.1915,
      "step": 2430
    },
    {
      "epoch": 1.0832408435072143,
      "grad_norm": 2.567081928253174,
      "learning_rate": 0.0003195677915926584,
      "loss": 1.2986,
      "step": 2440
    },
    {
      "epoch": 1.0876803551609322,
      "grad_norm": 2.2984960079193115,
      "learning_rate": 0.0003188277087033748,
      "loss": 1.1703,
      "step": 2450
    },
    {
      "epoch": 1.0921198668146503,
      "grad_norm": 2.108612537384033,
      "learning_rate": 0.0003180876258140912,
      "loss": 1.2901,
      "step": 2460
    },
    {
      "epoch": 1.0965593784683685,
      "grad_norm": 2.3637850284576416,
      "learning_rate": 0.0003173475429248076,
      "loss": 1.1323,
      "step": 2470
    },
    {
      "epoch": 1.1009988901220866,
      "grad_norm": 1.8329159021377563,
      "learning_rate": 0.000316607460035524,
      "loss": 1.3127,
      "step": 2480
    },
    {
      "epoch": 1.1054384017758045,
      "grad_norm": 2.1613948345184326,
      "learning_rate": 0.00031586737714624035,
      "loss": 1.3645,
      "step": 2490
    },
    {
      "epoch": 1.1098779134295227,
      "grad_norm": 3.2343435287475586,
      "learning_rate": 0.0003151272942569568,
      "loss": 1.3153,
      "step": 2500
    },
    {
      "epoch": 1.1143174250832408,
      "grad_norm": 2.944899797439575,
      "learning_rate": 0.00031438721136767316,
      "loss": 1.211,
      "step": 2510
    },
    {
      "epoch": 1.118756936736959,
      "grad_norm": 2.2123000621795654,
      "learning_rate": 0.0003136471284783896,
      "loss": 1.2605,
      "step": 2520
    },
    {
      "epoch": 1.1231964483906771,
      "grad_norm": 2.521188735961914,
      "learning_rate": 0.00031290704558910597,
      "loss": 1.1525,
      "step": 2530
    },
    {
      "epoch": 1.127635960044395,
      "grad_norm": 2.9518613815307617,
      "learning_rate": 0.00031216696269982237,
      "loss": 1.4673,
      "step": 2540
    },
    {
      "epoch": 1.1320754716981132,
      "grad_norm": 2.6925246715545654,
      "learning_rate": 0.0003114268798105388,
      "loss": 1.3116,
      "step": 2550
    },
    {
      "epoch": 1.1365149833518313,
      "grad_norm": 2.416891574859619,
      "learning_rate": 0.0003106867969212552,
      "loss": 1.367,
      "step": 2560
    },
    {
      "epoch": 1.1409544950055495,
      "grad_norm": 2.159923553466797,
      "learning_rate": 0.0003099467140319716,
      "loss": 1.1389,
      "step": 2570
    },
    {
      "epoch": 1.1453940066592674,
      "grad_norm": 2.171494245529175,
      "learning_rate": 0.000309206631142688,
      "loss": 1.1552,
      "step": 2580
    },
    {
      "epoch": 1.1498335183129855,
      "grad_norm": 2.370046377182007,
      "learning_rate": 0.00030846654825340434,
      "loss": 1.2822,
      "step": 2590
    },
    {
      "epoch": 1.1542730299667037,
      "grad_norm": 2.736478567123413,
      "learning_rate": 0.0003077264653641208,
      "loss": 1.2089,
      "step": 2600
    },
    {
      "epoch": 1.1587125416204218,
      "grad_norm": 3.6072492599487305,
      "learning_rate": 0.00030698638247483715,
      "loss": 1.2394,
      "step": 2610
    },
    {
      "epoch": 1.16315205327414,
      "grad_norm": 2.3369784355163574,
      "learning_rate": 0.0003062462995855536,
      "loss": 1.4634,
      "step": 2620
    },
    {
      "epoch": 1.1675915649278579,
      "grad_norm": 2.442995071411133,
      "learning_rate": 0.00030550621669627,
      "loss": 1.213,
      "step": 2630
    },
    {
      "epoch": 1.172031076581576,
      "grad_norm": 2.9687016010284424,
      "learning_rate": 0.00030476613380698637,
      "loss": 1.1588,
      "step": 2640
    },
    {
      "epoch": 1.1764705882352942,
      "grad_norm": 3.002044677734375,
      "learning_rate": 0.0003040260509177028,
      "loss": 1.1027,
      "step": 2650
    },
    {
      "epoch": 1.1809100998890123,
      "grad_norm": 2.1949100494384766,
      "learning_rate": 0.0003032859680284192,
      "loss": 1.227,
      "step": 2660
    },
    {
      "epoch": 1.1853496115427302,
      "grad_norm": 2.2456674575805664,
      "learning_rate": 0.00030254588513913564,
      "loss": 1.0935,
      "step": 2670
    },
    {
      "epoch": 1.1897891231964484,
      "grad_norm": 3.1037633419036865,
      "learning_rate": 0.000301805802249852,
      "loss": 1.2114,
      "step": 2680
    },
    {
      "epoch": 1.1942286348501665,
      "grad_norm": 1.8266792297363281,
      "learning_rate": 0.0003010657193605684,
      "loss": 1.1689,
      "step": 2690
    },
    {
      "epoch": 1.1986681465038846,
      "grad_norm": 2.212174415588379,
      "learning_rate": 0.0003003256364712848,
      "loss": 1.2804,
      "step": 2700
    },
    {
      "epoch": 1.2031076581576028,
      "grad_norm": 2.7373647689819336,
      "learning_rate": 0.0002995855535820012,
      "loss": 1.3174,
      "step": 2710
    },
    {
      "epoch": 1.2075471698113207,
      "grad_norm": 2.871727228164673,
      "learning_rate": 0.0002988454706927176,
      "loss": 1.2202,
      "step": 2720
    },
    {
      "epoch": 1.2119866814650389,
      "grad_norm": 2.1525402069091797,
      "learning_rate": 0.000298105387803434,
      "loss": 1.1276,
      "step": 2730
    },
    {
      "epoch": 1.216426193118757,
      "grad_norm": 2.4317424297332764,
      "learning_rate": 0.00029736530491415036,
      "loss": 1.2066,
      "step": 2740
    },
    {
      "epoch": 1.220865704772475,
      "grad_norm": 2.238224506378174,
      "learning_rate": 0.0002966252220248668,
      "loss": 1.1431,
      "step": 2750
    },
    {
      "epoch": 1.225305216426193,
      "grad_norm": 2.328303813934326,
      "learning_rate": 0.00029588513913558317,
      "loss": 1.207,
      "step": 2760
    },
    {
      "epoch": 1.2297447280799112,
      "grad_norm": 2.8975861072540283,
      "learning_rate": 0.00029514505624629963,
      "loss": 1.2341,
      "step": 2770
    },
    {
      "epoch": 1.2341842397336293,
      "grad_norm": 1.982830286026001,
      "learning_rate": 0.000294404973357016,
      "loss": 1.18,
      "step": 2780
    },
    {
      "epoch": 1.2386237513873475,
      "grad_norm": 2.5605311393737793,
      "learning_rate": 0.0002936648904677324,
      "loss": 1.0867,
      "step": 2790
    },
    {
      "epoch": 1.2430632630410654,
      "grad_norm": 1.880393624305725,
      "learning_rate": 0.0002929248075784488,
      "loss": 1.1795,
      "step": 2800
    },
    {
      "epoch": 1.2475027746947835,
      "grad_norm": 2.493835210800171,
      "learning_rate": 0.0002921847246891652,
      "loss": 1.1725,
      "step": 2810
    },
    {
      "epoch": 1.2519422863485017,
      "grad_norm": 3.806032419204712,
      "learning_rate": 0.0002914446417998816,
      "loss": 1.1731,
      "step": 2820
    },
    {
      "epoch": 1.2563817980022198,
      "grad_norm": 1.874725580215454,
      "learning_rate": 0.000290704558910598,
      "loss": 1.1509,
      "step": 2830
    },
    {
      "epoch": 1.2608213096559377,
      "grad_norm": 2.4433417320251465,
      "learning_rate": 0.00028996447602131436,
      "loss": 1.3995,
      "step": 2840
    },
    {
      "epoch": 1.265260821309656,
      "grad_norm": 2.454763174057007,
      "learning_rate": 0.0002892243931320308,
      "loss": 1.186,
      "step": 2850
    },
    {
      "epoch": 1.269700332963374,
      "grad_norm": 2.5560622215270996,
      "learning_rate": 0.00028848431024274717,
      "loss": 1.2642,
      "step": 2860
    },
    {
      "epoch": 1.2741398446170922,
      "grad_norm": 2.6806108951568604,
      "learning_rate": 0.0002877442273534636,
      "loss": 1.1983,
      "step": 2870
    },
    {
      "epoch": 1.2785793562708103,
      "grad_norm": 2.1315486431121826,
      "learning_rate": 0.00028700414446418,
      "loss": 1.18,
      "step": 2880
    },
    {
      "epoch": 1.2830188679245282,
      "grad_norm": 2.376688003540039,
      "learning_rate": 0.0002862640615748964,
      "loss": 1.2564,
      "step": 2890
    },
    {
      "epoch": 1.2874583795782464,
      "grad_norm": 3.168863534927368,
      "learning_rate": 0.0002855239786856128,
      "loss": 1.2495,
      "step": 2900
    }
  ],
  "logging_steps": 10,
  "max_steps": 6756,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4593574965805056.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
